{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Books – Data Collection and Integration Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Integration and Analytic Data Processing – Project Phase I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin: 0;\">\n",
    "  <table style=\"margin-left: 0;\">\n",
    "    <tr>\n",
    "      <th>Clara Saldanha</th>\n",
    "      <th>Daniel João</th>\n",
    "      <th>Diogo Antunes</th>\n",
    "      <th>Mariana Tomás</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>fc64501@alunos.fc.ul.pt</code></td>\n",
    "      <td><code>fc56455@alunos.fc.ul.pt</code></td>\n",
    "      <td><code>fc64337@alunos.fc.ul.pt</code></td>\n",
    "      <td><code>fc60421@alunos.fc.ul.pt</code></td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "**Group:** 12\n",
    "\n",
    "**Professor:** Assistant Professor **André Rodrigues** from the Informatics Department\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ]\n",
      "pandas version: 2.2.3\n",
      "numpy version: 1.26.4\n",
      "matplotlib version: 3.9.2\n",
      "seaborn version: 0.13.2\n",
      "rapidfuzz version: 3.13.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib.metadata as metadata\n",
    "\n",
    "# Add whenever you use more bruh\n",
    "packages = {\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"seaborn\": \"seaborn\",\n",
    "    \"rapidfuzz\": \"rapidfuzz\"\n",
    "}\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "for pkg_name, pkg_identifier in packages.items():\n",
    "    try:\n",
    "        version = metadata.version(pkg_identifier)\n",
    "        print(f\"{pkg_name} version: {version}\")\n",
    "    except metadata.PackageNotFoundError:\n",
    "        print(f\"{pkg_name} is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import csv\n",
    "from rapidfuzz import process, fuzz\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "# working directory just to make sure\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory:\", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see full entry values in pandas, regardless of their size\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it back column to have a limit\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Datasets Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Amazon Reviews’23 - Books\n",
    "https://amazon-reviews-2023.github.io/\n",
    "\n",
    "The **Amazon Reviews’23** dataset is a large-scale collection of Amazon product reviews assembled by the McAuley Lab in 2023. It comprises over 571 million **reviews**, with user feedback that includes ratings, review texts, and helpfulness votes. \n",
    "\n",
    "In addition, it provides detailed **item metadata** such as product descriptions, prices, and raw images, alongside relational data like user-item interaction graphs and bought-together links. \n",
    "\n",
    "Covering interactions (reviews) from May 1996 to September 2023, the dataset features fine-grained timestamps, cleaner preprocessed review datasets, and standardized data splits. Its mostly used for benchmarking recommendation systems.\n",
    "\n",
    "We selected the a pre-made subset specific to items categorized has \"Books\", emcompasing both physical books and *ebooks*. We took two files from it, both came zipped to accomodate better data transfer.\n",
    "\n",
    "- **Books.jsonl.gz** - Preprocessed with the rewiews data\n",
    "- **meta_books.jsonl.gz** - File with the metadata of items (descriptions, price, rating, etc.) catagorized as Books in amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Goodreads - 2017\n",
    "https://cseweb.ucsd.edu/~jmcauley/datasets/goodreads.html#datasets\n",
    "\n",
    "This dataset was collected from goodreads.com in late 2017, focusing on user‑submitted public shelves that do not require login to view. All user and review identifiers have been anonymized. It covers approximately 2.36 million books (including works, book series, and authors), 876,000 users, and over 228 million user‑book interactions (ratings, reads, and other shelf statuses).\n",
    "\n",
    "It also includes **detailed metadata about books**, authors, works, and series, as well as comprehensive review data. Subsets organized by genre (e.g., Children’s, Fantasy, Romance) are provided for more manageable exploration.\n",
    "\n",
    "(1) meta-data of the books, (2) user-book interactions (users' public shelves) and (3) users' detailed book reviews. These datasets can be merged together by joining on book/user/review ids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Goodreads - 2019\n",
    "https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks/data\n",
    "\n",
    "This kaggle dataset is a curated, clean collection of book information compiled using the **Goodreads API**.\n",
    "\n",
    "Its creator built it to overcome common issues in other book datasets—such as **missing key columns** and **unclean data** and **focused on including reliable numerical data** (like ratings and counts) along with important details such as **publisher information, publication dates, and properly formatted author names (with multiple authors delimited by '/')**. Unlike the prior goodreads dataset (2017), this one has only has one table with the book metadata information.\n",
    "\n",
    "It was **initiated in May 2019** and saw **updates until December 2020**, when <ins>changes to the Goodreads API led to its discontinuation</ins>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Book‑Crossing Community\n",
    "https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset\n",
    "\n",
    "A kaggle dataset that was collected from the Book‑Crossing community by Cai‑Nicolas Ziegler over a four‑week crawl in August/September 2004. It has been preprocessed and cleaned to remove invalid entries, with user IDs anonymized. \n",
    "\n",
    "The Book‑Crossing community is a global network of readers who share and track books through the website bookcrossing.com. Members register each book online, then “release” it—either in public places for anyone to find (wild releases) or directly to other participants (controlled releases)—and record its journey on the site. By encouraging people to pass books along rather than keep them, Book‑Crossing aspires to transform the whole world into a library. Over time, this community has grown to include forums, meetups, and conventions, with more than 1.9 million members worldwide.\n",
    "\n",
    "In total, it covers:\n",
    "\n",
    "- **278,858 users** (with possible demographic information such as location and age),  \n",
    "- **271,379 unique books** (identified by valid ISBNs and accompanied by metadata like title, author, publication year, publisher, and Amazon cover image links),  \n",
    "- **1,149,780 ratings**, which may be explicit (on a 1–10 scale) or implicit (denoted by 0).\n",
    "\n",
    "The data is organized into three main files: **Users**, **Books**, and **Ratings**. This structure **allows us to link user demographics to specific book records and the corresponding rating behavior**. \n",
    "\n",
    "--> The dataset is particularly useful for exploring or benchmarking recommendation systems and other data‑intensive analyses within the realm of reading preferences and user behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Books Sales and Ratings\n",
    "https://www.kaggle.com/datasets/thedevastator/books-sales-and-ratings/data\n",
    "\n",
    "This is dataset featuring various attributes about books from nine different publishers, with **publishing years ranging from 1600s to 2016**. Included in the data is attributes reagarding sales, ratings and book identities. \n",
    "\n",
    "The data was sourced on the linked kaggle dataset but <ins>it was orginally published by Josh Murrey on data.world under the name Books</ins> (https://data.world/josh-nbu). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Amazon Kindle Books Dataset 2023 (130K Books) \n",
    "https://www.kaggle.com/datasets/asaniczka/amazon-kindle-books-dataset-2023-130k-books\n",
    "\n",
    "This kaggle dataset comprises data on **130,000 Kindle e-books**, scraped from **publicly available information on Amazon’s Kindle Books webpage in October 2023**.\n",
    "\n",
    "The data were systematically collected by navigating through the Kindle book category pages on amazon.com/kindle-books, capturing a range of book details and sales information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Wonderbook \n",
    "https://www.kaggle.com/datasets/elvinrustam/books-dataset\n",
    "\n",
    "\n",
    "This kaggle dataset is derived from **wonderbk.com** (an amazon competitor), a popular online bookstore, using a **Python-based web scraping approach**. The data acquisition process employed libraries such as requests, Beautiful Soup (bs4), and Selenium, with two primary functions defined: one to gather URLs for individual books, and another to extract pertinent details including title, authors, description, category, publisher, starting price, and publication dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Data Profiling\n",
    "\n",
    "**Data profiling** is the process of examining, analyzing, and summarizing data to gain informaiton into its structure, quality, and content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Structure Analysis \n",
    "\n",
    "In this section, we determine the data types present (i.e: numerical, categorical, text) and how the data is in files/ tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Amazon Reviews’23 - Books\n",
    "\n",
    "A gzipped JSON Lines (*jsonl.gz*) file and a gzipped CSV file were downloaded from this dataset's website\n",
    "\n",
    "- a detailed book graph encompassing the metadata (**meta_Books.jsonl.gz *~4.6 GB***,\n",
    "- book reviews simplified and de-duplicated (user_id, parent_asin, rating, timestamp) to have a lighter load and no repeated entries when compared to the raw book review dataset  (**Books.csv.gz *~574MB***)\n",
    "    - <ins>pros</ins>: Comprehensive data still with maximum diversity.\n",
    "    - <ins>cons</ins>: Still has imbalance reviews per book.\n",
    " \n",
    "Each line in a JSONL file is a valid JSON object, and records are delimited by newline characters, making it helpful to parse the file line by line without loading the entire dataset into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To lighten the computational power needed to run this dataset, specifically on our RAM (even with 16GB we were having issues opening the data on pandas) we decided to subset the data sets by removing less relevant books (low rating) in Google's cloud base tool **Google BigQuery**. We uploaded our files into Google storage buckets, imported the files into tables and then queried new subsetted tables that took up less storage. \n",
    "\n",
    "##### **Data Subsetting on Google's BigQuery**:\n",
    " - **meta_Books.jsonl**\n",
    "     - Removed entries with a value on \"rating_number\" null or lower than 1000.\n",
    "     - **transformed JSONL format (meta_Books.jsonl) into a CSV format**\n",
    " - **Books.csv**\n",
    "     - Removed entries that had no \"parent_asin\" correspondence to the meta_Books \"parent_asin\" table.\n",
    "     - Keeping review proportions for each book, reduced the overall number of the dataset entries by 70%.\n",
    "\n",
    "After subsetting and transformation, the following <ins>files unzipped</ins> were produced:\n",
    " - **amazon_meta_books.csv (1.3GB)**\n",
    " - **amazon_reviews_filtered.csv (570.5MB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) i) Amazon Metadata Books\n",
    "\n",
    "Amazon books metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/1_amazon/amazon_meta_books.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 280338 entries, 0 to 280337\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   main_category    280155 non-null  object \n",
      " 1   title            280338 non-null  object \n",
      " 2   subtitle         223708 non-null  object \n",
      " 3   author           230008 non-null  object \n",
      " 4   average_rating   280338 non-null  float64\n",
      " 5   rating_number    280338 non-null  int64  \n",
      " 6   features         280338 non-null  object \n",
      " 7   description      280338 non-null  object \n",
      " 8   price            197032 non-null  float64\n",
      " 9   images           280338 non-null  object \n",
      " 10  videos           280338 non-null  object \n",
      " 11  store            270276 non-null  object \n",
      " 12  categories       280338 non-null  object \n",
      " 13  details          280338 non-null  object \n",
      " 14  parent_asin      280338 non-null  object \n",
      " 15  bought_together  0 non-null       float64\n",
      "dtypes: float64(3), int64(1), object(12)\n",
      "memory usage: 34.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>bought_together</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Swan Thieves</td>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>{\"name\":\"Elizabeth Kostova\",\"about\":[\"Elizabeth Kostova's engrossing debut novel is the culminat...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1559</td>\n",
       "      <td>[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>{\"Language\":\"English\",\"Dimensions\":\"5.5 x 2.15 x 5.75 inches\",\"Publisher\":\"Hachette Audio; Una R...</td>\n",
       "      <td>B0062GL89I</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category             title                                    subtitle  \\\n",
       "0         Books  The Swan Thieves  Audio CD – Bargain Price, November 3, 2010   \n",
       "\n",
       "                                                                                                author  \\\n",
       "0  {\"name\":\"Elizabeth Kostova\",\"about\":[\"Elizabeth Kostova's engrossing debut novel is the culminat...   \n",
       "\n",
       "   average_rating  rating_number  \\\n",
       "0             4.2           1559   \n",
       "\n",
       "                                                                                              features  \\\n",
       "0  [\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...   \n",
       "\n",
       "  description  price images videos  \\\n",
       "0          []    NaN     []     []   \n",
       "\n",
       "                                                                                                 store  \\\n",
       "0  Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...   \n",
       "\n",
       "                                         categories  \\\n",
       "0  [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "\n",
       "                                                                                               details  \\\n",
       "0  {\"Language\":\"English\",\"Dimensions\":\"5.5 x 2.15 x 5.75 inches\",\"Publisher\":\"Hachette Audio; Una R...   \n",
       "\n",
       "  parent_asin  bought_together  \n",
       "0  B0062GL89I              NaN  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the hierarchical nature of JSON files, some columns have nested columns within them**. Some of these have to be extracted, specifically on \"author\" and \"details\".\n",
    "\n",
    "Keeping \"features\" unflattened and \"details\" only partially, due to them producing a massive set of columns with high NaN/null content as we will see ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting author, details \n",
    "df['author'] = df['author'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = pd.json_normalize(df['author'])\n",
    "df = df.join(author_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping \"author\" column just because we already split it into its inner elements\n",
    "df.drop('author', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# renaming some of the columns quickly\n",
    "df = df.rename(columns={'name': 'author_name', 'avatar': 'author_avatar', 'about': 'author_about'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>bought_together</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_about</th>\n",
       "      <th>author_avatar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Swan Thieves</td>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1559</td>\n",
       "      <td>[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>{\"Language\":\"English\",\"Dimensions\":\"5.5 x 2.15 x 5.75 inches\",\"Publisher\":\"Hachette Audio; Una R...</td>\n",
       "      <td>B0062GL89I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elizabeth Kostova</td>\n",
       "      <td>[Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a li...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audible Audiobooks</td>\n",
       "      <td>Death in a White Tie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1033</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...</td>\n",
       "      <td>[\"Books\",\"Mystery, Thriller &amp; Suspense\",\"Mystery\",\"Traditional Detectives\"]</td>\n",
       "      <td>{}</td>\n",
       "      <td>B001F1ZPDK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Books</td>\n",
       "      <td>Reunion Pass (Thorndike Romance)</td>\n",
       "      <td>Hardcover – Large Print, May 18, 2016</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1140</td>\n",
       "      <td>[\"A New York Times Bestselling Author An Eternity Springs Novel Six years ago, Chase Timberlake ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>12.00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Emily March (Author)</td>\n",
       "      <td>[\"Books\",\"Large Print\",\"Literature &amp; Fiction\"]</td>\n",
       "      <td>{\"ISBN 13\":\"978-1410490117\",\"Language\":\"English\",\"ISBN 10\":\"9781410490117\",\"Dimensions\":\"6 x 1 x...</td>\n",
       "      <td>1410490114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily March</td>\n",
       "      <td>[Emily March is the New York Times, Publishers Weekly, and USA Today bestselling author of over ...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/amzn-author-media-prod/an3i0pthoa0nsmk8mnn25ndb7n._SY600_.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Books</td>\n",
       "      <td>Key Lock Man</td>\n",
       "      <td>Mass Market Paperback – January 1, 1981</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1907</td>\n",
       "      <td>[\"book 163\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>5.55</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Louis L'Amour (Author)</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\"ISBN 13\":\"978-0553230888\",\"Language\":\"English\",\"Mass Market Paperback\":\"0 pages\",\"ISBN 10\":\"05...</td>\n",
       "      <td>0553230883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis L'Amour</td>\n",
       "      <td>[\"I think of myself in the oral tradition--as a troubadour, a village tale-teller, the man in th...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31pdVqK+eZL._SY600_.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Robots of Dawn (Robot, 3)</td>\n",
       "      <td>MP3 CD – Unabridged, July 15, 2007</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5660</td>\n",
       "      <td>[\"A puzzling case of roboticide sends New York Detective Elijah Baley on an intense search for a...</td>\n",
       "      <td>[\"About the Author\",\"Isaac Asimov, who was named \\\"Grand Master of Science Fiction\\\" by the Scie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Isaac Asimov (Author),  William Dufris (Narrator)</td>\n",
       "      <td>[\"Books\",\"Computers &amp; Technology\",\"Computer Science\"]</td>\n",
       "      <td>{\"ISBN 13\":\"978-1400154234\",\"Language\":\"English\",\"ISBN 10\":\"1400154235\",\"Dimensions\":\"5.3 x 0.6 ...</td>\n",
       "      <td>1400154235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Isaac Asimov</td>\n",
       "      <td>[Isaac Asimov (/ˈaɪzᵻk ˈæzᵻmɒv/; born Isaak Yudovich Ozimov; circa January 2, 1920 – April 6, 19...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/amzn-author-media-prod/6ce4rnjgpci8m81tu3aibtlf5r._SY600_.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        main_category                             title  \\\n",
       "0               Books                  The Swan Thieves   \n",
       "1  Audible Audiobooks              Death in a White Tie   \n",
       "2               Books  Reunion Pass (Thorndike Romance)   \n",
       "3               Books                      Key Lock Man   \n",
       "4               Books     The Robots of Dawn (Robot, 3)   \n",
       "\n",
       "                                     subtitle  average_rating  rating_number  \\\n",
       "0  Audio CD – Bargain Price, November 3, 2010             4.2           1559   \n",
       "1                                         NaN             4.5           1033   \n",
       "2       Hardcover – Large Print, May 18, 2016             4.6           1140   \n",
       "3     Mass Market Paperback – January 1, 1981             4.6           1907   \n",
       "4          MP3 CD – Unabridged, July 15, 2007             4.7           5660   \n",
       "\n",
       "                                                                                              features  \\\n",
       "0  [\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...   \n",
       "1                                                                                                   []   \n",
       "2  [\"A New York Times Bestselling Author An Eternity Springs Novel Six years ago, Chase Timberlake ...   \n",
       "3                                                                                         [\"book 163\"]   \n",
       "4  [\"A puzzling case of roboticide sends New York Detective Elijah Baley on an intense search for a...   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0                                                                                                   []   \n",
       "1                                                                                                   []   \n",
       "2                                                                                                   []   \n",
       "3                                                                                                   []   \n",
       "4  [\"About the Author\",\"Isaac Asimov, who was named \\\"Grand Master of Science Fiction\\\" by the Scie...   \n",
       "\n",
       "   price images videos  \\\n",
       "0    NaN     []     []   \n",
       "1    NaN     []     []   \n",
       "2  12.00     []     []   \n",
       "3   5.55     []     []   \n",
       "4    NaN     []     []   \n",
       "\n",
       "                                                                                                 store  \\\n",
       "0  Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...   \n",
       "1  Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...   \n",
       "2                                                                                 Emily March (Author)   \n",
       "3                                                                               Louis L'Amour (Author)   \n",
       "4                                                    Isaac Asimov (Author),  William Dufris (Narrator)   \n",
       "\n",
       "                                                                    categories  \\\n",
       "0                             [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "1  [\"Books\",\"Mystery, Thriller & Suspense\",\"Mystery\",\"Traditional Detectives\"]   \n",
       "2                               [\"Books\",\"Large Print\",\"Literature & Fiction\"]   \n",
       "3                                                                           []   \n",
       "4                        [\"Books\",\"Computers & Technology\",\"Computer Science\"]   \n",
       "\n",
       "                                                                                               details  \\\n",
       "0  {\"Language\":\"English\",\"Dimensions\":\"5.5 x 2.15 x 5.75 inches\",\"Publisher\":\"Hachette Audio; Una R...   \n",
       "1                                                                                                   {}   \n",
       "2  {\"ISBN 13\":\"978-1410490117\",\"Language\":\"English\",\"ISBN 10\":\"9781410490117\",\"Dimensions\":\"6 x 1 x...   \n",
       "3  {\"ISBN 13\":\"978-0553230888\",\"Language\":\"English\",\"Mass Market Paperback\":\"0 pages\",\"ISBN 10\":\"05...   \n",
       "4  {\"ISBN 13\":\"978-1400154234\",\"Language\":\"English\",\"ISBN 10\":\"1400154235\",\"Dimensions\":\"5.3 x 0.6 ...   \n",
       "\n",
       "  parent_asin  bought_together        author_name  \\\n",
       "0  B0062GL89I              NaN  Elizabeth Kostova   \n",
       "1  B001F1ZPDK              NaN                NaN   \n",
       "2  1410490114              NaN        Emily March   \n",
       "3  0553230883              NaN      Louis L'Amour   \n",
       "4  1400154235              NaN       Isaac Asimov   \n",
       "\n",
       "                                                                                          author_about  \\\n",
       "0  [Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a li...   \n",
       "1                                                                                                  NaN   \n",
       "2  [Emily March is the New York Times, Publishers Weekly, and USA Today bestselling author of over ...   \n",
       "3  [\"I think of myself in the oral tradition--as a troubadour, a village tale-teller, the man in th...   \n",
       "4  [Isaac Asimov (/ˈaɪzᵻk ˈæzᵻmɒv/; born Isaak Yudovich Ozimov; circa January 2, 1920 – April 6, 19...   \n",
       "\n",
       "                                                                                       author_avatar  \n",
       "0                                        https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg  \n",
       "1                                                                                                NaN  \n",
       "2  https://m.media-amazon.com/images/S/amzn-author-media-prod/an3i0pthoa0nsmk8mnn25ndb7n._SY600_.jpg  \n",
       "3                                        https://m.media-amazon.com/images/I/31pdVqK+eZL._SY600_.jpg  \n",
       "4  https://m.media-amazon.com/images/S/amzn-author-media-prod/6ce4rnjgpci8m81tu3aibtlf5r._SY600_.jpg  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/1_amazon/amazon_meta_books_X.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***amazon_meta_books.csv* Characterization Table**\n",
    "\n",
    "| Column           | Description                                                             | Data Type         | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
    "|------------------|-------------------------------------------------------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| main_category    | The primary category of the product                                     | String            | Books                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
    "| title            | The title of the product or item                                        | String            | The Swan Thieves                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
    "| subtitle         | Additional description or subtitle                                      | String            | Audio CD – Bargain Price, November 3, 2010                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
    "| average_rating   | The average customer rating                                             | Float/Number      | 4.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "| rating_number    | The total number of ratings received                                    | Integer           | 1559                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
    "| features         | Key features or highlights as a list of strings                         | List of Strings   | `[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devotion to his profession and the painting hobby he loves. ...\"]`                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "| description      | Additional descriptive details (may be empty or a list)                 | List              | []                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "| price            | The price of the product (if available)                                 | Float/NaN         | NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "| images           | List of image URLs associated with the product                          | List              | []                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "| videos           | List of video URLs associated with the product                           | List              | []                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "| store            | Store or seller information (can include multiple names)                | String            | Elizabeth Kostova (Author), Treat Williams (Reader), Anne Heche (Reader), Erin Cottrell (Reader), Sarah Zimmerman (Reader), John Lee (Reader)                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
    "| categories       | Categories or genres the product belongs to                             | LIST   | `[\"Books\", \"Literature & Fiction\", \"Genre Fiction\"]`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
    "| details          | Additional product details (e.g., dimensions, publisher) as a dictionary  | Dictionary        | `{'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; Una Rei edition (November 3, 2010)', 'Item Weight': '1.05 pounds'}`                                                                                                                                                                                                                                                                                                                                                                                       |\n",
    "| parent_asin      | Unique identifier (ASIN) for the product                                | String            | B0062GL89I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
    "| bought_together  | Information on products frequently bought together                      | LIST          | NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "| author_name      | The name of the author associated with the product                       | String            | Elizabeth Kostova                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
    "| author_about     | A brief biography or description about the author                        | List or String    | `[Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a lifetime of imagining--... ]`                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
    "| author_avatar    | URL to the author's avatar image                                         | String            | https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) ii) Amazon Reviews\n",
    "\n",
    "Amazon books reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/1_amazon/amazon_reviews_filtered.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEAHYXN6YKMNPZKVEY4ROOEOU5AA</td>\n",
       "      <td>0006480411</td>\n",
       "      <td>1</td>\n",
       "      <td>1334395472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGQG3VEWZKUM6TVGLULDIBLIHL3Q</td>\n",
       "      <td>0006490344</td>\n",
       "      <td>1</td>\n",
       "      <td>1651361248390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHD6ZKMA37UCQNGQWIFK5RTWHU5A</td>\n",
       "      <td>0006497802</td>\n",
       "      <td>1</td>\n",
       "      <td>1001426115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFCGHC53VJZZR5ZVHJCIPCTD772A</td>\n",
       "      <td>0006514642</td>\n",
       "      <td>1</td>\n",
       "      <td>1193857022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AG6Z734FKPPJPZK5CXM5AI7HWV5Q</td>\n",
       "      <td>0006546064</td>\n",
       "      <td>1</td>\n",
       "      <td>1647291639876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683296</th>\n",
       "      <td>AHBPXM3BSVNB76URMXF37DYKGPUQ</td>\n",
       "      <td>B0C9V7R2BF</td>\n",
       "      <td>5</td>\n",
       "      <td>1689948745143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683297</th>\n",
       "      <td>AFGCUZLUXZSJL3JORCVX46QMPDJQ</td>\n",
       "      <td>B0CCCNBNRQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1690130519431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683298</th>\n",
       "      <td>AFQXVDVJNYHAIJ7AQVOGBBZ6A2LQ</td>\n",
       "      <td>B0CCCSMRW3</td>\n",
       "      <td>5</td>\n",
       "      <td>1690471341941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683299</th>\n",
       "      <td>AE6DGMEPPX6YD34PDSAPFKWKXULQ</td>\n",
       "      <td>B0CCCSMRW3</td>\n",
       "      <td>5</td>\n",
       "      <td>1693935434123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683300</th>\n",
       "      <td>AG5JYAFRHGYTTQEJH3GREFXJNU6A</td>\n",
       "      <td>B0CGG9DS8H</td>\n",
       "      <td>5</td>\n",
       "      <td>1692886745783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10683301 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               user_id parent_asin  rating      timestamp\n",
       "0         AEAHYXN6YKMNPZKVEY4ROOEOU5AA  0006480411       1  1334395472000\n",
       "1         AGQG3VEWZKUM6TVGLULDIBLIHL3Q  0006490344       1  1651361248390\n",
       "2         AHD6ZKMA37UCQNGQWIFK5RTWHU5A  0006497802       1  1001426115000\n",
       "3         AFCGHC53VJZZR5ZVHJCIPCTD772A  0006514642       1  1193857022000\n",
       "4         AG6Z734FKPPJPZK5CXM5AI7HWV5Q  0006546064       1  1647291639876\n",
       "...                                ...         ...     ...            ...\n",
       "10683296  AHBPXM3BSVNB76URMXF37DYKGPUQ  B0C9V7R2BF       5  1689948745143\n",
       "10683297  AFGCUZLUXZSJL3JORCVX46QMPDJQ  B0CCCNBNRQ       5  1690130519431\n",
       "10683298  AFQXVDVJNYHAIJ7AQVOGBBZ6A2LQ  B0CCCSMRW3       5  1690471341941\n",
       "10683299  AE6DGMEPPX6YD34PDSAPFKWKXULQ  B0CCCSMRW3       5  1693935434123\n",
       "10683300  AG5JYAFRHGYTTQEJH3GREFXJNU6A  B0CGG9DS8H       5  1692886745783\n",
       "\n",
       "[10683301 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***amazon_reviews_filtered.csv* Characterization Table:**\n",
    "\n",
    "| Column      | Description                                                             | Data Type    | Example                      |\n",
    "|-------------|-------------------------------------------------------------------------|--------------|------------------------------|\n",
    "| user_id     | Unique identifier of a user                                             | TEXT       | AEAHYXN6YKMNPZKVEY4ROOEOU5AA   |\n",
    "| parent_asin | Unique product identifier (ASIN)                                       | TEXT       | 0006480411                   |\n",
    "| rating      | User's rating for the product                                          | NUMERIC      | 1                            |\n",
    "| timestamp   | Time when the rating was recorded (in milliseconds since epoch)        | DATE      | 1334395472000                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all information in 'details' seems useful, specifically because of these NaN in most entries. Will opt to drop some of them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Item Weight</th>\n",
       "      <th>ISBN 13</th>\n",
       "      <th>ISBN 10</th>\n",
       "      <th>Hardcover</th>\n",
       "      <th>Mass Market Paperback</th>\n",
       "      <th>Paperback</th>\n",
       "      <th>File size</th>\n",
       "      <th>...</th>\n",
       "      <th>Best Sellers Rank.Children's Activity Books</th>\n",
       "      <th>Diskette</th>\n",
       "      <th>Best Sellers Rank.Teen &amp; Young Adult Epic Fantasy</th>\n",
       "      <th>Best Sellers Rank.Genre Literature &amp; Fiction</th>\n",
       "      <th>Best Sellers Rank.Children's Arts, Music &amp; Photography Books</th>\n",
       "      <th>Best Sellers Rank.Children's Craft &amp; Hobby Books</th>\n",
       "      <th>Best Sellers Rank.Literary Fiction</th>\n",
       "      <th>Best Sellers Rank.Folklore</th>\n",
       "      <th>Best Sellers Rank.Classic Literature &amp; Fiction</th>\n",
       "      <th>Sports Apparel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5.5 x 2.15 x 5.75 inches</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>1.05 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>6 x 1 x 9 inches</td>\n",
       "      <td>Thorndike Press Large Print; Large Print edition (May 18, 2016)</td>\n",
       "      <td>1.2 pounds</td>\n",
       "      <td>978-1410490117</td>\n",
       "      <td>9781410490117</td>\n",
       "      <td>422 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bantam (January 1, 1981)</td>\n",
       "      <td>3.2 ounces</td>\n",
       "      <td>978-0553230888</td>\n",
       "      <td>0553230883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>5.3 x 0.6 x 7.4 inches</td>\n",
       "      <td>Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)</td>\n",
       "      <td>2.61 ounces</td>\n",
       "      <td>978-1400154234</td>\n",
       "      <td>1400154235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280333</th>\n",
       "      <td>English</td>\n",
       "      <td>6.42 x 1.4 x 9.52 inches</td>\n",
       "      <td>Doubleday (September 13, 2011)</td>\n",
       "      <td>1.64 pounds</td>\n",
       "      <td>978-0385534635</td>\n",
       "      <td>0385534639</td>\n",
       "      <td>400 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280334</th>\n",
       "      <td>English</td>\n",
       "      <td>5.02 x 0.85 x 5.94 inches</td>\n",
       "      <td>Macmillan Audio; Unabridged edition (November 4, 2014)</td>\n",
       "      <td>8.6 ounces</td>\n",
       "      <td>978-1427252098</td>\n",
       "      <td>1427252092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scribner; 31685th edition (January 1, 1994)</td>\n",
       "      <td>1 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280336</th>\n",
       "      <td>English</td>\n",
       "      <td>6 x 0.25 x 9 inches</td>\n",
       "      <td>Cloud Forest Press (March 3, 2020)</td>\n",
       "      <td>6.7 ounces</td>\n",
       "      <td>978-1646081639</td>\n",
       "      <td>1646081633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280337</th>\n",
       "      <td>English</td>\n",
       "      <td>5.75 x 1.4 x 8.63 inches</td>\n",
       "      <td>VIZ Media LLC (December 2, 2008)</td>\n",
       "      <td>2.05 pounds</td>\n",
       "      <td>978-1421520667</td>\n",
       "      <td>1421520664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>560 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280338 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language                 Dimensions  \\\n",
       "0       English   5.5 x 2.15 x 5.75 inches   \n",
       "1           NaN                        NaN   \n",
       "2       English           6 x 1 x 9 inches   \n",
       "3       English                        NaN   \n",
       "4       English     5.3 x 0.6 x 7.4 inches   \n",
       "...         ...                        ...   \n",
       "280333  English   6.42 x 1.4 x 9.52 inches   \n",
       "280334  English  5.02 x 0.85 x 5.94 inches   \n",
       "280335      NaN                        NaN   \n",
       "280336  English        6 x 0.25 x 9 inches   \n",
       "280337  English   5.75 x 1.4 x 8.63 inches   \n",
       "\n",
       "                                                              Publisher  \\\n",
       "0                    Hachette Audio; Una Rei edition (November 3, 2010)   \n",
       "1                                                                   NaN   \n",
       "2       Thorndike Press Large Print; Large Print edition (May 18, 2016)   \n",
       "3                                              Bantam (January 1, 1981)   \n",
       "4             Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)   \n",
       "...                                                                 ...   \n",
       "280333                                   Doubleday (September 13, 2011)   \n",
       "280334           Macmillan Audio; Unabridged edition (November 4, 2014)   \n",
       "280335                      Scribner; 31685th edition (January 1, 1994)   \n",
       "280336                               Cloud Forest Press (March 3, 2020)   \n",
       "280337                                 VIZ Media LLC (December 2, 2008)   \n",
       "\n",
       "        Item Weight         ISBN 13        ISBN 10  Hardcover  \\\n",
       "0       1.05 pounds             NaN            NaN        NaN   \n",
       "1               NaN             NaN            NaN        NaN   \n",
       "2        1.2 pounds  978-1410490117  9781410490117  422 pages   \n",
       "3        3.2 ounces  978-0553230888     0553230883        NaN   \n",
       "4       2.61 ounces  978-1400154234     1400154235        NaN   \n",
       "...             ...             ...            ...        ...   \n",
       "280333  1.64 pounds  978-0385534635     0385534639  400 pages   \n",
       "280334   8.6 ounces  978-1427252098     1427252092        NaN   \n",
       "280335     1 pounds             NaN            NaN        NaN   \n",
       "280336   6.7 ounces  978-1646081639     1646081633        NaN   \n",
       "280337  2.05 pounds  978-1421520667     1421520664        NaN   \n",
       "\n",
       "       Mass Market Paperback  Paperback File size  ...  \\\n",
       "0                        NaN        NaN       NaN  ...   \n",
       "1                        NaN        NaN       NaN  ...   \n",
       "2                        NaN        NaN       NaN  ...   \n",
       "3                    0 pages        NaN       NaN  ...   \n",
       "4                        NaN        NaN       NaN  ...   \n",
       "...                      ...        ...       ...  ...   \n",
       "280333                   NaN        NaN       NaN  ...   \n",
       "280334                   NaN        NaN       NaN  ...   \n",
       "280335                   NaN        NaN       NaN  ...   \n",
       "280336                   NaN  108 pages       NaN  ...   \n",
       "280337                   NaN  560 pages       NaN  ...   \n",
       "\n",
       "       Best Sellers Rank.Children's Activity Books Diskette  \\\n",
       "0                                              NaN      NaN   \n",
       "1                                              NaN      NaN   \n",
       "2                                              NaN      NaN   \n",
       "3                                              NaN      NaN   \n",
       "4                                              NaN      NaN   \n",
       "...                                            ...      ...   \n",
       "280333                                         NaN      NaN   \n",
       "280334                                         NaN      NaN   \n",
       "280335                                         NaN      NaN   \n",
       "280336                                         NaN      NaN   \n",
       "280337                                         NaN      NaN   \n",
       "\n",
       "       Best Sellers Rank.Teen & Young Adult Epic Fantasy  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "280333                                               NaN   \n",
       "280334                                               NaN   \n",
       "280335                                               NaN   \n",
       "280336                                               NaN   \n",
       "280337                                               NaN   \n",
       "\n",
       "       Best Sellers Rank.Genre Literature & Fiction  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "...                                             ...   \n",
       "280333                                          NaN   \n",
       "280334                                          NaN   \n",
       "280335                                          NaN   \n",
       "280336                                          NaN   \n",
       "280337                                          NaN   \n",
       "\n",
       "       Best Sellers Rank.Children's Arts, Music & Photography Books  \\\n",
       "0                                                               NaN   \n",
       "1                                                               NaN   \n",
       "2                                                               NaN   \n",
       "3                                                               NaN   \n",
       "4                                                               NaN   \n",
       "...                                                             ...   \n",
       "280333                                                          NaN   \n",
       "280334                                                          NaN   \n",
       "280335                                                          NaN   \n",
       "280336                                                          NaN   \n",
       "280337                                                          NaN   \n",
       "\n",
       "       Best Sellers Rank.Children's Craft & Hobby Books  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "280333                                              NaN   \n",
       "280334                                              NaN   \n",
       "280335                                              NaN   \n",
       "280336                                              NaN   \n",
       "280337                                              NaN   \n",
       "\n",
       "       Best Sellers Rank.Literary Fiction Best Sellers Rank.Folklore  \\\n",
       "0                                     NaN                        NaN   \n",
       "1                                     NaN                        NaN   \n",
       "2                                     NaN                        NaN   \n",
       "3                                     NaN                        NaN   \n",
       "4                                     NaN                        NaN   \n",
       "...                                   ...                        ...   \n",
       "280333                                NaN                        NaN   \n",
       "280334                                NaN                        NaN   \n",
       "280335                                NaN                        NaN   \n",
       "280336                                NaN                        NaN   \n",
       "280337                                NaN                        NaN   \n",
       "\n",
       "       Best Sellers Rank.Classic Literature & Fiction Sports Apparel  \n",
       "0                                                 NaN            NaN  \n",
       "1                                                 NaN            NaN  \n",
       "2                                                 NaN            NaN  \n",
       "3                                                 NaN            NaN  \n",
       "4                                                 NaN            NaN  \n",
       "...                                               ...            ...  \n",
       "280333                                            NaN            NaN  \n",
       "280334                                            NaN            NaN  \n",
       "280335                                            NaN            NaN  \n",
       "280336                                            NaN            NaN  \n",
       "280337                                            NaN            NaN  \n",
       "\n",
       "[280338 rows x 185 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the 'details' column.\n",
    "details_df = pd.json_normalize(df['details'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language                                                                    19.686236\n",
      "Dimensions                                                                  34.149848\n",
      "Publisher                                                                   21.550771\n",
      "Item Weight                                                                 26.822265\n",
      "ISBN 13                                                                     30.399018\n",
      "ISBN 10                                                                     32.670919\n",
      "Hardcover                                                                   84.411318\n",
      "Mass Market Paperback                                                       95.490087\n",
      "Paperback                                                                   66.483317\n",
      "File size                                                                   92.290021\n",
      "Text to Speech                                                              92.290735\n",
      "Word Wise                                                                   92.292875\n",
      "Enhanced typesetting                                                        92.291805\n",
      "Screen Reader                                                               92.562906\n",
      "X Ray                                                                       92.405239\n",
      "Publication date                                                            91.757450\n",
      "Sticky notes                                                                92.289308\n",
      "Print length                                                                92.323552\n",
      "MP3 CD                                                                      99.848041\n",
      "Reading age                                                                 89.027174\n",
      "Page numbers source ISBN                                                    98.513580\n",
      "Grade level                                                                 94.037911\n",
      "Audio CD                                                                    98.963037\n",
      "Lexile measure                                                              94.813047\n",
      "Novelty Book                                                                99.990725\n",
      "Imitation Leather                                                           99.818433\n",
      "Library Binding                                                             99.407501\n",
      "Simultaneous device usage                                                   98.638786\n",
      "Board book                                                                  99.476346\n",
      "Sheet music                                                                 99.997503\n",
      "Audio Cassette                                                              99.870870\n",
      "Product Dimensions                                                          99.455300\n",
      "Is Discontinued By Manufacturer                                             99.906898\n",
      "Country of Origin                                                           99.799171\n",
      "Release date                                                                99.499176\n",
      "Spiral bound                                                                99.760289\n",
      "Date First Available                                                        99.951844\n",
      "File Size                                                                   99.960048\n",
      "Loose Leaf                                                                  99.985018\n",
      "School Library Binding                                                      99.777412\n",
      "Package Dimensions                                                          99.866233\n",
      "Leather Bound                                                               99.854105\n",
      "Bonded Leather                                                              99.968609\n",
      "Pamphlet                                                                    99.973960\n",
      "Plastic Comb                                                                99.991439\n",
      "Unknown Binding                                                             99.884425\n",
      "Run time                                                                    99.905471\n",
      "Cards                                                                       99.885495\n",
      "CD ROM                                                                      99.995719\n",
      "Misc Supplies                                                               99.985375\n",
      "Comic                                                                       99.984305\n",
      "Pocket Book                                                                 99.919383\n",
      "Perfect Paperback                                                           99.966112\n",
      "Item model number                                                           99.979311\n",
      "Pages                                                                       99.998930\n",
      "Binding                                                                     99.998930\n",
      "Ruling Type                                                                 99.999643\n",
      "Color                                                                       99.996790\n",
      "Sheet Size                                                                  99.999643\n",
      "Cover Material                                                              99.999287\n",
      "Theme                                                                       99.998930\n",
      "Number of Items                                                             99.997503\n",
      "Special Feature                                                             99.999287\n",
      "Calendar                                                                    99.931155\n",
      "Product Bundle                                                              99.984305\n",
      "Flexibound                                                                  99.977527\n",
      "Roughcut                                                                    99.998216\n",
      "Misc                                                                        99.999643\n",
      "Vinyl Bound                                                                 99.999287\n",
      "X Ray for textbooks                                                         99.998216\n",
      "Diary                                                                       99.983591\n",
      "Hardcover spiral                                                            99.988942\n",
      "Paperback Shinsho                                                           99.999643\n",
      "Pop Up                                                                      99.998930\n",
      "Map                                                                         99.994293\n",
      "Format                                                                      99.999287\n",
      "Contributor                                                                 99.998930\n",
      "Manufacturer recommended age                                                99.995719\n",
      "Domestic Shipping                                                           99.993579\n",
      "International Shipping                                                      99.993579\n",
      "Department                                                                  99.996790\n",
      "Manufacturer                                                                99.987158\n",
      "Best Sellers Rank.Children's Game Books                                     99.997860\n",
      "Paperback Bunko                                                             99.997503\n",
      "Tankobon Hardcover                                                          99.998216\n",
      "Card Book                                                                   99.998573\n",
      "Ring bound                                                                  99.993579\n",
      "Best Sellers Rank.Dragons & Mythical Creatures Fantasy                      99.998930\n",
      "Best Sellers Rank.Books                                                     99.991439\n",
      "Best Sellers Rank.Dungeons & Dragons Game                                   99.997860\n",
      "Best Sellers Rank.Sword & Sorcery Fantasy                                   99.998573\n",
      "Journal                                                                     99.999287\n",
      "Type of item                                                                99.998216\n",
      "Pricing                                                                     99.998216\n",
      "Poster                                                                      99.998573\n",
      "Bookmark                                                                    99.997146\n",
      "Best Sellers Rank.Children's Animals Books                                  99.999643\n",
      "Best Sellers Rank.Stories                                                   99.999643\n",
      "Staple Bound                                                                99.995719\n",
      "Toy                                                                         99.998930\n",
      "Digital Audiobook                                                           99.999643\n",
      "DVD                                                                         99.999643\n",
      "Turtleback                                                                  99.998573\n",
      "Single Issue Magazine                                                       99.997503\n",
      "Bath Book                                                                   99.998573\n",
      "Wall Chart                                                                  99.998930\n",
      "Accessory                                                                   99.998930\n",
      "Unbound                                                                     99.999287\n",
      "Best Sellers Rank.Contemporary Literature & Fiction                         99.999643\n",
      "Printed Access Code                                                         99.998930\n",
      "Print on Demand Paperback                                                   99.998216\n",
      "Textbook Binding                                                            99.999643\n",
      "Game                                                                        99.999287\n",
      "Audio CD Library Binding                                                    99.999287\n",
      "Edition                                                                     99.999643\n",
      "Brand                                                                       99.997146\n",
      "Publication Date                                                            99.999287\n",
      "Item Dimensions LxWxH                                                       99.999287\n",
      "Genre                                                                       99.998573\n",
      "Other display features                                                      99.998216\n",
      "Best Sellers Rank.Children's Books                                          99.999287\n",
      "Best Sellers Rank.Teen & Young Adult Dark Fantasy                           99.999287\n",
      "Best Sellers Rank.Teen & Young Adult Fantasy Action & Adventure             99.999287\n",
      "Preloaded Digital Audio Player                                              99.998216\n",
      "Best Sellers Rank.Piano Songbooks                                           99.999643\n",
      "Batteries Required?                                                         99.998573\n",
      "Hardware Platform                                                           99.999643\n",
      "Rag Book                                                                    99.998930\n",
      "Item Package Quantity                                                       99.998930\n",
      "Batteries Included?                                                         99.999287\n",
      "Best Sellers Rank.Epic Fantasy                                              99.999643\n",
      "Best Sellers Rank.Space Operas                                              99.999643\n",
      "Stationery                                                                  99.998930\n",
      "Blu ray                                                                     99.999643\n",
      "Best Sellers Rank.Textbooks                                                 99.999643\n",
      "Best Sellers Rank.Reference                                                 99.999287\n",
      "Best Sellers Rank.Puzzle & Game Reference                                   99.999287\n",
      "Best Sellers Rank.Children's Coloring Books                                 99.999643\n",
      "Digital                                                                     99.998930\n",
      "Hardcover Comic                                                             99.999643\n",
      "Size                                                                        99.999287\n",
      "Best Sellers Rank.Children's Friendship Books                               99.999643\n",
      "Best Sellers Rank.Children's Chapter Books                                  99.999643\n",
      "Best Sellers Rank.Children's Action & Adventure Books                       99.999643\n",
      "Manufacturer Part Number                                                    99.998573\n",
      "Best Sellers Rank.Education                                                 99.999643\n",
      "Best Sellers Rank.Teaching Materials                                        99.999643\n",
      "Best Sellers Rank.Reading & Phonics Teaching Materials                      99.999643\n",
      "Best Sellers Rank.Christian Inspirational                                   99.999643\n",
      "Best Sellers Rank.Christian Bibles                                          99.999287\n",
      "Best Sellers Rank.Inspiration & Spirituality                                99.999643\n",
      "Best Sellers Rank.Italian Cooking, Food & Wine                              99.999643\n",
      "Part Number                                                                 99.999287\n",
      "Best Sellers Rank.Drawing                                                   99.999643\n",
      "Best Sellers Rank.Christian Devotionals                                     99.999643\n",
      "Best Sellers Rank.Religious & Inspirational Coloring Books for Grown-Ups    99.999643\n",
      "DVD ROM                                                                     99.999287\n",
      "Item Package Dimensions L x W x H                                           99.999643\n",
      "Item Dimensions  LxWxH                                                      99.999643\n",
      "Brand Name                                                                  99.999643\n",
      "Model Year                                                                  99.999643\n",
      "Style                                                                       99.999643\n",
      "Suggested Users                                                             99.999643\n",
      "Package Weight                                                              99.999643\n",
      "Best Sellers Rank.Christian New Testament References                        99.999643\n",
      "Runtime                                                                     99.999643\n",
      "Best Sellers Rank.Psychic Mysteries                                         99.999643\n",
      "Best Sellers Rank.Women Sleuths                                             99.999643\n",
      "Batteries                                                                   99.999287\n",
      "Mook                                                                        99.999643\n",
      "Best Sellers Rank.Fiction Writing Reference                                 99.999643\n",
      "Best Sellers Rank.Instruction Methods                                       99.999643\n",
      "Best Sellers Rank.Early Childhood Education Materials                       99.999643\n",
      "Notebook                                                                    99.999643\n",
      "Print Magazine                                                              99.999643\n",
      "Best Sellers Rank.Children's Activity Books                                 99.999643\n",
      "Diskette                                                                    99.999643\n",
      "Best Sellers Rank.Teen & Young Adult Epic Fantasy                           99.999643\n",
      "Best Sellers Rank.Genre Literature & Fiction                                99.999643\n",
      "Best Sellers Rank.Children's Arts, Music & Photography Books                99.999643\n",
      "Best Sellers Rank.Children's Craft & Hobby Books                            99.999643\n",
      "Best Sellers Rank.Literary Fiction                                          99.999643\n",
      "Best Sellers Rank.Folklore                                                  99.999643\n",
      "Best Sellers Rank.Classic Literature & Fiction                              99.999643\n",
      "Sports Apparel                                                              99.999643\n"
     ]
    }
   ],
   "source": [
    "nan_percentage = details_df.isna().mean() * 100\n",
    "print(nan_percentage.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### b) Goodreads - 2017\n",
    "\n",
    "Three gzipped json files (*json.gz*) files were downloaded from this dataset's website: \n",
    "- a detailed book graph emcompasing the metadata of about 2.3 million books (**goodreads_books.json.gz *~2.1 GB***),\n",
    "- and a exclusive english review subset parsed emcompasing around 1.3 million book reviews, 25 thousand books and 19 thousand users, parsed at sentence level, meaning each of the reviews were decomposed in sentenses with a list (**goodreads_reviews_spoiler.json.gz *~591MB***)\n",
    "- Detailed information of authors (**goodreads_book_authors.json ~17.2MB**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Due to the files' large size - which made working with them, especially in packages like **pandas**, a daunting task — these files were also imported and subsetted using ***Google Cloud's BigQuery***.\n",
    "\n",
    "The number of entries of **goodreads_books.json** was reduced via query by:\n",
    "\n",
    "- removing entries with no *publicaiton_year* or older than 2010 (<2010);\n",
    "- removing entries with *ratings_count* lower than 1000 - ratings are not the same as reviews being in much higher counts across the dataset.\n",
    "\n",
    "The number of entries of **goodreads_reviews_spoiler.json** was reduced via query by:\n",
    "\n",
    "- only keeping entries with the *book_id*'s present in the prior split goodreads_books.json dataset, to cleave out books that would not be present in the metadata.\n",
    "- removing 70% of the original reviews.\n",
    "\n",
    "The number of entries of **goodreads_book_authors.json** was reduced via query by:\n",
    "\n",
    "- only keeping entries with the *author_id*'s present in the prior split goodreads_books.json dataset, to cleave out books that would not be present in the metadata.\n",
    "\n",
    "After subsetting and transformation, the following <ins>unzipped files</ins> were produced:\n",
    " - **goodreads_meta_books.json (~181.5MB)**\n",
    " - **goodreads_reviews_filtered.csv (~399.5MB)**\n",
    "\n",
    "#### b) i) Goodreads Metadata - *goodreads_books.json.gz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/2_3_goodreads/goodreads_meta_books.json'\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34758 entries, 0 to 34757\n",
      "Data columns (total 29 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title_without_series  34758 non-null  object \n",
      " 1   title                 34758 non-null  object \n",
      " 2   work_id               34758 non-null  int64  \n",
      " 3   book_id               34758 non-null  int64  \n",
      " 4   publication_year      34758 non-null  int64  \n",
      " 5   num_pages             32950 non-null  float64\n",
      " 6   ratings_count         34758 non-null  int64  \n",
      " 7   kindle_asin           34758 non-null  object \n",
      " 8   publisher             34758 non-null  object \n",
      " 9   authors               34758 non-null  object \n",
      " 10  format                34758 non-null  object \n",
      " 11  country_code          34758 non-null  object \n",
      " 12  series                34758 non-null  object \n",
      " 13  average_rating        34758 non-null  float64\n",
      " 14  similar_books         34758 non-null  object \n",
      " 15  image_url             34758 non-null  object \n",
      " 16  isbn13                34758 non-null  object \n",
      " 17  is_ebook              34758 non-null  bool   \n",
      " 18  text_reviews_count    34758 non-null  int64  \n",
      " 19  language_code         34758 non-null  object \n",
      " 20  description           34758 non-null  object \n",
      " 21  link                  34758 non-null  object \n",
      " 22  url                   34758 non-null  object \n",
      " 23  asin                  34758 non-null  object \n",
      " 24  popular_shelves       34758 non-null  object \n",
      " 25  edition_information   34758 non-null  object \n",
      " 26  isbn                  34758 non-null  object \n",
      " 27  publication_day       33100 non-null  float64\n",
      " 28  publication_month     34006 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(5), object(19)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing some examples of this data with the nested features visible:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title_without_series\": \"Heaven is for Real: A Little Boy's Astounding Story of His Trip to Heaven and Back\",\n",
      "  \"title\": \"Heaven is for Real: A Little Boy's Astounding Story of His Trip to Heaven and Back\",\n",
      "  \"work_id\": \"11283577\",\n",
      "  \"book_id\": \"7933292\",\n",
      "  \"publication_year\": \"2010\",\n",
      "  \"num_pages\": \"162\",\n",
      "  \"ratings_count\": \"229153\",\n",
      "  \"kindle_asin\": \"B004A90BXS\",\n",
      "  \"publisher\": \"\",\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"role\": \"\",\n",
      "      \"author_id\": \"3446736\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"\",\n",
      "      \"author_id\": \"266797\"\n",
      "    }\n",
      "  ],\n",
      "  \"format\": \"\",\n",
      "  \"country_code\": \"US\",\n",
      "  \"series\": [],\n",
      "  \"average_rating\": 4.01,\n",
      "  \"similar_books\": [\n",
      "    \"8100288\",\n",
      "    \"8765461\",\n",
      "    \"89375\",\n",
      "    \"13158130\",\n",
      "    \"6836258\",\n",
      "    \"299795\",\n",
      "    \"9640038\",\n",
      "    \"97862\",\n",
      "    \"104189\",\n",
      "    \"6436732\",\n",
      "    \"232631\",\n",
      "    \"6817610\",\n",
      "    \"13137883\",\n",
      "    \"7570892\",\n",
      "    \"11880626\",\n",
      "    \"824844\",\n",
      "    \"89376\",\n",
      "    \"8142508\"\n",
      "  ],\n",
      "  \"image_url\": \"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\",\n",
      "  \"isbn13\": \"9780849946158\",\n",
      "  \"is_ebook\": false,\n",
      "  \"text_reviews_count\": \"13633\",\n",
      "  \"language_code\": \"eng\",\n",
      "  \"description\": \"When Colton Burpo made it through an emergency appendectomy, his family was overjoyed at his miraculous survival. What they weren't expecting, though, was the story that emerged in the months that followed--a story as beautiful as it was extraordinary, detailing their little boy's trip to heaven and back.\\nColton, not yet four years old, told his parents he left his body during the surgery--and authenticated that claim by describing exactly what his parents were doing in another part of the hospital while he was being operated on. He talked of visiting heaven and relayed stories told to him by people he met there whom he had never met in life, sharing events that happened even before he was born. He also astonished his parents with descriptions and obscure details about heaven that matched the Bible exactly, though he had not yet learned to read.\\nWith disarming innocence and the plainspoken boldness of a child, Colton tells of meeting long-departed family members. He describes Jesus, the angels, how \\\"really, really big\\\" God is, and how much God loves us. Retold by his father, but using Colton's uniquely simple words, \\\"Heaven is for Real\\\" offers a glimpse of the world that awaits us, where as Colton says, \\\"Nobody is old and nobody wears glasses.\\\"\\n\\\"Heaven is for Real\\\" will forever change the way you think of eternity, offering you the chance to see, and believe, like a child.\",\n",
      "  \"link\": \"https://www.goodreads.com/book/show/7933292-heaven-is-for-real\",\n",
      "  \"url\": \"https://www.goodreads.com/book/show/7933292-heaven-is-for-real\",\n",
      "  \"asin\": \"\",\n",
      "  \"popular_shelves\": [\n",
      "    {\n",
      "      \"name\": \"to-read\",\n",
      "      \"count\": \"751\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"faith\",\n",
      "      \"count\": \"214\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"currently-reading\",\n",
      "      \"count\": \"205\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christianity\",\n",
      "      \"count\": \"179\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle\",\n",
      "      \"count\": \"136\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"heaven\",\n",
      "      \"count\": \"134\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"memoirs\",\n",
      "      \"count\": \"106\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christian-books\",\n",
      "      \"count\": \"71\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audiobook\",\n",
      "      \"count\": \"67\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"non-fiction\",\n",
      "      \"count\": \"66\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"my-books\",\n",
      "      \"count\": \"63\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"owned-books\",\n",
      "      \"count\": \"62\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audiobooks\",\n",
      "      \"count\": \"62\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"library\",\n",
      "      \"count\": \"62\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christian-life\",\n",
      "      \"count\": \"61\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"near-death-experience\",\n",
      "      \"count\": \"61\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"children\",\n",
      "      \"count\": \"60\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"heaven-is-for-real\",\n",
      "      \"count\": \"57\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audio\",\n",
      "      \"count\": \"56\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"biographies\",\n",
      "      \"count\": \"51\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"my-library\",\n",
      "      \"count\": \"51\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christian\",\n",
      "      \"count\": \"48\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christian-living\",\n",
      "      \"count\": \"47\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"finished\",\n",
      "      \"count\": \"45\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"i-own\",\n",
      "      \"count\": \"41\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christian-nonfiction\",\n",
      "      \"count\": \"40\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read-2014\",\n",
      "      \"count\": \"38\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"nonfiction\",\n",
      "      \"count\": \"36\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"borrowed\",\n",
      "      \"count\": \"36\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"book-club-books\",\n",
      "      \"count\": \"36\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-books\",\n",
      "      \"count\": \"35\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"theology\",\n",
      "      \"count\": \"35\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"family\",\n",
      "      \"count\": \"34\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"bookclub\",\n",
      "      \"count\": \"34\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-i-own\",\n",
      "      \"count\": \"33\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books\",\n",
      "      \"count\": \"33\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"favorite-books\",\n",
      "      \"count\": \"33\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audio-books\",\n",
      "      \"count\": \"33\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"inspiration\",\n",
      "      \"count\": \"33\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"shelfari-favorites\",\n",
      "      \"count\": \"30\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read-2012\",\n",
      "      \"count\": \"30\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"biography\",\n",
      "      \"count\": \"29\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"todd-burpo\",\n",
      "      \"count\": \"28\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"memoir-biography\",\n",
      "      \"count\": \"27\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"true-stories\",\n",
      "      \"count\": \"27\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"young-adult\",\n",
      "      \"count\": \"26\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"to-buy\",\n",
      "      \"count\": \"26\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"want-to-read\",\n",
      "      \"count\": \"26\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"religion\",\n",
      "      \"count\": \"25\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-i-have\",\n",
      "      \"count\": \"25\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"god\",\n",
      "      \"count\": \"25\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"true-story\",\n",
      "      \"count\": \"25\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"angels\",\n",
      "      \"count\": \"23\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-it\",\n",
      "      \"count\": \"23\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"adult-non-fiction\",\n",
      "      \"count\": \"23\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"jesus\",\n",
      "      \"count\": \"22\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"inspiring\",\n",
      "      \"count\": \"22\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"owned\",\n",
      "      \"count\": \"21\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audible\",\n",
      "      \"count\": \"21\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2012-reads\",\n",
      "      \"count\": \"20\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"my-favorites\",\n",
      "      \"count\": \"20\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"book-club-reads\",\n",
      "      \"count\": \"20\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"wish-list\",\n",
      "      \"count\": \"20\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"other\",\n",
      "      \"count\": \"19\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"adult-nonfiction\",\n",
      "      \"count\": \"19\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2014-reads\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read-in-2013\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"abandoned\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audio-book\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"favorite\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-read-in-2011\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"religious\",\n",
      "      \"count\": \"17\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"have\",\n",
      "      \"count\": \"17\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"self-help\",\n",
      "      \"count\": \"17\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"couldn-t-finish\",\n",
      "      \"count\": \"17\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"life\",\n",
      "      \"count\": \"17\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"faith-based\",\n",
      "      \"count\": \"17\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"spiritual\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2014-books\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2012-books\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"listened-to\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"recommended\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"bio-memoir\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"religous\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2011-read\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"biographical\",\n",
      "      \"count\": \"16\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-to-movies\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-read-in-2012\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"have-read\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"did-not-finish\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"church\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"burpo\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"non-fic\",\n",
      "      \"count\": \"15\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"101-books-that-you-must-read-before\",\n",
      "      \"count\": \"14\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-read-in-2014\",\n",
      "      \"count\": \"14\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2013-reads\",\n",
      "      \"count\": \"14\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"already-read\",\n",
      "      \"count\": \"14\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"christian-lit\",\n",
      "      \"count\": \"14\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"thought-provoking\",\n",
      "      \"count\": \"14\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"must-read\",\n",
      "      \"count\": \"14\"\n",
      "    }\n",
      "  ],\n",
      "  \"edition_information\": \"\",\n",
      "  \"isbn\": \"0849946158\"\n",
      "}\n",
      "{\n",
      "  \"title_without_series\": \"All the Little Children\",\n",
      "  \"title\": \"All the Little Children\",\n",
      "  \"work_id\": \"55111067\",\n",
      "  \"book_id\": \"34093937\",\n",
      "  \"publication_year\": \"2017\",\n",
      "  \"num_pages\": \"320\",\n",
      "  \"publication_day\": \"1\",\n",
      "  \"ratings_count\": \"6911\",\n",
      "  \"kindle_asin\": \"B01MR8641A\",\n",
      "  \"publisher\": \"\",\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"role\": \"\",\n",
      "      \"author_id\": \"16370494\"\n",
      "    }\n",
      "  ],\n",
      "  \"publication_month\": \"9\",\n",
      "  \"format\": \"\",\n",
      "  \"country_code\": \"US\",\n",
      "  \"series\": [],\n",
      "  \"average_rating\": 3.72,\n",
      "  \"similar_books\": [],\n",
      "  \"image_url\": \"https://images.gr-assets.com/books/1500923450m/34093937.jpg\",\n",
      "  \"isbn13\": \"\",\n",
      "  \"is_ebook\": true,\n",
      "  \"text_reviews_count\": \"557\",\n",
      "  \"language_code\": \"eng\",\n",
      "  \"description\": \"When a family camping trip takes a dark turn, how far will one mother go to keep her family safe?Struggling with working-mother guilt, Marlene Greene hopes a camping trip in the forest will provide quality time with her three young children--until they see fires in the distance, columns of smoke distorting the sweeping view. Overnight, all communication with the outside world is lost.\\nKnowing something terrible has happened, Marlene suspects that the isolation of the remote campsite is all that's protecting her family. But the arrival of a lost boy reveals they are not alone in the woods, and as the unfolding disaster ravages the land, more youngsters seek refuge under her wing. The lives of her own children aren't the only ones at stake.\\nWhen their sanctuary is threatened, Marlene faces the mother of all dilemmas: Should she save her own kids or try to save them all?\",\n",
      "  \"link\": \"https://www.goodreads.com/book/show/34093937-all-the-little-children\",\n",
      "  \"url\": \"https://www.goodreads.com/book/show/34093937-all-the-little-children\",\n",
      "  \"asin\": \"B01MR8641A\",\n",
      "  \"popular_shelves\": [\n",
      "    {\n",
      "      \"name\": \"to-read\",\n",
      "      \"count\": \"6135\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"currently-reading\",\n",
      "      \"count\": \"4201\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle\",\n",
      "      \"count\": \"64\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-first\",\n",
      "      \"count\": \"47\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"fiction\",\n",
      "      \"count\": \"34\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"thriller\",\n",
      "      \"count\": \"21\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"mystery\",\n",
      "      \"count\": \"18\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-books\",\n",
      "      \"count\": \"13\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2017-books\",\n",
      "      \"count\": \"12\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"suspense\",\n",
      "      \"count\": \"12\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"netgalley\",\n",
      "      \"count\": \"11\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read-in-2017\",\n",
      "      \"count\": \"10\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"mystery-thriller\",\n",
      "      \"count\": \"10\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"books-i-own\",\n",
      "      \"count\": \"10\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ebook\",\n",
      "      \"count\": \"10\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"apocalyptic\",\n",
      "      \"count\": \"9\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"owned\",\n",
      "      \"count\": \"9\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read-2017\",\n",
      "      \"count\": \"8\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"post-apocalyptic\",\n",
      "      \"count\": \"8\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"dystopian\",\n",
      "      \"count\": \"7\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"horror\",\n",
      "      \"count\": \"7\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"e-book\",\n",
      "      \"count\": \"7\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ebooks\",\n",
      "      \"count\": \"7\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"mystery-suspense\",\n",
      "      \"count\": \"6\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"freebie\",\n",
      "      \"count\": \"6\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"e-books\",\n",
      "      \"count\": \"6\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-kindle\",\n",
      "      \"count\": \"6\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"giveaways\",\n",
      "      \"count\": \"5\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"dystopia\",\n",
      "      \"count\": \"5\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"dnf\",\n",
      "      \"count\": \"5\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"on-kindle\",\n",
      "      \"count\": \"5\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"amazon\",\n",
      "      \"count\": \"5\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"did-not-finish\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"unfinished\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"abandoned\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"maybe\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"audible\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"owned-kindle\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-unlimited\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"crime-thriller\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"purchased\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-tbr\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"on-my-kindle\",\n",
      "      \"count\": \"4\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"library\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"mystery-thriller-suspense\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-freebies\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-on-kindle\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2017-read\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-to-read\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"mine\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"survival\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"book-club\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ebooks-i-own\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"i-own\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"literary-fiction\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"to-read-kindle\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-first-reads\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-first-books\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-ebook\",\n",
      "      \"count\": \"3\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ebooks-to-read\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"have\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"action-thriller\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2017-releases\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-but-not-read\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"womens-fiction\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"written-by-women\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"new-not-yet-released\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read-soon\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"scifi-fantasy-horror\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"post-apocalyptic-fiction\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"amazon-kindle-first\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"to-read-next\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"own-e-book\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"sci-fi\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"2017-books-read\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"to-read-have\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"never-finished\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"novels\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"england\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ebook-owned\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"purchased-to-read\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ebooks-own\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"suspense-thriller\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"not-read\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"unavailable\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"thriller-suspense\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"contemporary-fiction\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"jo-furniss\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"didn-t-finish\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"adventure\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"on-deck\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"first-reads\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"free\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"to-read-owned\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindlefirst\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"mystery-suspense-thriller\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"kindle-ebooks\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"tbr-kindle\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"not-lendable\",\n",
      "      \"count\": \"2\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"to-read-own-kindle\",\n",
      "      \"count\": \"2\"\n",
      "    }\n",
      "  ],\n",
      "  \"edition_information\": \"\",\n",
      "  \"isbn\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, 'r') as f:\n",
    "    # Read and print the first 2 JSON objects (one per line)\n",
    "    for i in range(2):\n",
    "        line = f.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        obj = json.loads(line)\n",
    "        print(json.dumps(obj, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***goodreads_meta_books.json* Characterization Table**\n",
    "\n",
    "| Column Name           | Data Type         | Description                                                                                                         | Example Value                                                                                                                                                          |\n",
    "|-----------------------|-------------------|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| title_without_series  | TEXT            | Book title without series info                                                                                      | The Way of Kings (The Stormlight Archive, #1)                                                                                                                          |\n",
    "| title                 | TEXT            | Full book title                                                                                                     | The Way of Kings (The Stormlight Archive, #1)                                                                                                                          |\n",
    "| work_id               | NUMERIC               | Identifier for the work (across editions)                                                                    | 8134945                                                                                                                                                                |\n",
    "| book_id               | NUMERIC               | Unique identifier for this specific edition                                                                         | 9647295                                                                                                                                                                |\n",
    "| publication_year      | NUMERIC              | Year the book was published                                                                                          | 2011                                                                                                                                                                   |\n",
    "| num_pages             | NUMERIC               | Number of pages in the book                                                                                          | 1258                                                                                                                                                                   |\n",
    "| ratings_count         | NUMERIC               | Total number of ratings the book has received                                                                        | 3114                                                                                                                                                                   |\n",
    "| kindle_asin           | TEXT            | ASIN for the Kindle version (if applicable)                                                                          | B003P2WO5E                                                                                                                                                             |\n",
    "| publisher             | TEXT            | Name of the publisher                                                                                                | Tom Doherty                                                                                                                                                            |\n",
    "| authors               | ARRAY<STRUCT>     | Array of author objects; each contains fields like \"role\" and \"author_id\"                                             | `[{'role': '', 'author_id': '38550'}]`                                                                                                                                  |\n",
    "| format                | TEXT            | Book format (e.g., Paperback, Hardcover)                                                                             | Mass Market Paperback                                                                                                                                                  |\n",
    "| country_code          | TEXT            | Country code of publication                                                                                          | US                                                                                                                                                                     |\n",
    "| series                | ARRAY             | List of series IDs (empty array if none)                                                                             | `['178728', '675258']`                                                                                                                                                 |\n",
    "| average_rating        | NUMERIC             | Average rating of the book                                                                                           | 4.64                                                                                                                                                                   |\n",
    "| similar_books         | ARRAY<NUMERIC>        | List of similar book IDs                                                                                             | `[6736971, 10790277, 55398, 12499290, 1166599, 2315892, 15790883, 133664, 8752885, 2890090]`                                                                           |\n",
    "| image_url             | TEXT            | URL for the book's cover image                                                                                       | `https://images.gr-assets.com/books/1436456720m/9647295.jpg`                                                                                                           |\n",
    "| isbn13                | TEXT            | ISBN-13 identifier                                                                                                   | 9780765365279                                                                                                                                                          |\n",
    "| is_ebook              | CATEGORICAL           | Indicates whether the book is available as an ebook                                                                  | False                                                                                                                                                                  |\n",
    "| text_reviews_count    | NUMERIC               | Number of text reviews submitted for the book                                                                        | 561                                                                                                                                                                    |\n",
    "| language_code         | TEXT            | ISO language code for the book                                                                                       | eng                                                                                                                                                                    |\n",
    "| description           | TEXT            | Detailed description of the book                                                                                     | I long for the days before the Last Desolation. Before the Heralds abandoned us and the Knights Radiant turned against us. (truncated for brevity)                   |\n",
    "| link                  | TEXT            | Goodreads URL for the book                                                                                           | `https://www.goodreads.com/book/show/9647295-the-way-of-kings`                                                                                                         |\n",
    "| url                   | TEXT            | URL for the book (often the same as \"link\")                                                                          | `https://www.goodreads.com/book/show/9647295-the-way-of-kings`                                                                                                         |\n",
    "| asin                  | TEXT            | Amazon Standard Identification Number (if available)                                                                 | (empty)                                                                                                                                                                |\n",
    "| popular_shelves       | ARRAY<STRUCT>     | Array of shelf objects; each with a shelf \"name\" and a \"count\" indicating how many users added the book to that shelf  | `[{'name': 'to-read', 'count': '122552'}, {'name': 'currently-reading', 'count': '10145'}, ...]`                                                                    |\n",
    "| edition_information   | TEXT            | Additional edition information (often empty)                                                                         | (empty)                                                                                                                                                                |\n",
    "| isbn                  | TEXT            | Standard ISBN (may differ from ISBN-13)                                                                                | 0765365278                                                                                                                                                             |\n",
    "| publication_day       | NUMERIC             | Day of publication                                                                                                   | 24.0                                                                                                                                                                   |\n",
    "| publication_month     | NUMERIC             | Month of publication                                                                                                 | 5.0                                                                                                                                                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### b) ii) Goodreads Reviews - *goodreads_reviews_spoiler.json*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/2_3_goodreads/goodreads_reviews_filtered.json'\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 262061 entries, 0 to 262060\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   review_id         262061 non-null  object        \n",
      " 1   book_id           262061 non-null  int64         \n",
      " 2   rating            262061 non-null  int64         \n",
      " 3   review_sentences  262061 non-null  object        \n",
      " 4   has_spoiler       262061 non-null  bool          \n",
      " 5   timestamp         262061 non-null  datetime64[ns]\n",
      " 6   user_id           262061 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***goodreads_reviews_filtered.json* Characterization Table**\n",
    " \n",
    "| Column Name      | Data Type         | Description                                                                               | Example Value                                                                                                                                                                                                                                                       |\n",
    "|------------------|-------------------|-------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| review_id        | TEXT            | Unique identifier for the review                                                          | e02075d42b5d7096c3c4fa86acccf8f3                                                                                                                                                                                                                                     |\n",
    "| book_id          | NUMERIC               | Identifier of the book being reviewed                                                     | 9647295                                                                                                                                                                                                                                                              |\n",
    "| rating           | NUMERIC               | The rating given by the reviewer (typically on a scale of 1 to 5)                           | 4                                                                                                                                                                                                                                                                    |\n",
    "| review_sentences | ARRAY<.TEXT>    | List of review sentences; each element is a structure with keys like \"text\" and \"flag\"      | `[{'text': 'Satisfying, and quite the page turner.', 'flag': '0'}, {'text': 'However, a book of this length must inevitably be guilty of meandering, and meander it most certainly did.', 'flag': '0'}, …]`                                                     |\n",
    "| has_spoiler      | CATEGORICAL           | Indicates whether the review contains spoilers                                              | False                                                                                                                                                                                                                                                                |\n",
    "| timestamp        | DATE              | Date the review was posted                                                                  | 2016-01-20                                                                                                                                                                                                                                                           |\n",
    "| user_id          | TEXT            | Unique identifier for the user who submitted the review                                     | 0b0eb7f583962f6f8c5fd9e08cf27042                                                                                                                                                                                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) iii) Goodreads Reviews - goodreads_book_authors.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/2_3_goodreads/goodreads_book_authors.json'\n",
    "\n",
    "df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>name</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>author_id</th>\n",
       "      <th>average_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Vesna Velkovrh Bukilica</td>\n",
       "      <td>0</td>\n",
       "      <td>17287029</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>smyr Hydry</td>\n",
       "      <td>0</td>\n",
       "      <td>17209412</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tynrjby khmsy</td>\n",
       "      <td>0</td>\n",
       "      <td>16066657</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7174</td>\n",
       "      <td>Mariam T. Tennoe</td>\n",
       "      <td>12</td>\n",
       "      <td>13975984</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7174</td>\n",
       "      <td>Susan F. Henssonow</td>\n",
       "      <td>12</td>\n",
       "      <td>13975985</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings_count                     name  text_reviews_count  author_id  \\\n",
       "0              0  Vesna Velkovrh Bukilica                   0   17287029   \n",
       "1              0               smyr Hydry                   0   17209412   \n",
       "2              0            tynrjby khmsy                   0   16066657   \n",
       "3           7174         Mariam T. Tennoe                  12   13975984   \n",
       "4           7174       Susan F. Henssonow                  12   13975985   \n",
       "\n",
       "   average_rating  \n",
       "0            0.00  \n",
       "1            0.00  \n",
       "2            0.00  \n",
       "3            4.33  \n",
       "4            4.33  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #goodreads_book_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Goodreads - 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file was sourced from a newer dataset from the prior (obtained independently), primarly containing detailed information about the books. Detailed description for each column can be found alongside.\n",
    "\n",
    "- **goodreads_2019_2020.csv (~1.5MB)** \n",
    "\n",
    "\n",
    "There is an issue with the file's formating. Instead of only 12 fields some entries have 13. We have to look more closely to fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookID\n",
      "title\n",
      "authors\n",
      "average_rating\n",
      "isbn\n",
      "isbn13\n",
      "language_code\n",
      "  num_pages\n",
      "ratings_count\n",
      "text_reviews_count\n",
      "publication_date\n",
      "publisher\n",
      "1\n",
      "Harry Potter and the Half-Blood Prince (Harry Potter  #6)\n",
      "J.K. Rowling/Mary GrandPré\n",
      "4.57\n",
      "0439785960\n",
      "9780439785969\n",
      "eng\n",
      "652\n",
      "2095690\n",
      "27591\n",
      "9/16/2006\n",
      "Scholastic Inc.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/2_3_goodreads/goodreads_2019_2020.csv\"\n",
    "\n",
    "with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 2:\n",
    "            for _ in row:\n",
    "                print(_)\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Datasets/goodreads/goodReads_2019_2020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m./Datasets/goodreads/goodReads_2019_2020.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IPAI/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IPAI/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IPAI/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IPAI/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IPAI/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './Datasets/goodreads/goodReads_2019_2020.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins> Pandas is having issues with the format, we'll fix it in data cleaning section</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***goodreads_2019_2020.json* Characterization Table**\n",
    "\n",
    "| Column Name         | Data Type | Description                                                                                          | Example Value                                                       |\n",
    "|---------------------|-----------|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| bookID              | NUMERIC       | A unique identification number for each book (in this data set they were by incrementation)                                                     | 1                                                                    |\n",
    "| title               | TEXT    | The name under which the book was published.                                                       | Harry Potter and the Half-Blood Prince (Harry Potter  #6)            |\n",
    "| authors             | TEXT    | Names of the authors. Multiple authors are delimited by a hyphen (-) or slash (/).                   | J.K. Rowling/Mary GrandPré                                           |\n",
    "| average_rating      | NUMERIC     | The average rating the book received in total.                                                     | 4.57                                                                 |\n",
    "| isbn                | TEXT    | The International Standard Book Number (unique identifier for the book).                           | 0439785960                                                           |\n",
    "| isbn13              | TEXT    | A 13-digit ISBN used to identify the book.                                                         | 9780439785969                                                        |\n",
    "| language_code       | TEXT    | The primary language code of the book (e.g., \"eng\" for English).                                   | eng                                                                  |\n",
    "| num_pages           | NUMERIC       | Number of pages contained in the book.                                                             | 652                                                                  |\n",
    "| ratings_count       | NUMERIC       | Total number of ratings the book has received.                                                     | 2095690                                                              |\n",
    "| text_reviews_count  | NUMERIC       | Total number of written text reviews the book has received.                                        | 27591                                                                |\n",
    "| publication_date    | DATE      | The date when the book was first published. (Can be stored as a DATE or string in a consistent format.) | 2006-09-16  (or 9/16/2006)                                            |\n",
    "| publisher           | TEXT    | The name of the publisher.                                                                         | Scholastic Inc.                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)  Book‑Crossing Community\n",
    "\n",
    "This dataset is divided into 3 tables:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) i) Books - *book_crossing_Books.csv*\n",
    "\n",
    "Books are identified by their respective ISBN and invalid ISBNs were already removed from this the dataset. ***~69.9MB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISBN\n",
      "Book-Title\n",
      "Book-Author\n",
      "Year-Of-Publication\n",
      "Publisher\n",
      "Image-URL-S\n",
      "Image-URL-M\n",
      "Image-URL-L\n",
      "0195153448\n",
      "Classical Mythology\n",
      "Mark P. O. Morford\n",
      "2002\n",
      "Oxford University Press\n",
      "http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg\n",
      "http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg\n",
      "http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/4_bookcrossing/book_crossing_Books.csv\"\n",
    "\n",
    "with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 2:\n",
    "            for _ in row:\n",
    "                print(_)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***book_crossing_Books.csv* Characterization Table**\n",
    "\n",
    "| Column Name               | Data Type        | Description                                                                                         | Example Value                                                                                                                          |\n",
    "|---------------------------|------------------|-----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| ISBN                      | **TEXT**           | The International Standard Book Number that uniquely identifies the book.                         | 0590485733                                                                                                                             |\n",
    "| Book-Title                | TEXT           | The title under which the book was published.                                                     | Dog to the Rescue II: Seventeen More True Tales of Dog Heroism (Dog to the Rescue)                                                       |\n",
    "| Book-Author               | TEXT           | The name(s) of the author(s) of the book. If multiple, they are delimited (e.g., by a slash or hyphen). | Jeannette Sanderson                                                                                                                    |\n",
    "| Year-Of-Publication       | DATE   | The year the book was first published.                                                            | 1995                                                                                                                                   |\n",
    "| Publisher                 | TEXT           | The name of the publisher.                                                                          | Scholastic                                                                                                                             |\n",
    "| Image-URL-S               | TEXT           | URL for the small version of the book's cover image.                                                | http://images.amazon.com/images/P/0590485733.01.THUMBZZZ.jpg                                                                             |\n",
    "| Image-URL-M               | TEXT           | URL for the medium version of the book's cover image.                                               | http://images.amazon.com/images/P/0590485733.01.MZZZZZZZ.jpg                                                                             |\n",
    "| Image-URL-L               | TEXT           | URL for the large version of the book's cover image.                                                | http://images.amazon.com/images/P/0590485733.01.LZZZZZZZ.jpg                                                                             |\n",
    "                                                                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check what is really causing this issue!**\n",
    "\n",
    "Seems that some date are being taken as NUMERIC and others as STR. Later well convert them to unix time though. We played around with the acceptable range of valid years to see the issue.\n",
    "\n",
    "- 4621 entries/ rows have \"0\" has their 'Year-Of-Publication'.\n",
    "- 12 entries/ rows have with years ahead of 2024 has their 'Year-Of-Publication', which is implausible since this dataset was published in 2024.\n",
    "- 3 entries/ rows have an issue with their structuring it - The titles contain extra escaped quotes (\\\") and semicolons (;) that appear to be artifacts from the export process. This induced the titles to not be properly parsed during data extraction with pandas, the supposed 'Book-Author' values to be fused with \"Book-Title\" values, and consequently the other values to be moved to the incorrect column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with invalid Year-Of-Publication:\n",
      "       Year-Of-Publication Year-Of-Publication_clean  Year_numeric\n",
      "176                      0                         0           0.0\n",
      "188                      0                         0           0.0\n",
      "288                      0                         0           0.0\n",
      "351                      0                         0           0.0\n",
      "542                      0                         0           0.0\n",
      "...                    ...                       ...           ...\n",
      "270794                   0                         0           0.0\n",
      "270913                   0                         0           0.0\n",
      "271094                   0                         0           0.0\n",
      "271182                   0                         0           0.0\n",
      "271196                   0                         0           0.0\n",
      "\n",
      "[4621 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "df['Year-Of-Publication_clean'] = df['Year-Of-Publication'].astype(str).str.strip()\n",
    "df['Year_numeric'] = pd.to_numeric(df['Year-Of-Publication_clean'], errors='coerce')\n",
    "\n",
    "# Define a plausible range for valid years. Adjust min_year as needed. (some of this dates feel wrong)\n",
    "min_year = 1\n",
    "max_year = 3000\n",
    "\n",
    "invalid_years = df[\n",
    "    (df['Year_numeric'].isna()) |\n",
    "    (df['Year_numeric'] < min_year) |\n",
    "    (df['Year_numeric'] > max_year)\n",
    "]\n",
    "\n",
    "print(\"Entries with invalid Year-Of-Publication:\")\n",
    "print(invalid_years[['Year-Of-Publication', 'Year-Of-Publication_clean', 'Year_numeric']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with invalid Year-Of-Publication:\n",
      "       Year-Of-Publication Year-Of-Publication_clean  Year_numeric\n",
      "37487                 2030                      2030        2030.0\n",
      "55676                 2030                      2030        2030.0\n",
      "78168                 2030                      2030        2030.0\n",
      "80264                 2050                      2050        2050.0\n",
      "97826                 2050                      2050        2050.0\n",
      "116053                2038                      2038        2038.0\n",
      "118294                2026                      2026        2026.0\n",
      "192993                2030                      2030        2030.0\n",
      "209538   DK Publishing Inc         DK Publishing Inc           NaN\n",
      "220731           Gallimard                 Gallimard           NaN\n",
      "221678   DK Publishing Inc         DK Publishing Inc           NaN\n",
      "228173                2030                      2030        2030.0\n",
      "240169                2030                      2030        2030.0\n",
      "255409                2037                      2037        2037.0\n",
      "260974                2030                      2030        2030.0\n"
     ]
    }
   ],
   "source": [
    "df['Year-Of-Publication_clean'] = df['Year-Of-Publication'].astype(str).str.strip()\n",
    "df['Year_numeric'] = pd.to_numeric(df['Year-Of-Publication_clean'], errors='coerce')\n",
    "\n",
    "# Define a plausible range for valid years. Adjust min_year as needed. (some of this dates feel wrong)\n",
    "min_year = 0\n",
    "max_year = 2024 # this dataset was published in 2024\n",
    "\n",
    "invalid_years = df[\n",
    "    (df['Year_numeric'].isna()) |\n",
    "    (df['Year_numeric'] < min_year) |\n",
    "    (df['Year_numeric'] > max_year)\n",
    "]\n",
    "\n",
    "print(\"Entries with invalid Year-Of-Publication:\")\n",
    "print(invalid_years[['Year-Of-Publication', 'Year-Of-Publication_clean', 'Year_numeric']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209538</th>\n",
       "      <td>078946697X</td>\n",
       "      <td>DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\\";Michael Teitelbaum\"</td>\n",
       "      <td>2000</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220731</th>\n",
       "      <td>2070426769</td>\n",
       "      <td>Peuple du ciel, suivi de 'Les Bergers\\\";Jean-Marie Gustave Le ClÃ?Â©zio\"</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221678</th>\n",
       "      <td>0789466953</td>\n",
       "      <td>DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\\";James Buckley\"</td>\n",
       "      <td>2000</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN  \\\n",
       "209538  078946697X   \n",
       "220731  2070426769   \n",
       "221678  0789466953   \n",
       "\n",
       "                                                                                                         Book-Title  \\\n",
       "209538         DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\\";Michael Teitelbaum\"   \n",
       "220731                                     Peuple du ciel, suivi de 'Les Bergers\\\";Jean-Marie Gustave Le ClÃ?Â©zio\"   \n",
       "221678  DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\\";James Buckley\"   \n",
       "\n",
       "       Book-Author Year-Of-Publication  \\\n",
       "209538        2000   DK Publishing Inc   \n",
       "220731        2003           Gallimard   \n",
       "221678        2000   DK Publishing Inc   \n",
       "\n",
       "                                                           Publisher  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-S  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-M  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg   \n",
       "\n",
       "       Image-URL-L Year-Of-Publication_clean  Year_numeric  \n",
       "209538         NaN         DK Publishing Inc           NaN  \n",
       "220731         NaN                 Gallimard           NaN  \n",
       "221678         NaN         DK Publishing Inc           NaN  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_rows = df.loc[[209538, 220731, 221678]]\n",
    "selected_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 209538:\n",
      "DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\\";Michael Teitelbaum\"\n",
      "\n",
      "Index 220731:\n",
      "Peuple du ciel, suivi de 'Les Bergers\\\";Jean-Marie Gustave Le ClÃ?Â©zio\"\n",
      "\n",
      "Index 221678:\n",
      "DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\\";James Buckley\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_titles = df.loc[[209538, 220731, 221678], 'Book-Title']\n",
    "\n",
    "for idx, title in selected_titles.items():\n",
    "    print(f\"Index {idx}:\")\n",
    "    print(title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209538</th>\n",
       "      <td>078946697X</td>\n",
       "      <td>DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\</td>\n",
       "      <td>Michael Teitelbaum</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220731</th>\n",
       "      <td>2070426769</td>\n",
       "      <td>Peuple du ciel, suivi de 'Les Bergers\\</td>\n",
       "      <td>Jean-Marie Gustave Le ClÃ?Â©zio</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221678</th>\n",
       "      <td>0789466953</td>\n",
       "      <td>DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\</td>\n",
       "      <td>James Buckley</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN  \\\n",
       "209538  078946697X   \n",
       "220731  2070426769   \n",
       "221678  0789466953   \n",
       "\n",
       "                                                                                         Book-Title  \\\n",
       "209538              DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\   \n",
       "220731                                                       Peuple du ciel, suivi de 'Les Bergers\\   \n",
       "221678  DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\   \n",
       "\n",
       "                            Book-Author Year-Of-Publication  \\\n",
       "209538               Michael Teitelbaum   DK Publishing Inc   \n",
       "220731  Jean-Marie Gustave Le ClÃ?Â©zio           Gallimard   \n",
       "221678                    James Buckley   DK Publishing Inc   \n",
       "\n",
       "                                                           Publisher  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-S  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-M  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg   \n",
       "\n",
       "       Image-URL-L Year-Of-Publication_clean  Year_numeric  \n",
       "209538         NaN         DK Publishing Inc           NaN  \n",
       "220731         NaN                 Gallimard           NaN  \n",
       "221678         NaN         DK Publishing Inc           NaN  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set display option to avoid truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Define a function to fix the fused Book-Title field.\n",
    "# It expects the problematic pattern '\";' (an escaped quote followed by a semicolon)\n",
    "def fix_title_and_author(fused_value):\n",
    "    if '\";' in fused_value:\n",
    "        # Split on the problematic pattern\n",
    "        parts = fused_value.split('\";')\n",
    "        # Remove extra quotes and whitespace from both parts\n",
    "        title = parts[0].replace('\"', '').strip()\n",
    "        author = parts[1].replace('\"', '').strip() if len(parts) > 1 else ''\n",
    "        return pd.Series([title, author])\n",
    "    else:\n",
    "        return pd.Series([fused_value, None])\n",
    "\n",
    "# Define the ISBNs of the rows to fix\n",
    "problematic_isbns = [\"078946697X\", \"2070426769\", \"0789466953\"]\n",
    "\n",
    "# Create a CATEGORICAL mask for rows with these ISBNs\n",
    "mask = df['ISBN'].isin(problematic_isbns)\n",
    "\n",
    "# Apply the fix to the \"Book-Title\" column for the problematic rows\n",
    "# The function returns a Series with [corrected title, extracted author]\n",
    "fixed = df.loc[mask, 'Book-Title'].apply(fix_title_and_author)\n",
    "fixed.columns = ['Book-Title', 'Book-Author']  # Name the new columns\n",
    "\n",
    "# Update the original DataFrame with the fixed values\n",
    "df.loc[mask, ['Book-Title', 'Book-Author']] = fixed\n",
    "\n",
    "# Verify the results by printing the problematic rows (showing ISBN, Book-Title, and Book-Author)\n",
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest will be left to the data cleaning section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The other issues will be left to the data cleaning section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ISBN                                                                          Book-Title          Book-Author Year-Of-Publication   Publisher                                                   Image-URL-S                                                   Image-URL-M                                                   Image-URL-L Year-Of-Publication_clean  Year_numeric\n",
      "53897  0590485733  Dog to the Rescue II: Seventeen More True Tales of Dog Heroism (Dog to the Rescue)  Jeannette Sanderson                1995  Scholastic  http://images.amazon.com/images/P/0590485733.01.THUMBZZZ.jpg  http://images.amazon.com/images/P/0590485733.01.MZZZZZZZ.jpg  http://images.amazon.com/images/P/0590485733.01.LZZZZZZZ.jpg                      1995        1995.0\n"
     ]
    }
   ],
   "source": [
    "example = df[df['Book-Author'].str.contains(\"Sanderson\", case=False, na=False)]\n",
    "print(example.head(1).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) ii) Ratings - *book_crossing_Ratings.csv*\n",
    "\n",
    "Ratings (Book-Rating) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation). Ratings can also be expressed implicitly by 0, meaning user has interest (clicked on the book's link ) and there is no rating data yet rather than a user intentionally giving a bad score.\n",
    "\n",
    "AKA - \"0\" shows user didn't vote the book, but interacted with it in some way.\n",
    "\n",
    "***~21.6MB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   User-ID      1149780 non-null  int64 \n",
      " 1   ISBN         1149780 non-null  object\n",
      " 2   Book-Rating  1149780 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/4_bookcrossing/book_crossing_Ratings.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User-ID         int64\n",
       "ISBN           object\n",
       "Book-Rating     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating\n",
       "0   276725  034545104X            0\n",
       "1   276726  0155061224            5\n",
       "2   276727  0446520802            0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***book_crossing_Ratings.csv* Characterization Table**\n",
    "\n",
    "| Column Name  | Data Type | Description                                                           | Example Value |\n",
    "|--------------|-----------|-----------------------------------------------------------------------|---------------|\n",
    "| User-ID      | NUMERIC       | A unique identifier for the user who submitted the rating.           | 276725        |\n",
    "| ISBN         | TEXT    | The International Standard Book Number for the book being rated.     | 034545104X    |\n",
    "| Book-Rating  | NUMERIC       | The rating given by the user. | 0             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) iii) Users - *book_crossing_Users.csv*\n",
    "\n",
    "Contains the users. Note that user IDs (User-ID) have been anonymized and map to integers (still correspond to their conterparts the other tables). \n",
    "\n",
    "Demographic data is provided (Location, Age) if available. Otherwise, these fields contain NULL-values.\n",
    "\n",
    "\n",
    "***~10.5MB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   User-ID      1149780 non-null  int64 \n",
      " 1   ISBN         1149780 non-null  object\n",
      " 2   Book-Rating  1149780 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/4_bookcrossing/book_crossing_Users.csv\"\n",
    "\n",
    "df_users = pd.read_csv(file_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***book_crossing_Users.csv* Characterization Table**\n",
    "\n",
    "| Column Name | Data Type | Description                                                                 | Example Value                         |\n",
    "|-------------|-----------|-----------------------------------------------------------------------------|--------------------------------------|\n",
    "| User-ID     | NUMERIC       | A unique identifier for each user.                                          | 1                                    |\n",
    "| Location    | TEXT    | The user’s location, typically in the format \"city, state/region, country\". | nyc, new york, usa                   |\n",
    "| Age         | NUMERIC     | The age of the user. Can be missing (NaN) or imprecise; often needs cleaning. | 18.0             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Books Sales and Ratings\n",
    "\n",
    "This dataset has one file - ***Books_Data_Clean.csv ~158.3KB*** \n",
    "\n",
    "Ignored index column, it's probably just a byproduct of the data produces exporting it with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1070 entries, 0 to 1069\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   index                1070 non-null   int64  \n",
      " 1   Publishing Year      1069 non-null   float64\n",
      " 2   Book Name            1047 non-null   object \n",
      " 3   Author               1070 non-null   object \n",
      " 4   language_code        1017 non-null   object \n",
      " 5   Author_Rating        1070 non-null   object \n",
      " 6   Book_average_rating  1070 non-null   float64\n",
      " 7   Book_ratings_count   1070 non-null   int64  \n",
      " 8   genre                1070 non-null   object \n",
      " 9   gross sales          1070 non-null   float64\n",
      " 10  publisher revenue    1070 non-null   float64\n",
      " 11  sale price           1070 non-null   float64\n",
      " 12  sales rank           1070 non-null   int64  \n",
      " 13  Publisher            1070 non-null   object \n",
      " 14  units sold           1070 non-null   int64  \n",
      "dtypes: float64(5), int64(4), object(6)\n",
      "memory usage: 125.5+ KB\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/5_sales_N_ratings/Books_Data_Clean.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Publishing Year</th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>language_code</th>\n",
       "      <th>Author_Rating</th>\n",
       "      <th>Book_average_rating</th>\n",
       "      <th>Book_ratings_count</th>\n",
       "      <th>genre</th>\n",
       "      <th>gross sales</th>\n",
       "      <th>publisher revenue</th>\n",
       "      <th>sale price</th>\n",
       "      <th>sales rank</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>units sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>Beowulf</td>\n",
       "      <td>Unknown, Seamus Heaney</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Novice</td>\n",
       "      <td>3.42</td>\n",
       "      <td>155903</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>34160.00</td>\n",
       "      <td>20496.000</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Batman: Year One</td>\n",
       "      <td>Frank Miller, David Mazzucchelli, Richmond Lewis, Dennis O'Neil</td>\n",
       "      <td>eng</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>4.23</td>\n",
       "      <td>145267</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>12437.50</td>\n",
       "      <td>7462.500</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Go Set a Watchman</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>eng</td>\n",
       "      <td>Novice</td>\n",
       "      <td>3.31</td>\n",
       "      <td>138669</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>47795.00</td>\n",
       "      <td>28677.000</td>\n",
       "      <td>8.69</td>\n",
       "      <td>3</td>\n",
       "      <td>Amazon Digital Services,  Inc.</td>\n",
       "      <td>5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>When You Are Engulfed in Flames</td>\n",
       "      <td>David Sedaris</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>4.04</td>\n",
       "      <td>150898</td>\n",
       "      <td>fiction</td>\n",
       "      <td>41250.00</td>\n",
       "      <td>24750.000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>3</td>\n",
       "      <td>Hachette Book Group</td>\n",
       "      <td>5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Daughter of Smoke &amp; Bone</td>\n",
       "      <td>Laini Taylor</td>\n",
       "      <td>eng</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>4.04</td>\n",
       "      <td>198283</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>37952.50</td>\n",
       "      <td>22771.500</td>\n",
       "      <td>7.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Penguin Group (USA) LLC</td>\n",
       "      <td>4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1065</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Gray Mountain</td>\n",
       "      <td>John Grisham</td>\n",
       "      <td>eng</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>3.52</td>\n",
       "      <td>37379</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>104.94</td>\n",
       "      <td>62.964</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1268</td>\n",
       "      <td>Amazon Digital Services,  Inc.</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1066</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>The Power of One</td>\n",
       "      <td>Bryce Courtenay</td>\n",
       "      <td>eng</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4.34</td>\n",
       "      <td>57312</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>846.94</td>\n",
       "      <td>508.164</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1270</td>\n",
       "      <td>Random House LLC</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1067</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>The Maltese Falcon</td>\n",
       "      <td>Dashiell Hammett</td>\n",
       "      <td>eng</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>3.92</td>\n",
       "      <td>58742</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>846.94</td>\n",
       "      <td>508.164</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1271</td>\n",
       "      <td>Hachette Book Group</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1068</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Night Road</td>\n",
       "      <td>Kristin Hannah</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4.17</td>\n",
       "      <td>58028</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>104.94</td>\n",
       "      <td>62.964</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1272</td>\n",
       "      <td>Amazon Digital Services,  Inc.</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1069</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Tripwire</td>\n",
       "      <td>Lee Child</td>\n",
       "      <td>eng</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4.07</td>\n",
       "      <td>55251</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>316.94</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1273</td>\n",
       "      <td>Amazon Digital Services,  Inc.</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Publishing Year                        Book Name  \\\n",
       "0         0           1975.0                          Beowulf   \n",
       "1         1           1987.0                 Batman: Year One   \n",
       "2         2           2015.0                Go Set a Watchman   \n",
       "3         3           2008.0  When You Are Engulfed in Flames   \n",
       "4         4           2011.0         Daughter of Smoke & Bone   \n",
       "...     ...              ...                              ...   \n",
       "1065   1065           2014.0                    Gray Mountain   \n",
       "1066   1066           1989.0                 The Power of One   \n",
       "1067   1067           1930.0               The Maltese Falcon   \n",
       "1068   1068           2011.0                       Night Road   \n",
       "1069   1069           1999.0                         Tripwire   \n",
       "\n",
       "                                                               Author  \\\n",
       "0                                              Unknown, Seamus Heaney   \n",
       "1     Frank Miller, David Mazzucchelli, Richmond Lewis, Dennis O'Neil   \n",
       "2                                                          Harper Lee   \n",
       "3                                                       David Sedaris   \n",
       "4                                                        Laini Taylor   \n",
       "...                                                               ...   \n",
       "1065                                                     John Grisham   \n",
       "1066                                                  Bryce Courtenay   \n",
       "1067                                                 Dashiell Hammett   \n",
       "1068                                                   Kristin Hannah   \n",
       "1069                                                        Lee Child   \n",
       "\n",
       "     language_code Author_Rating  Book_average_rating  Book_ratings_count  \\\n",
       "0            en-US        Novice                 3.42              155903   \n",
       "1              eng  Intermediate                 4.23              145267   \n",
       "2              eng        Novice                 3.31              138669   \n",
       "3            en-US  Intermediate                 4.04              150898   \n",
       "4              eng  Intermediate                 4.04              198283   \n",
       "...            ...           ...                  ...                 ...   \n",
       "1065           eng  Intermediate                 3.52               37379   \n",
       "1066           eng     Excellent                 4.34               57312   \n",
       "1067           eng  Intermediate                 3.92               58742   \n",
       "1068         en-US     Excellent                 4.17               58028   \n",
       "1069           eng     Excellent                 4.07               55251   \n",
       "\n",
       "              genre  gross sales  publisher revenue  sale price  sales rank  \\\n",
       "0     genre fiction     34160.00          20496.000        4.88           1   \n",
       "1     genre fiction     12437.50           7462.500        1.99           2   \n",
       "2     genre fiction     47795.00          28677.000        8.69           3   \n",
       "3           fiction     41250.00          24750.000        7.50           3   \n",
       "4     genre fiction     37952.50          22771.500        7.99           4   \n",
       "...             ...          ...                ...         ...         ...   \n",
       "1065     nonfiction       104.94             62.964        0.99        1268   \n",
       "1066  genre fiction       846.94            508.164        7.99        1270   \n",
       "1067  genre fiction       846.94            508.164        7.99        1271   \n",
       "1068  genre fiction       104.94             62.964        0.99        1272   \n",
       "1069  genre fiction       316.94              0.000        2.99        1273   \n",
       "\n",
       "                          Publisher   units sold  \n",
       "0           HarperCollins Publishers        7000  \n",
       "1           HarperCollins Publishers        6250  \n",
       "2     Amazon Digital Services,  Inc.        5500  \n",
       "3                Hachette Book Group        5500  \n",
       "4            Penguin Group (USA) LLC        4750  \n",
       "...                              ...         ...  \n",
       "1065  Amazon Digital Services,  Inc.         106  \n",
       "1066                Random House LLC         106  \n",
       "1067             Hachette Book Group         106  \n",
       "1068  Amazon Digital Services,  Inc.         106  \n",
       "1069  Amazon Digital Services,  Inc.         106  \n",
       "\n",
       "[1070 rows x 15 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name           | Data Type      | Description                                                                                                  | Example Value                     |\n",
    "|-----------------------|----------------|--------------------------------------------------------------------------------------------------------------|-----------------------------------|\n",
    "| Publishing Year       | NUMERIC | The year in which the book was published.                                                                  | 1975.0                            |\n",
    "| Book Name             | TEXT         | The title of the book.                                                                                       | Beowulf                           |\n",
    "| Author                | TEXT         | The name(s) of the author(s) of the book.                                                                    | Unknown, Seamus Heaney            |\n",
    "| language_code         | TEXT         | The code representing the language in which the book is written.                                             | en-US                             |\n",
    "| Author_Rating         | CATEGORICAL | The rating of the author based on their works (Novice, Intermediate, Excellent or Famous  ).      | Novice                            |\n",
    "| Book_average_rating   | NUMERIC          | The average rating given to the book by readers.                                                             | 3.42                              |\n",
    "| Book_ratings_count    | NUMERIC            | The total number of ratings the book received.                                                               | 155903                            |\n",
    "| genre                 | TEXT         | The genre or category of the book.                                                                           | genre fiction                     |\n",
    "| gross sales           | NUMERIC          | The total sales revenue generated by the book.                                                               | 34160.0                           |\n",
    "| publisher revenue     | NUMERIC          | The revenue earned by the publisher from selling the book.                                                   | 20496.0                           |\n",
    "| sale price            | NUMERIC          | The price at which the book was sold.                                                                        | 4.88                              |\n",
    "| sales rank            | NUMERIC            | The rank of the book based on its sales performance.                                                         | 1                                 |\n",
    "| Publisher             | TEXT         | The name of the publisher.                                                                                     | HarperCollins Publishers          |\n",
    "| units sold            | NUMERIC            | The number of units sold for the book.                                                                       | 7000                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author_Rating\n",
      "Intermediate    625\n",
      "Excellent       362\n",
      "Famous           53\n",
      "Novice           30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Author_Rating'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) i) Amazon Metadata \n",
    "***amazon_com_extras.csv ~7.8MB*** \n",
    "\n",
    "This file contains detailed information on 63.743 books (different formats are different entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIN</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>PUBLISHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1250150183</td>\n",
       "      <td>book</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>The Swamp: Washington's Murky Pool of Corruption and Cronyism and How Trump Can Drain It</td>\n",
       "      <td>Eric Bolling</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0778319997</td>\n",
       "      <td>book</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>Rise and Shine, Benedict Stone: A Novel</td>\n",
       "      <td>Phaedra Patrick</td>\n",
       "      <td>Park Row Books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ASIN GROUP     FORMAT  \\\n",
       "0  1250150183  book  hardcover   \n",
       "1  0778319997  book  hardcover   \n",
       "\n",
       "                                                                                      TITLE  \\\n",
       "0  The Swamp: Washington's Murky Pool of Corruption and Cronyism and How Trump Can Drain It   \n",
       "1                                                   Rise and Shine, Benedict Stone: A Novel   \n",
       "\n",
       "            AUTHOR           PUBLISHER  \n",
       "0     Eric Bolling  St. Martin's Press  \n",
       "1  Phaedra Patrick      Park Row Books  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./Datasets/6_ranks_print_kindle/amazon_com_extras.csv\"\n",
    "\n",
    "df_ACE = pd.read_csv(file_path, on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "df_ACE.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name | Data Type | Description                                                               | Example Value                                      |\n",
    "|-------------|-----------|---------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| ASIN        | TEXT   | Amazon Standard Identification Number; a unique product identifier        | 1250150183                                         |\n",
    "| GROUP       | TEXT      | Product category or group                                                 | book                                               |\n",
    "| FORMAT      | TEXT      | Format of the product (e.g., book, paperback, kindle,...)                 | hardcover                                          |\n",
    "| TITLE       | TEXT      | Title of the product                                                      | The Swamp: Washington's Murky Pool of Corruption and Cronyism and How Trump Can Drain It  |\n",
    "| AUTHOR      | TEXT      | Author(s) of the product                                                  | Eric Bolling                                       |\n",
    "| PUBLISHER   | TEXT      | Publisher of the product                                                  | St. Martin's Press                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's view the bad line!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad line (line 999):\n",
      "\"0062067656\",\"book\",\"hardcover\",\"Power & Beauty: A Love Story of Life on the Streets\",\"Tip \"T.I.\" Harris, David Ritz\",\"William Morrow\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/6_ranks_print_kindle/amazon_com_extras.csv\"\n",
    "bad_line_number = 808\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file, start=1):\n",
    "        if i == bad_line_number:\n",
    "            print(f\"Bad line (line {bad_line_number}):\")\n",
    "            print(line)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found faulty line. Replacing it with the corrected version.\n",
      "File update complete.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/6_ranks_print_kindle/amazon_com_extras.csv\"\n",
    "\n",
    "# quickly id the bad line\n",
    "faulty_substr = 'Girl in Glass: How My \"Distressed Baby\"'\n",
    "\n",
    "fixed_line = (\n",
    "    '\"1620409917\",\"book\",\"hardcover\",'\n",
    "    '\"Girl in Glass: How My \"\"Distressed Baby\"\" Defied the Odds, Shamed a CEO, and Taught Me the Essence of Love, Heartbreak, and Miracles\",'\n",
    "    '\"Deanna Fei\",\"Bloomsbury USA\"\\n'\n",
    ")\n",
    "\n",
    "# wasnt utf-8 for some reason\n",
    "with open(file_path, 'r', encoding='latin-1') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if faulty_substr in line:\n",
    "        print(\"Found faulty line. Replacing it with the corrected version.\")\n",
    "        new_lines.append(fixed_line)\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "\n",
    "with open(file_path, 'w', encoding='latin-1') as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "print(\"File update complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THIS PART SHOULD GO TO DATA CLEANING PART Script to detect bad lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No similar errors found.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = \"./Datasets/6_ranks_print_kindle/amazon_com_extras.csv\"\n",
    "expected_columns = 6 \n",
    "\n",
    "bad_lines = []\n",
    "\n",
    "# Using latin-1 encoding since there were encoding issues with utf-8\n",
    "with open(file_path, 'r', encoding='latin-1') as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        try:\n",
    "            reader = csv.reader([line])\n",
    "            row = next(reader)\n",
    "            if len(row) != expected_columns:\n",
    "                bad_lines.append((line_number, line, f\"{len(row)} columns found\"))\n",
    "        except Exception as e:\n",
    "            bad_lines.append((line_number, line, f\"Error: {e}\"))\n",
    "\n",
    "if bad_lines:\n",
    "    print(\"Found lines with potential errors:\")\n",
    "    for line_number, content, info in bad_lines:\n",
    "        print(f\"Line {line_number} ({info}): {content.strip()}\")\n",
    "else:\n",
    "    print(\"No similar errors found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix all the bad lines manually just to open the doc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created as: ./Datasets/6_ranks_print_kindle/amazon_com_extras.csv.bak\n",
      "File update complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"./Datasets/6_ranks_print_kindle/amazon_com_extras.csv\"\n",
    "backup_path = file_path + \".bak\"\n",
    "\n",
    "if not os.path.exists(backup_path):\n",
    "    os.rename(file_path, backup_path)\n",
    "    os.rename(backup_path, file_path) \n",
    "    os.system(f'cp \"{file_path}\" \"{backup_path}\"')\n",
    "print(\"Backup created as:\", backup_path)\n",
    "\n",
    "# Define corrections. For multi‐line issues, the key will be the first line number \n",
    "# and the value is the complete corrected record (with a trailing newline).\n",
    "# (All corrected lines are assumed to have exactly 6 columns.)\n",
    "corrections = {\n",
    "    999:  '\"0062067656\",\"book\",\"hardcover\",\"Power & Beauty: A Love Story of Life on the Streets\",\"Tip \"\"T.I.\"\" Harris, David Ritz\",\"William Morrow\"\\n',\n",
    "    1693: '\"1498756301\",\"book\",\"hardcover\",\"\"\"Faster, Better, Cheaper\"\" in the History of Manufacturing: From the Stone Age to Lean Manufacturing and Beyond\",\"Christoph Roser\",\"Productivity Press\"\\n',\n",
    "    2391: '\"0723568421\",\"book\",\"hardcover\",\"\"\"Pet Shop Boys\"\", Annually\",\"Chris Heath\",\"TBS The Book Service Ltd\"\\n',\n",
    "    2486: '\"1101985879\",\"book\",\"hardcover\",\"The Awkward Thoughts of W. Kamau Bell: Tales of a 6\\' 4\"\" African American, Heterosexual, Cisgender, Left-Leaning, Asthmatic, Black and Proud Blerd, Mama\\'s Boy, Dad, and Stand-Up Comedian\",\"W. Kamau Bell\",\"Dutton\"\\n',\n",
    "    3021: '\"1439103186\",\"book\",\"hardcover\",\"My Infamous Life: The Autobiography of Mobb Deep\\'s Prodigy\",\"Albert \"\"Prodigy\"\" Johnson, Laura Checkoway\",\"Touchstone\"\\n',\n",
    "    5025: '\"022651210X\",\"book\",\"hardcover\",\"Toward \"\"Natural Right and History\"\": Lectures and Essays by Leo Strauss, 1937â1946\",\"Leo Strauss, J. A. Colen, Svetozar Minkov\",\"University Of Chicago Press\"\\n',\n",
    "    7090: '\"1772262153\",\"book\",\"hardcover\",\"The Book of Military Strategy: Sun Tzu\\'s \"\"The Art of War,\"\" Machiavelli\\'s \"\"The Prince,\"\" and Clausewitz\\'s \"\"On War\"\" (Annotated) (1000 Copy Limited Edition)\",\"Sun Tzu, NiccolÃ² Machiavelli, Carl von Clausewitz\",\"Engage Books\"\\n',\n",
    "    7386: '\"1947856197\",\"book\",\"hardcover\",\"Easy For You To Say\",\"Stuttering\"\" John Melendez, Jay Leno\",\"Rare Bird Books, A Vireo Book\"\\n',\n",
    "    # Merge lines 7841-7843 into one corrected record:\n",
    "    7841: ('\"1557667667\",\"book\",\"hardcover\",\"Medical Care for Children & Adults With Developmental Disabilities\",'\n",
    "           '\"I. Rubin M.D., Allen Crocker M.D., David Satcher \"\"M.D.  Ph.D.\"\", Randall Alexander \"\"M.D.  Ph.D.\"\", '\n",
    "           'Deborah Allen Sc.D., Norberto Alvarez M.D., Jack Arbiser M.D., Linda Barnes \"\"Ph.D.  M.A.\"\", '\n",
    "           'Stuart Bauer M.D., Joan Beasley Ph.D., Lauren Berman \"\"M.S.W.\"\"\",\"Paul H Brookes Pub Co\"\\n'),\n",
    "    # Merge lines 15865-15871 into one corrected record:\n",
    "    15865: ('\"B00997YJZM\",\"kindle\",\"kindle edition\",\"Dating and the Single Parent: * Are You Ready to Date? * Talking With the Kids '\n",
    "           '* Avoiding a Big Mistake * Finding Lasting Love\",\"Ron L. Deal, Dennis Rainey\",\"Bethany House Publishers\"\\n'),\n",
    "    16638: '\"B07C7WR152\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Witch and the Futa Succubus\"\": a Futanari, Futa on Female Erotic Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    17130: '\"B07DB6BYZ8\",\"kindle\",\"kindle edition\",\"FutaWorld! Sci-Fi Futas Taken by the Alien 4: A Futanari, Futa-on-Female, Futa-on-Futa, Tentacle Monster Erotic Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    17244: '\"B00RUHHGNU\",\"kindle\",\"kindle edition\",\"CÃ³mo Construir LÃ­DERES En Redes De Mercadeo Volumen Uno: CreaciÃ³n Paso A Paso De Profesionales En MLM (Spanish Edition)\",\"Tom \"\"Big Al\"\" Schreiter, Alejandro GonzÃ¡lez LÃ³pez\",\"Fortune Network Publishing\"\\n',\n",
    "    17486: '\"B07CPJ89QF\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Futa Harem 4\"\": A Futanari, Futa on Female, Futa on Futa, Erotic Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    17689: '\"B07B25Z4LV\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Roommate: Taking Her In Public\"\" Part 4: A Futanari, Dickgirl, Futa-on-Female Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    17691: '\"B005HFQ1N8\",\"kindle\",\"kindle edition\",\"Star Griffin\",\"Michael Kurland, Dick \"\"Ditmar\"\" Jenssen, Richard A. Lupoff\",\"Ramble House\"\\n',\n",
    "    17909: '\"B07BFMRTGK\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Futa Succubus\"\": A Futanari, Futa on Female, Dickgirl, Fantasy Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    18260: '\"B00W8HCGJ6\",\"kindle\",\"kindle edition\",\"CÃ³mo Construir LÃ­deres En Redes De Mercadeo Volumen Dos: Actividades Y Lecciones Para LÃ­deres de MLM (Spanish Edition)\",\"Tom \"\"Big Al\"\" Schreiter, Alejandro GonzÃ¡lez LÃ³pez\",\"Fortune Network Publishing\"\\n',\n",
    "    19604: '\"B079S72Z4Q\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Roommate: Showing Her Off\"\" Part 3: A Futanari, Dickgirl, Futa-on-Female Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    19949: '\"B07BKXH4RD\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Best Friend\"\": A Futanari, Futa on Female Erotica Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    22320: '\"B00L2SUDNI\",\"kindle\",\"kindle edition\",\"Infidelity:  Betrayal And Broken Trust In The Marriage: How to deal with the \"\"little foxes\"\" of flirting, temptations and infidelity  in marriage and proven ways to sustain the marital relationship\",\"William Appiah, Rev. Mrs Dorothy Appiah\",\"The House Of Change\"\\n',\n",
    "    25163: '\"B079WSQ4HB\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa In Charge: Sexy New Friend\"\" Part 2: A Futanari, Dickgirl, Futa on Female Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    26323: '\"B07CCHQWVR\",\"kindle\",\"kindle edition\",\"FutaWorld! Sci-Fi \"\"Futas Taken by the Alien 2\"\": A Futanari, Futa on Futa, Tentacle Alien Erotic Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    26539: '\"B07BJ7WMS8\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Roommate: Je T\\'aime\"\" Part 5: A Futa on Female, Futanari Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    # Delete stray line 27967 (a single quote line) by mapping it to an empty string:\n",
    "    27967: '',\n",
    "    30256: '\"B07CLZPY9P\",\"kindle\",\"kindle edition\",\"FutaWorld! Sci-Fi \"\"Futas Taken by the Alien\"\" Part 3: A Futanari, Futa on Female, Futa on Futa, Tentacle Alien Erotic Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    30582: '\"B07B7BR7ZY\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Professor\"\": A Futa on Female, Futanari, Dickgirl Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    31095: '\"B07BZM99LS\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Naughty Futa Bundle\"\" - 5 Story Bundle!: 5 Stories of Futanari, Futa on Female, Transgender Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    31317: '\"B079Q476PD\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa In Charge\"\" Part 1: A Futanari, Dickgirl, Futa-on-Female Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    31425: '\"B01ES10DFI\",\"kindle\",\"kindle edition\",\"The Slime Dungeon: Book 1 (The Slime Dungeon Chronicles)\",\"Jeffrey \"\"Falcon\"\" Logue, Silvia Lew\",\"Jeffrey \"\"Falcon\"\" Logue\"\\n',\n",
    "    33283: '\"B07BRSPZ63\",\"kindle\",\"kindle edition\",\"FutaWorld! SciFi: \"\"Futas Taken By The Alien\"\": A Futanari, Futa on Futa, Alien Tentacle Erotic Story (FutaWorld! Sci-Fi)\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    33513: '\"B07C3VFVRC\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Futa Harem\"\" Part 2: A Futanari, Futa on Female, Futa on Futa, Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    # Merge lines 33730-33731:\n",
    "    33730: '\"B009AC3358\",\"kindle\",\"kindle edition\",\"Pirates (with panel zoom) - Classics Illustrated World Around Us\",\"Albert Lewis Kanter, Jr., William B. Jones\",\"Classics Illustrated\"\\n',\n",
    "    33897: '\"B07CJ9KVW5\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Futa Harem\"\" Part 3: A Futanari, Futa on Female, Futa on Futa Erotic Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    # Merge lines 34353-34354:\n",
    "    34353: '\"B009AC30TW\",\"kindle\",\"kindle edition\",\"Royal Canadian Mounted Police (with panel zoom) - Classics Illustrated Special Issue\",\"Lorenz Graham, Jr., William B. Jones\",\"Classics Illustrated\"\\n',\n",
    "    34850: ('\"B07BB2P3W6\",\"kindle\",\"kindle edition\",\"The Complete Richard Hannay: \"\"The Thirty-Nine Steps\"\"\",'\n",
    "            '\"Greenmantle\",\"Mr Standfas, John Buchan, JA\"\\n'),\n",
    "    36535: '\"B079VPH4TY\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Master\"\" COMPLETE BUNDLE : 3 Part Story of Futa on Male, Domination, Fem Dom, Futanari Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    36629: '\"B06XW2PTPC\",\"kindle\",\"kindle edition\",\"The Little Black Book of Violence: What Every Young Man Needs to Know About Fighting\",\"Lawrence A. Kane, Kris Wilder, Marc \"\"Animal\"\" MacYoung, Rory Miller, Lt. Col. John R. Finch\",\"YMAA Publication Center\"\\n',\n",
    "    36863: ('\"B00DOAHBZU\",\"kindle\",\"kindle edition\",\"Books for Kids: \"\"TERRY TREETOP AND LOST EGG\"\" (Animal story, Bedtime story, Beginner readers, '\n",
    "             'Values kids book, Rhymes, Adventure & Education, Preschool ... learn) (The Terry Treetop Series Book 1)\",\"Tali Carmi\",\"eBook-Pro\"\\n'),\n",
    "    37593: '\"B07BW91W45\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Futa Succubus\"\" Part 2: Cuckolded by The Succubus: A Futanari, Futa on Female, Transgender Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    37599: '\"B07CNSDRC3\",\"kindle\",\"kindle edition\",\"FutaWorld! Fantasy \"\"The Futa Succubus\"\" Part 3: A Futanari, Futa on Female, Fertile Erotica Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    37933: '\"B079K1737X\",\"kindle\",\"kindle edition\",\"FutaWorld!: \"\"My Futa Roommate: Ma Cherie\"\" Part 2: A Futanari, Dickgirl, Futa-on-Female Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    38161: '\"B079DTBFRK\",\"kindle\",\"kindle edition\",\"FutaWorld!: \"\"My Futa Master: Forever Her Slave\"\" Part 3: A Futanari, Dickgirl, FemDom, Futa on Male Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    38277: '\"B07969DP7N\",\"kindle\",\"kindle edition\",\"FutaWorld!: \"\"My Futa Roommate\"\" Part 1: A Futanari, Dickgirl, Futa-on-Female Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    38405: '\"B0799PHY49\",\"kindle\",\"kindle edition\",\"FutaWorld!: \"\"My Futa Teammate: Filling Her Up\"\" Part 4 - a Futanari, Dickgirl, Futa on Female, Futa on Futa Menage Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    38508: ('\"B078T9QK47\",\"kindle\",\"kindle edition\",\"FutaWorld!: \"\"My Futa Teammate, Part 2 - Making Her Mine\"\": A Futa-on-Female, Dickgirl, '\n",
    "             'Steamy Domination Erotica Short Story\",\"Angel Kitty, Angela Bellerose\",\"\"\\n'),\n",
    "    38862: '\"B078ZFC4WS\",\"kindle\",\"kindle edition\",\"FutaWorld!: \"\"My Futa Teammate: Taking Them Both\"\" Part 3: A Futa on Female, Futa on Futa, Dickgirl, Menage Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    39089: '\"B079LDY7PN\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Teammate: Taking the Team\"\" Part 5:  - A Futanari, Dickgirl, Futa-on-Female, Female on Female, Menage Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    40616: ('\"1504040732\",\"book\",\"paperback\",\"Did You Know Thatâ¦?: \"\"Revised and Expanded\"\" Edition: Surprising-But-True Facts About History, Science, '\n",
    "             'Inventions, Geography, Origins, Art, Music, and More\",\"Marko Perko\",\"Open Road Distribution\"\\n'),\n",
    "    41769: ('\"1974009084\",\"book\",\"paperback\",\"4\"\"x6\"\" Dotted Journal: Mini Small Pocket Cute Design Dot-Matrix/Dot-Grid Diary Notebook to write in, '\n",
    "             'Bullet Journaling Essential Everyday Use Workbook ... Paperback (Mini Dotted Journals) (Volume 99)\",\"Divine Stationaries\",\"CreateSpace Independent Publishing Platform\"\\n'),\n",
    "    42050: '\"0062302647\",\"book\",\"paperback\",\"Diary of a Madman: The Geto Boys, Life, Death, and the Roots of Southern Rap\",\"Brad \"\"Scarface\"\" Jordan, Benjamin Meadows Ingram\",\"Dey Street Books\"\\n',\n",
    "    43099: ('\"022603660X\",\"book\",\"paperback\",\"From Black Sox to Three-Peats: A Century of Chicago\\'s Best Sportswriting from the \"\"Tribune,\"\" '\n",
    "             '\"\"Sun-Times,\"\" and Other Newspapers\",\"Ron Rapoport\",\"University Of Chicago Press\"\\n'),\n",
    "    43882: ('\"1477117350\",\"book\",\"paperback\",\"\"\"\"\"Girl, You Ain\\'t Gonna Make It\"\"\"\",: So They Said\",\"Chloris C. Hall\",\"XLIBRIS\"\\n'),\n",
    "    46999: '\"1683225554\",\"book\",\"paperback\",\"When God Says \"\"Go\"\": Rising to Challenge and Change without Losing Your Confidence, Your Courage, or Your Cool\",\"Elizabeth Laing Thompson\",\"Shiloh Run Press\"\\n',\n",
    "    47958: ('\"1717551017\",\"book\",\"paperback\",\"Appalachian Trail Who\\'s Who on YouTube: 1977-2013 (Volume 1)\",\"Jeffrey \"\"Loner\"\" Gray, Joe \"\"Apache\"\" Brewer, '\n",
    "             'Chad Wesselman\",\"CreateSpace Independent Publishing Platform\"\\n'),\n",
    "    # Merge lines 48163-48164:\n",
    "    48163: ('\"9058565041\",\"book\",\"paperback\",\"Exploring Ikebana (Dutch and English Edition)\",\"Ilse Beunen, Stichting Kunstboak (Acc)\",\"\"\\n'),\n",
    "    48471: '\"015205085X\",\"book\",\"paperback\",\"Bloody Jack: Being an Account of the Curious Adventures of Mary \"\"Jacky\"\" Faber, Ship\\'s Boy (Bloody Jack Adventures)\",\"L. A. Meyer\",\"HMH Books for Young Readers\"\\n',\n",
    "    49293: '\"0996372415\",\"book\",\"paperback\",\"Too Tough to Love: My Life with Johnny Ramone\",\"Cynthia \"\"Roxy\"\" Whitney, emily xyz\",\"New Green Press\"\\n',\n",
    "    51355: ('\"0399534903\",\"book\",\"paperback\",\"The Alternate-Day Diet: Turn on Your \"\"Skinny Gene,\"\" Shed the Pounds, and Live a Longer and HealthierLife\",'\n",
    "             '\"James B. Johnson M.D., Donald R. Laub Sr. M.D.\",\"Perigee Trade\"\\n'),\n",
    "    51930: '\"0061944181\",\"book\",\"paperback\",\"The Black Hand: The Bloody Rise and Redemption of \"\"Boxer\"\" Enriquez, a Mexican Mob Killer\",\"Chris Blatchford\",\"William Morrow Paperbacks\"\\n',\n",
    "    53155: ('\"1598572709\",\"book\",\"paperback\",\"Equity and Full Participation for Individuals with Severe Disabilities: A Vision for the Future\",'\n",
    "             '\"Martin Agran Ph.D., Fredda Brown Ph.D., Carolyn Hughes Ph.D., Carol Quirk Ed.D., Dr. Diane Lea Ryndak Ph.D., Barbara Trader, '\n",
    "             'David L. Westling Ed.D., Christine Bigby \"\"B.A. (Hons)  M.\"\", Linda M. Bambara Ed.D., Jane Boone, Kristen Bottema-Beutel Ph.D., Matt\",'\n",
    "             '\"Brookes Publishing\"\\n'),\n",
    "    55513: '\"1683220129\",\"book\",\"paperback\",\"When God Says \"\"Wait\"\": navigating lifeâs detours and delays without losing your faith, your friends, or your mind\",\"Elizabeth Laing Thompson\",\"Shiloh Run Press\"\\n',\n",
    "    56818: ('\"1491041560\",\"book\",\"paperback\",\"The \"\"Encyclopedia\"\" of Pool Hustlers: A rowdy assortment of anecdotes, insights, encounters, '\n",
    "             'and esoteric knowledge of the legendary pool hustlers of the second half of the 20th century\",\"mr Freddy The Beard Bentivegna\",\"CreateSpace Independent Publishing Platform\"\\n'),\n",
    "    57236: ('\"1422158004\",\"book\",\"paperback\",\"HBR\\'s 10 Must Reads on Change Management (including featured article \"\"Leading Change,\"\" by John P. Kotter)\",\"Harvard Business Review, John P. Kotter, W. Chan Kim, RenÃ©e A. Mauborgne\",\"Harvard Business Review Press\"\\n'),\n",
    "    57729: '\"1425936091\",\"book\",\"paperback\",\"\"\"Is It A Child\\'s Fault, Too?\"\"\",\"Andrea Taylor\",\"AuthorHouse\"\\n',\n",
    "    58296: '\"1455501379\",\"book\",\"paperback\",\"Mo\\' Meta Blues: The World According to Questlove\",\"Ahmir \"\"Questlove\"\" Thompson, Ben Greenman\",\"Grand Central Publishing\"\\n',\n",
    "    58760: ('\"1523897988\",\"book\",\"paperback\",\"Quad Ruled Notebook 1/4 inch Squares 120 Pages: 8.5\"\"x11\"\" Quad ruled notebook with colorful garden cover, '\n",
    "             'roman grid of 4 squares per inch, perfect ... doodling, composition notebook or journal\",\"Spicy Journals\",\"CreateSpace Independent Publishing Platform\"\\n'),\n",
    "    59976: '\"0761185526\",\"book\",\"paperback\",\"Prison Ramen: Recipes and Stories from Behind Bars\",\"Clifton Collins Jr., Gustavo \"\"Goose\"\" Alvarez, Samuel L. Jackson\",\"Workman Publishing Company\"\\n',\n",
    "    60424: '\"0692789278\",\"book\",\"paperback\",\"Snow On The Barb Wire\",\"Bill \"\"El Wingador\"\" Simmons, Joe Vallee, Angelo Cataldi\",\"William T. Simmons and Joseph J. Vallee Jr.\"\\n',\n",
    "    61181: ('\"1594391297\",\"book\",\"paperback\",\"The Little Black Book of Violence: What Every Young Man Needs to Know About Fighting\",'\n",
    "             '\"Lawrence A. Kane, Kris Wilder, Lt. Col. John R. Finch, Marc \"\"Animal\"\" MacYoung, Rory Miller\",\"Ymaa Publication Center\"\\n'),\n",
    "    61300: '\"1510712305\",\"book\",\"paperback\",\"Basic Pool: The Ultimate Beginner\\'s Guide\",\"Arthur \"\"Babe\"\" Cranfield, Laurence S. Moy\",\"Skyhorse Publishing\"\\n',\n",
    "    62538: '\"1500701653\",\"book\",\"paperback\",\"The Eighth Sense?: Awareness of the Light. The Story of \"\"Pinky\"\" King - Healer, Psychic and Painter/Decorator\",\"Pippin Mole, Mark (Pinky) King\",\"CreateSpace Independent Publishing Platform\"\\n',\n",
    "    62611: '\"0062001388\",\"book\",\"paperback\",\"The WSJ Guide to the 50 Economic Indicators That Really Matter: From Big Macs to \"\"Zombie Banks,\"\" the Indicators Smart Investors Watch to Beat the Market (Wall Street Journal Guides)\",\"Simon Constable, Robert E. Wright\",\"HarperBusiness\"\\n',\n",
    "}\n",
    "\n",
    "# its in latin-1 for some reason\n",
    "with open(file_path, 'r', encoding='latin-1') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "new_lines = []\n",
    "line_num = 1\n",
    "i = 0\n",
    "total_lines = len(lines)\n",
    "while i < total_lines:\n",
    "    if line_num in corrections:\n",
    "        new_lines.append(corrections[line_num])\n",
    "        if line_num == 7841:\n",
    "            i += 3  # skip 3 lines\n",
    "            line_num += 3\n",
    "            continue\n",
    "        elif line_num == 15865:\n",
    "            i += 7\n",
    "            line_num += 7\n",
    "            continue\n",
    "        elif line_num == 33730:\n",
    "            i += 2\n",
    "            line_num += 2\n",
    "            continue\n",
    "        elif line_num == 34353:\n",
    "            i += 2\n",
    "            line_num += 2\n",
    "            continue\n",
    "        elif line_num == 48163:\n",
    "            i += 2\n",
    "            line_num += 2\n",
    "            continue\n",
    "        else:\n",
    "            i += 1\n",
    "            line_num += 1\n",
    "            continue\n",
    "    else:\n",
    "        # Otherwise, keep the original line.\n",
    "        new_lines.append(lines[i])\n",
    "        i += 1\n",
    "        line_num += 1\n",
    "\n",
    "with open(file_path, 'w', encoding='latin-1') as f:\n",
    "    f.writelines(new_lines)\n",
    "\n",
    "print(\"File update complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at: ./Datasets/6_ranks_print_kindle/amazon_com_extras.csv.bak\n",
      "File update complete.\n"
     ]
    }
   ],
   "source": [
    "# still had some weird lines\n",
    "\n",
    "file_path = \"./Datasets/6_ranks_print_kindle/amazon_com_extras.csv\"\n",
    "backup_path = file_path + \".bak\"\n",
    "\n",
    "if not os.path.exists(backup_path):\n",
    "    with open(file_path, 'rb') as original, open(backup_path, 'wb') as backup:\n",
    "        backup.write(original.read())\n",
    "print(\"Backup created at:\", backup_path)\n",
    "\n",
    "corrections = {\n",
    "    8599: '\"022637064X\",\"book\",\"hardcover\",\"But Can I Start a Sentence with \"\"But\"\"?: Advice from the Chicago Style Q&A (Chicago Guides to Writing, Editing, and Publishing)\",\"The University of Chicago Press Editorial Staff, Carol Fisher Saller\",\"University Of Chicago Press\"\\n',\n",
    "    8609: '\"1405246413\",\"book\",\"hardcover\",\"\"\"Go, Diego, Go!\"\" Annual 2010\",\"VARIOUS\",\"Egmont Books Ltd\"\\n',\n",
    "    9462: '\"1477202978\",\"book\",\"hardcover\",\"\"\"DON\\'T THANK ME, THANK YOUR RECRUITER\"\"\",\"Ken Conklin\",\"AuthorHouse\"\\n',\n",
    "    10462: '\"B07BTL75LR\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa Roommate\"\" COMPLETE BUNDLE: 5 Stories of Futanari, Futa on Female, Transgender Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n',\n",
    "    12673: '\"B01LYY7WEZ\",\"kindle\",\"kindle edition\",\"The Dungeon\\'s Town (The Slime Dungeon Chronicles Book 2)\",\"Jeffrey \"\"Falcon\"\" Logue, Silvia Lew\",\"Jeffrey \"\"Falcon\"\" Logue\"\\n',\n",
    "    13079: '\"B01H6XKVRA\",\"kindle\",\"kindle edition\",\"Der vergessene Soldat: Originaltitel \"\"Le Soldat oubliÃ©\"\", Ãbersetzung aus dem FranzÃ¶sischen (German Edition)\",\"Guy Sajer, Wolf MÃ¼ller, Frederike Keller\",\"Helios Verlag\"\\n',\n",
    "    14939: '\"B07BBNL1L4\",\"kindle\",\"kindle edition\",\"FutaWorld! \"\"My Futa In Charge: Hard at Work\"\" Part 3: A Futanari, Futa on Female, Dickgirl Erotica\",\"Angel Kitty, Angela Bellerose\",\"\"\\n'\n",
    "}\n",
    "\n",
    "with open(file_path, 'r', encoding='latin-1') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "new_lines = []\n",
    "line_num = 1\n",
    "i = 0\n",
    "total_lines = len(lines)\n",
    "\n",
    "while i < total_lines:\n",
    "    if line_num in corrections:\n",
    "        new_lines.append(corrections[line_num])\n",
    "        i += 1  \n",
    "        line_num += 1\n",
    "    else:\n",
    "        new_lines.append(lines[i])\n",
    "        i += 1\n",
    "        line_num += 1\n",
    "\n",
    "with open(file_path, 'w', encoding='latin-1') as f:\n",
    "    f.writelines(new_lines)\n",
    "\n",
    "print(\"File update complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACE = pd.read_csv(file_path, on_bad_lines='skip', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Kindle 2023 \n",
    "- ***Kindle_data.csv ~35.8MB***\n",
    "\n",
    "This file contains data for 133,102 kindle e-books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Datasets/7_kindle/kindle_data.csv\"\n",
    "\n",
    "df_kindle = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 133102 entries, 0 to 133101\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   asin               133102 non-null  object \n",
      " 1   title              133102 non-null  object \n",
      " 2   author             132677 non-null  object \n",
      " 3   soldBy             123869 non-null  object \n",
      " 4   imgUrl             133102 non-null  object \n",
      " 5   productURL         133102 non-null  object \n",
      " 6   stars              133102 non-null  float64\n",
      " 7   reviews            133102 non-null  int64  \n",
      " 8   price              133102 non-null  float64\n",
      " 9   isKindleUnlimited  133102 non-null  bool   \n",
      " 10  category_id        133102 non-null  int64  \n",
      " 11  isBestSeller       133102 non-null  bool   \n",
      " 12  isEditorsPick      133102 non-null  bool   \n",
      " 13  isGoodReadsChoice  133102 non-null  bool   \n",
      " 14  publishedDate      84086 non-null   object \n",
      " 15  category_name      133102 non-null  object \n",
      "dtypes: bool(4), float64(2), int64(2), object(8)\n",
      "memory usage: 12.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_kindle.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>soldBy</th>\n",
       "      <th>imgUrl</th>\n",
       "      <th>productURL</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "      <th>price</th>\n",
       "      <th>isKindleUnlimited</th>\n",
       "      <th>category_id</th>\n",
       "      <th>isBestSeller</th>\n",
       "      <th>isEditorsPick</th>\n",
       "      <th>isGoodReadsChoice</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00TZE87S4</td>\n",
       "      <td>Adult Children of Emotionally Immature Parents: How to Heal from Distant, Rejecting, or Self-Involved Parents</td>\n",
       "      <td>Lindsay C. Gibson</td>\n",
       "      <td>Amazon.com Services LLC</td>\n",
       "      <td>https://m.media-amazon.com/images/I/713KZTsaYpL._AC_UY218_.jpg</td>\n",
       "      <td>https://www.amazon.com/dp/B00TZE87S4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "      <td>9.99</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>Parenting &amp; Relationships</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  B00TZE87S4   \n",
       "\n",
       "                                                                                                           title  \\\n",
       "0  Adult Children of Emotionally Immature Parents: How to Heal from Distant, Rejecting, or Self-Involved Parents   \n",
       "\n",
       "              author                   soldBy  \\\n",
       "0  Lindsay C. Gibson  Amazon.com Services LLC   \n",
       "\n",
       "                                                           imgUrl  \\\n",
       "0  https://m.media-amazon.com/images/I/713KZTsaYpL._AC_UY218_.jpg   \n",
       "\n",
       "                             productURL  stars  reviews  price  \\\n",
       "0  https://www.amazon.com/dp/B00TZE87S4    4.8        0   9.99   \n",
       "\n",
       "   isKindleUnlimited  category_id  isBestSeller  isEditorsPick  \\\n",
       "0              False            6          True          False   \n",
       "\n",
       "   isGoodReadsChoice publishedDate              category_name  \n",
       "0              False    2015-06-01  Parenting & Relationships  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kindle.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name         | Data Type | Description                                                       | Example Value                                                   |\n",
    "|---------------------|-----------|-------------------------------------------------------------------|-----------------------------------------------------------------|\n",
    "| asin                | TEXT      | Amazon Standard Identification Number (unique product code)       | B00TZE87S4                                                      |\n",
    "| title               | TEXT      | Title of the product or book                                       | Adult Children of Emotionally Immature Parents: How to Heal...   |\n",
    "| author              | TEXT      | Author(s) of the product or book                                   | Lindsay C. Gibson                                               |\n",
    "| soldBy              | TEXT      | Seller or provider of the product                                  | Amazon.com Services LLC                                         |\n",
    "| imgUrl              | TEXT      | URL of the product image                                           | https://m.media-amazon.com/images/I/713KZTsaYpL._AC_UY218_.jpg      |\n",
    "| productURL          | TEXT      | URL linking to the product detail page                             | https://www.amazon.com/dp/B00TZE87S4                              |\n",
    "| stars               | NUMERIC   | Average star rating (out of 5)                                       | 4.8                                                             |\n",
    "| reviews             | NUMERIC   | Number of reviews                                                  | 0                                                               |\n",
    "| price               | NUMERIC   | Price of the product                                               | 9.99                                                            |\n",
    "| isKindleUnlimited   | CATEGORICAL   | Whether the product is available on Kindle Unlimited               | False                                                           |\n",
    "| category_id         | NUMERIC   | Identifier for the product category                                | 6                                                               |\n",
    "| isBestSeller        | CATEGORICAL   | Whether the product is marked as a bestseller                        | True                                                            |\n",
    "| isEditorsPick       | CATEGORICAL   | Whether the product is an editor's pick                              | False                                                           |\n",
    "| isGoodReadsChoice   | CATEGORICAL   | Whether the product is a GoodReads Choice                             | False                                                           |\n",
    "| publishedDate       | DATE      | Publication or release date of the product                         | 2015-06-01                                                      |\n",
    "| category_name       | TEXT      | Name of the product category                                       | Parenting & Relationships                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Wonder Book \n",
    "- ***BooksDataset.csv ~66.5MB***\n",
    "\n",
    "This file contains details of 103,063 books of an Amazon competitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103082 entries, 0 to 103081\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   Title         103082 non-null  object\n",
      " 1   Authors       103082 non-null  object\n",
      " 2   Description   70213 non-null   object\n",
      " 3   Category      76912 non-null   object\n",
      " 4   Publisher     103074 non-null  object\n",
      " 5   Publish Date  103082 non-null  object\n",
      " 6   Price         103082 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/8_wonderbk/wonderbooks.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goat Brothers</td>\n",
       "      <td>By Colton, Larry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>History , General</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>Friday, January 1, 1993</td>\n",
       "      <td>Price Starting at $8.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Missing Person</td>\n",
       "      <td>By Grumbach, Doris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fiction , General</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>Sunday, March 1, 1981</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't Eat Your Heart Out Cookbook</td>\n",
       "      <td>By Piscatella, Joseph C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooking , Reference</td>\n",
       "      <td>Workman Pub Co</td>\n",
       "      <td>Thursday, September 1, 1983</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment</td>\n",
       "      <td>By Davis, Paul D.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natl Pr Books</td>\n",
       "      <td>Monday, April 1, 1991</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Spangler's Breastfeeding : A Parent's Guide</td>\n",
       "      <td>By Spangler, Amy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amy Spangler</td>\n",
       "      <td>Saturday, February 1, 1997</td>\n",
       "      <td>Price Starting at $5.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103077</th>\n",
       "      <td>Build 3 Super Serving Carts</td>\n",
       "      <td>By Chuck Hampton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENDesigns Inc.</td>\n",
       "      <td>Wednesday, January 1, 1992</td>\n",
       "      <td>Price Starting at $9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103078</th>\n",
       "      <td>My Land of Israel</td>\n",
       "      <td>By Nover, Elizabeth Z.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Juvenile Nonfiction , People &amp; Places , Middle East</td>\n",
       "      <td>Behrman House</td>\n",
       "      <td>Friday, May 1, 1987</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103079</th>\n",
       "      <td>Tongues: To Speak or Not to Speak</td>\n",
       "      <td>By Donald W. Burdick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moody Press</td>\n",
       "      <td>Wednesday, January 1, 1969</td>\n",
       "      <td>Price Starting at $5.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103080</th>\n",
       "      <td>If I'm in charge here why is everybody laughing?</td>\n",
       "      <td>By Campbell, David P.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Argus Communications</td>\n",
       "      <td>Tuesday, January 1, 1980</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103081</th>\n",
       "      <td>Your First Puppy (Your First Series)</td>\n",
       "      <td>By Carpentier, Marcel</td>\n",
       "      <td>An introduction to the proper care of puppies discusses different breeds, feeding, training, gro...</td>\n",
       "      <td>Pets , Dogs , General</td>\n",
       "      <td>Tfh Pubns Inc</td>\n",
       "      <td>Friday, November 1, 1991</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103082 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         Title  \\\n",
       "0                                                                                Goat Brothers   \n",
       "1                                                                           The Missing Person   \n",
       "2                                                            Don't Eat Your Heart Out Cookbook   \n",
       "3       When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment   \n",
       "4                                              Amy Spangler's Breastfeeding : A Parent's Guide   \n",
       "...                                                                                        ...   \n",
       "103077                                                             Build 3 Super Serving Carts   \n",
       "103078                                                                       My Land of Israel   \n",
       "103079                                                       Tongues: To Speak or Not to Speak   \n",
       "103080                                        If I'm in charge here why is everybody laughing?   \n",
       "103081                                                    Your First Puppy (Your First Series)   \n",
       "\n",
       "                         Authors  \\\n",
       "0               By Colton, Larry   \n",
       "1             By Grumbach, Doris   \n",
       "2       By Piscatella, Joseph C.   \n",
       "3              By Davis, Paul D.   \n",
       "4               By Spangler, Amy   \n",
       "...                          ...   \n",
       "103077          By Chuck Hampton   \n",
       "103078    By Nover, Elizabeth Z.   \n",
       "103079      By Donald W. Burdick   \n",
       "103080     By Campbell, David P.   \n",
       "103081     By Carpentier, Marcel   \n",
       "\n",
       "                                                                                                Description  \\\n",
       "0                                                                                                       NaN   \n",
       "1                                                                                                       NaN   \n",
       "2                                                                                                       NaN   \n",
       "3                                                                                                       NaN   \n",
       "4                                                                                                       NaN   \n",
       "...                                                                                                     ...   \n",
       "103077                                                                                                  NaN   \n",
       "103078                                                                                                  NaN   \n",
       "103079                                                                                                  NaN   \n",
       "103080                                                                                                  NaN   \n",
       "103081  An introduction to the proper care of puppies discusses different breeds, feeding, training, gro...   \n",
       "\n",
       "                                                    Category  \\\n",
       "0                                          History , General   \n",
       "1                                          Fiction , General   \n",
       "2                                        Cooking , Reference   \n",
       "3                                                        NaN   \n",
       "4                                                        NaN   \n",
       "...                                                      ...   \n",
       "103077                                                   NaN   \n",
       "103078   Juvenile Nonfiction , People & Places , Middle East   \n",
       "103079                                                   NaN   \n",
       "103080                                                   NaN   \n",
       "103081                                 Pets , Dogs , General   \n",
       "\n",
       "                   Publisher                 Publish Date  \\\n",
       "0                  Doubleday      Friday, January 1, 1993   \n",
       "1           Putnam Pub Group        Sunday, March 1, 1981   \n",
       "2             Workman Pub Co  Thursday, September 1, 1983   \n",
       "3              Natl Pr Books        Monday, April 1, 1991   \n",
       "4               Amy Spangler   Saturday, February 1, 1997   \n",
       "...                      ...                          ...   \n",
       "103077        ENDesigns Inc.   Wednesday, January 1, 1992   \n",
       "103078         Behrman House          Friday, May 1, 1987   \n",
       "103079           Moody Press   Wednesday, January 1, 1969   \n",
       "103080  Argus Communications     Tuesday, January 1, 1980   \n",
       "103081         Tfh Pubns Inc     Friday, November 1, 1991   \n",
       "\n",
       "                          Price  \n",
       "0       Price Starting at $8.79  \n",
       "1       Price Starting at $4.99  \n",
       "2       Price Starting at $4.99  \n",
       "3       Price Starting at $4.99  \n",
       "4       Price Starting at $5.32  \n",
       "...                         ...  \n",
       "103077  Price Starting at $9.97  \n",
       "103078  Price Starting at $4.99  \n",
       "103079  Price Starting at $5.29  \n",
       "103080  Price Starting at $4.99  \n",
       "103081  Price Starting at $4.99  \n",
       "\n",
       "[103082 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title           object\n",
       "Authors         object\n",
       "Description     object\n",
       "Category        object\n",
       "Publisher       object\n",
       "Publish Date    object\n",
       "Price           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name       | Data Type | Description                                                             | Example Value                          |\n",
    "|-------------------|-----------|-------------------------------------------------------------------------|----------------------------------------|\n",
    "| title             | TEXT      | The title of the book.                                                  | Goat Brothers                          |\n",
    "| authors           | TEXT      | The authors of the book.                                                | By Colton, Larry                       |\n",
    "| description       | TEXT      | A brief description of the book (may be null or require cleaning).       | [null]                                 |\n",
    "| category          | TEXT      | The category or genre to which the book belongs.                        | History, General                       |\n",
    "| publisher         | TEXT      | The publishing house responsible for the book.                          | Doubleday                              |\n",
    "| publication_date  | DATE      | The publication date (year and month or full date) of the book.           | 1993-01-01 (Friday, January 1, 1993)     |\n",
    "| initial_price     | NUMERIC   | The initial price of the book (price extracted from a string like \"Price Starting at $8.79\"). | 8.79                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Content Analysis, Standardization and Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Amazon Reviews’23 - Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) i) Metadata books\n",
    "\n",
    "This table has 280.338 rows and 18 columns.\n",
    "\n",
    "\"parent_asin\" is the key unique value, however:\n",
    "- On Amazon, a parent ASIN represents the general product listing (like a t-shirt), while child ASINs represent specific variations of that product (like different sizes and colors of that t-shirt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 280338 entries, 0 to 280337\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   main_category    280155 non-null  object \n",
      " 1   title            280338 non-null  object \n",
      " 2   subtitle         223708 non-null  object \n",
      " 3   average_rating   280338 non-null  float64\n",
      " 4   rating_number    280338 non-null  int64  \n",
      " 5   features         280338 non-null  object \n",
      " 6   description      280338 non-null  object \n",
      " 7   price            197032 non-null  float64\n",
      " 8   images           280338 non-null  object \n",
      " 9   videos           280338 non-null  object \n",
      " 10  store            270276 non-null  object \n",
      " 11  categories       280338 non-null  object \n",
      " 12  details          280338 non-null  object \n",
      " 13  parent_asin      280338 non-null  object \n",
      " 14  bought_together  0 non-null       float64\n",
      " 15  author_name      230005 non-null  object \n",
      " 16  author_about     230005 non-null  object \n",
      " 17  author_avatar    230008 non-null  object \n",
      "dtypes: float64(3), int64(1), object(14)\n",
      "memory usage: 38.5+ MB\n"
     ]
    }
   ],
   "source": [
    "file_path = './Datasets/1_amazon/amazon_meta_books_X.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>bought_together</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_about</th>\n",
       "      <th>author_avatar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Swan Thieves</td>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1559</td>\n",
       "      <td>[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devotion to his profession and the painting hobby he loves. This order is destroyed when renowned painter Robert Oliver attacks a canvas in the National Gallery of Art and becomes his patient. In response, Marlowe finds himself going beyond his own legal and ethical boundaries to understand the secret that torments this genius, a journey that will lead him into the lives of the women closest to Robert Oliver and toward a tragedy at the heart of French Impressionism.Ranging from American museums to the coast of Normandy, from the late nineteenth century to the late twentieth, from young love to last love, THE SWAN THIEVES is a story of obsession, the losses of history, and the power of art to preserve human hope.\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Reader),  Sarah Zimmerman (Reader),  John Lee (Reader)</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>{'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; Una Rei edition (November 3, 2010)', 'Item Weight': '1.05 pounds'}</td>\n",
       "      <td>B0062GL89I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elizabeth Kostova</td>\n",
       "      <td>[\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a lifetime of imagining--since Kostova's girlhood, when her father entertained her with tales of Dracula, she has envisioned the story that would become The Historian. With her academic spirit and extraordinary talent, she's spun an intricate tale of sprawling mystery and suspense. Kostova graduated from Yale and holds an MFA from the University of Michigan, where she won the Hopwood Award for the Novel-in-Progress.\"]</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audible Audiobooks</td>\n",
       "      <td>Death in a White Tie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher)    &amp;                   0                  more</td>\n",
       "      <td>[\"Books\",\"Mystery, Thriller &amp; Suspense\",\"Mystery\",\"Traditional Detectives\"]</td>\n",
       "      <td>{}</td>\n",
       "      <td>B001F1ZPDK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Books</td>\n",
       "      <td>Reunion Pass (Thorndike Romance)</td>\n",
       "      <td>Hardcover – Large Print, May 18, 2016</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1140</td>\n",
       "      <td>[\"A New York Times Bestselling Author An Eternity Springs Novel Six years ago, Chase Timberlake bought an engagement ring for high school sweetheart Lori Reese. Then, life happened. Chase’s career took off and Lori got into veterinary school. When Chase’s jet-setting life takes a tragic turn, he returns to Eternity Springs a damaged man. Can she and Chase turn back the hands of time and pick up where they left off ― and give forever a chance?\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily March (Author)</td>\n",
       "      <td>[\"Books\",\"Large Print\",\"Literature &amp; Fiction\"]</td>\n",
       "      <td>{'ISBN 13': '978-1410490117', 'Language': 'English', 'ISBN 10': '9781410490117', 'Dimensions': '6 x 1 x 9 inches', 'Publisher': 'Thorndike Press Large Print; Large Print edition (May 18, 2016)', 'Item Weight': '1.2 pounds', 'Hardcover': '422 pages'}</td>\n",
       "      <td>1410490114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily March</td>\n",
       "      <td>['Emily March is the New York Times, Publishers Weekly, and USA Today bestselling author of over forty novels, including the critically acclaimed Eternity Springs series. Publishers Weekly calls March a \"master of delightful banter,\" and her heartwarming, emotionally charged stories have been named to Best of the Year lists by Publishers Weekly, Library Journal, and Romance Writers of America. A graduate of Texas A&amp;M University, Emily is an avid fan of Aggie sports, and her recipe for jalapeño relish has made her a tailgating legend.']</td>\n",
       "      <td>https://m.media-amazon.com/images/S/amzn-author-media-prod/an3i0pthoa0nsmk8mnn25ndb7n._SY600_.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Books</td>\n",
       "      <td>Key Lock Man</td>\n",
       "      <td>Mass Market Paperback – January 1, 1981</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1907</td>\n",
       "      <td>[\"book 163\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis L'Amour (Author)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ISBN 13': '978-0553230888', 'Language': 'English', 'Mass Market Paperback': '0 pages', 'ISBN 10': '0553230883', 'Publisher': 'Bantam (January 1, 1981)', 'Item Weight': '3.2 ounces'}</td>\n",
       "      <td>0553230883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis L'Amour</td>\n",
       "      <td>['\"I think of myself in the oral tradition--as a troubadour, a village tale-teller, the man in the shadows of a campfire. That\\'s the way I\\'d like to be remembered--as a storyteller. A good storyteller.\"', 'It is doubtful that any author could be as at home in the world re-created in his novels as Louis Dearborn L\\'Amour. Not only could he physically fill the boots of the rugged characters he wrote about, but he literally \"walked the land my characters walk.\" His personal experiences as well as his lifelong devotion to historical research combined to give Mr. L\\'Amour the unique knowledge and understanding of people, events, and the challenge of the American frontier that became the hallmarks of his popularity.', 'Of French-Irish descent, Mr. L\\'Amour could trace his own in North America back to the early 1600s and follow their steady progression westward, \"always on the frontier.\" As a boy growing up in Jamestown, North Dakota, he absorbed all he could about his family\\'s frontier heritage, including the story of his great-grandfather who was scalped by Sioux warriors.', 'Spurred by an eager curiosity and desire to broaden his horizons, Mr. L\\'Amour left home at the age of fifteen and enjoyed a wide variety of jobs, including seaman, lumberjack, elephant handler, skinner of dead cattle, and miner, and was an officer in the transportation corps during World War II. During his \"yondering\" days he also circled the world on a freighter, sailed a dhow on the Red Sea, was shipwrecked in the West Indies and stranded in the Mojave Desert. He won fifty-one of fifty-nine fights as a professional boxer and worked as a journalist and lecturer. He was a voracious reader and collector of rare books. His personal library contained 17,000 volumes.', 'Mr. L\\'Amour \"wanted to write almost from the time I could talk.\" After developing a widespread following for his many frontiers and adventure stories written for fiction magazines, Mr. L\\'Amour published his first full length novel, Hondo, in the United States in 1953. Every one of his more than 120 books is in print; there are more than 300 million copies of his books in print worldwide, making him one of the bestselling authors in modern literary history. His books have been translated into twenty languages, and more than forty-five of his novels and stories have been made into feature films and television movies.', \"The recipient of many great honor and awards, in 1983 Mr. L'Amour became the first novelist to ever to be awarded the Congressional Gold Medal by the United States Congress in honor of his life's work. In 1984 he was also awarded the Medal of Freedom by President Reagan.\", \"Louis L'Amour died on June 10, 1988. His wife, Kathy, and their two children, Beau and Angelique, carry the L'Amour publishing tradition forward with new books written by the author during his lifetime to be published by Bantam.\"]</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31pdVqK+eZL._SY600_.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Robots of Dawn (Robot, 3)</td>\n",
       "      <td>MP3 CD – Unabridged, July 15, 2007</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5660</td>\n",
       "      <td>[\"A puzzling case of roboticide sends New York Detective Elijah Baley on an intense search for a murderer. Armed with his own instincts, his quirky logic, and the immutable Three Laws of Robotics, Baley is determined to solve the case. But can anything prepare a simple Earthman for the psychological complexities of a world where a beautiful woman can easily have fallen in love with an all-too-human robot?\"]</td>\n",
       "      <td>[\"About the Author\",\"Isaac Asimov, who was named \\\"Grand Master of Science Fiction\\\" by the Science Fiction Writers of America, entertained and educated readers of all ages for close to five decades.William Dufris has been nominated nine times as a finalist for the APA's prestigious Audie Award and has garnered twenty-one Earphones Awards from AudioFile magazine, which also named him one of the Best Voices at the End of the Century.\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Isaac Asimov (Author),  William Dufris (Narrator)</td>\n",
       "      <td>[\"Books\",\"Computers &amp; Technology\",\"Computer Science\"]</td>\n",
       "      <td>{'ISBN 13': '978-1400154234', 'Language': 'English', 'ISBN 10': '1400154235', 'Dimensions': '5.3 x 0.6 x 7.4 inches', 'Publisher': 'Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)', 'Item Weight': '2.61 ounces'}</td>\n",
       "      <td>1400154235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Isaac Asimov</td>\n",
       "      <td>['Isaac Asimov (/ˈaɪzᵻk ˈæzᵻmɒv/; born Isaak Yudovich Ozimov; circa January 2, 1920 – April 6, 1992) was an American author and professor of biochemistry at Boston University, best known for his works of science fiction and for his popular science books. Asimov was prolific and wrote or edited more than 500 books and an estimated 90,000 letters and postcards. His books have been published in 9 of the 10 major categories of the Dewey Decimal Classification.', 'Asimov wrote hard science fiction and, along with Robert A. Heinlein and Arthur C. Clarke, he was considered one of the \"Big Three\" science fiction writers during his lifetime. Asimov\\'s most famous work is the Foundation Series; his other major series are the Galactic Empire series and the Robot series. The Galactic Empire novels are explicitly set in earlier history of the same fictional universe as the Foundation series. Later, beginning with Foundation\\'s Edge, he linked this distant future to the Robot and Spacer stories, creating a unified \"future history\" for his stories much like those pioneered by Robert A. Heinlein and previously produced by Cordwainer Smith and Poul Anderson. He wrote hundreds of short stories, including the social science fiction \"Nightfall\", which in 1964 was voted by the Science Fiction Writers of America the best short science fiction story of all time. Asimov wrote the Lucky Starr series of juvenile science-fiction novels using the pen name Paul French.', \"Asimov also wrote mysteries and fantasy, as well as much nonfiction. Most of his popular science books explain scientific concepts in a historical way, going as far back as possible to a time when the science in question was at its simplest stage. He often provides nationalities, birth dates, and death dates for the scientists he mentions, as well as etymologies and pronunciation guides for technical terms. Examples include Guide to Science, the three-volume set Understanding Physics, and Asimov's Chronology of Science and Discovery, as well as works on astronomy, mathematics, history, William Shakespeare's writing, and chemistry.\", 'Asimov was a long-time member and vice president of Mensa International, albeit reluctantly; he described some members of that organization as \"brain-proud and aggressive about their IQs\". He took more joy in being president of the American Humanist Association. The asteroid 5020 Asimov, a crater on the planet Mars, a Brooklyn elementary school, and a literary award are named in his honor.', 'Bio from Wikipedia, the free encyclopedia. Photo by Phillip Leonian from New York World-Telegram &amp; Sun [Public domain], via Wikimedia Commons.']</td>\n",
       "      <td>https://m.media-amazon.com/images/S/amzn-author-media-prod/6ce4rnjgpci8m81tu3aibtlf5r._SY600_.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        main_category                             title  \\\n",
       "0               Books                  The Swan Thieves   \n",
       "1  Audible Audiobooks              Death in a White Tie   \n",
       "2               Books  Reunion Pass (Thorndike Romance)   \n",
       "3               Books                      Key Lock Man   \n",
       "4               Books     The Robots of Dawn (Robot, 3)   \n",
       "\n",
       "                                     subtitle  average_rating  rating_number  \\\n",
       "0  Audio CD – Bargain Price, November 3, 2010             4.2           1559   \n",
       "1                                         NaN             4.5           1033   \n",
       "2       Hardcover – Large Print, May 18, 2016             4.6           1140   \n",
       "3     Mass Market Paperback – January 1, 1981             4.6           1907   \n",
       "4          MP3 CD – Unabridged, July 15, 2007             4.7           5660   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         features  \\\n",
       "0  [\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devotion to his profession and the painting hobby he loves. This order is destroyed when renowned painter Robert Oliver attacks a canvas in the National Gallery of Art and becomes his patient. In response, Marlowe finds himself going beyond his own legal and ethical boundaries to understand the secret that torments this genius, a journey that will lead him into the lives of the women closest to Robert Oliver and toward a tragedy at the heart of French Impressionism.Ranging from American museums to the coast of Normandy, from the late nineteenth century to the late twentieth, from young love to last love, THE SWAN THIEVES is a story of obsession, the losses of history, and the power of art to preserve human hope.\"]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NaN   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                [\"A New York Times Bestselling Author An Eternity Springs Novel Six years ago, Chase Timberlake bought an engagement ring for high school sweetheart Lori Reese. Then, life happened. Chase’s career took off and Lori got into veterinary school. When Chase’s jet-setting life takes a tragic turn, he returns to Eternity Springs a damaged man. Can she and Chase turn back the hands of time and pick up where they left off ― and give forever a chance?\"]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [\"book 163\"]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                      [\"A puzzling case of roboticide sends New York Detective Elijah Baley on an intense search for a murderer. Armed with his own instincts, his quirky logic, and the immutable Three Laws of Robotics, Baley is determined to solve the case. But can anything prepare a simple Earthman for the psychological complexities of a world where a beautiful woman can easily have fallen in love with an all-too-human robot?\"]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                              description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
       "4  [\"About the Author\",\"Isaac Asimov, who was named \\\"Grand Master of Science Fiction\\\" by the Science Fiction Writers of America, entertained and educated readers of all ages for close to five decades.William Dufris has been nominated nine times as a finalist for the APA's prestigious Audie Award and has garnered twenty-one Earphones Awards from AudioFile magazine, which also named him one of the Best Voices at the End of the Century.\"]   \n",
       "\n",
       "   price images videos  \\\n",
       "0    NaN    NaN    NaN   \n",
       "1    NaN    NaN    NaN   \n",
       "2  12.00    NaN    NaN   \n",
       "3   5.55    NaN    NaN   \n",
       "4    NaN    NaN    NaN   \n",
       "\n",
       "                                                                                                                                                store  \\\n",
       "0  Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Reader),  Sarah Zimmerman (Reader),  John Lee (Reader)   \n",
       "1      Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher)    &                   0                  more   \n",
       "2                                                                                                                                Emily March (Author)   \n",
       "3                                                                                                                              Louis L'Amour (Author)   \n",
       "4                                                                                                   Isaac Asimov (Author),  William Dufris (Narrator)   \n",
       "\n",
       "                                                                    categories  \\\n",
       "0                             [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "1  [\"Books\",\"Mystery, Thriller & Suspense\",\"Mystery\",\"Traditional Detectives\"]   \n",
       "2                               [\"Books\",\"Large Print\",\"Literature & Fiction\"]   \n",
       "3                                                                          NaN   \n",
       "4                        [\"Books\",\"Computers & Technology\",\"Computer Science\"]   \n",
       "\n",
       "                                                                                                                                                                                                                                                     details  \\\n",
       "0                                                                                         {'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; Una Rei edition (November 3, 2010)', 'Item Weight': '1.05 pounds'}   \n",
       "1                                                                                                                                                                                                                                                         {}   \n",
       "2  {'ISBN 13': '978-1410490117', 'Language': 'English', 'ISBN 10': '9781410490117', 'Dimensions': '6 x 1 x 9 inches', 'Publisher': 'Thorndike Press Large Print; Large Print edition (May 18, 2016)', 'Item Weight': '1.2 pounds', 'Hardcover': '422 pages'}   \n",
       "3                                                                    {'ISBN 13': '978-0553230888', 'Language': 'English', 'Mass Market Paperback': '0 pages', 'ISBN 10': '0553230883', 'Publisher': 'Bantam (January 1, 1981)', 'Item Weight': '3.2 ounces'}   \n",
       "4                              {'ISBN 13': '978-1400154234', 'Language': 'English', 'ISBN 10': '1400154235', 'Dimensions': '5.3 x 0.6 x 7.4 inches', 'Publisher': 'Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)', 'Item Weight': '2.61 ounces'}   \n",
       "\n",
       "  parent_asin  bought_together        author_name  \\\n",
       "0  B0062GL89I              NaN  Elizabeth Kostova   \n",
       "1  B001F1ZPDK              NaN                NaN   \n",
       "2  1410490114              NaN        Emily March   \n",
       "3  0553230883              NaN      Louis L'Amour   \n",
       "4  1400154235              NaN       Isaac Asimov   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           author_about  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a lifetime of imagining--since Kostova's girlhood, when her father entertained her with tales of Dracula, she has envisioned the story that would become The Historian. With her academic spirit and extraordinary talent, she's spun an intricate tale of sprawling mystery and suspense. Kostova graduated from Yale and holds an MFA from the University of Michigan, where she won the Hopwood Award for the Novel-in-Progress.\"]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ['Emily March is the New York Times, Publishers Weekly, and USA Today bestselling author of over forty novels, including the critically acclaimed Eternity Springs series. Publishers Weekly calls March a \"master of delightful banter,\" and her heartwarming, emotionally charged stories have been named to Best of the Year lists by Publishers Weekly, Library Journal, and Romance Writers of America. A graduate of Texas A&M University, Emily is an avid fan of Aggie sports, and her recipe for jalapeño relish has made her a tailgating legend.']   \n",
       "3  ['\"I think of myself in the oral tradition--as a troubadour, a village tale-teller, the man in the shadows of a campfire. That\\'s the way I\\'d like to be remembered--as a storyteller. A good storyteller.\"', 'It is doubtful that any author could be as at home in the world re-created in his novels as Louis Dearborn L\\'Amour. Not only could he physically fill the boots of the rugged characters he wrote about, but he literally \"walked the land my characters walk.\" His personal experiences as well as his lifelong devotion to historical research combined to give Mr. L\\'Amour the unique knowledge and understanding of people, events, and the challenge of the American frontier that became the hallmarks of his popularity.', 'Of French-Irish descent, Mr. L\\'Amour could trace his own in North America back to the early 1600s and follow their steady progression westward, \"always on the frontier.\" As a boy growing up in Jamestown, North Dakota, he absorbed all he could about his family\\'s frontier heritage, including the story of his great-grandfather who was scalped by Sioux warriors.', 'Spurred by an eager curiosity and desire to broaden his horizons, Mr. L\\'Amour left home at the age of fifteen and enjoyed a wide variety of jobs, including seaman, lumberjack, elephant handler, skinner of dead cattle, and miner, and was an officer in the transportation corps during World War II. During his \"yondering\" days he also circled the world on a freighter, sailed a dhow on the Red Sea, was shipwrecked in the West Indies and stranded in the Mojave Desert. He won fifty-one of fifty-nine fights as a professional boxer and worked as a journalist and lecturer. He was a voracious reader and collector of rare books. His personal library contained 17,000 volumes.', 'Mr. L\\'Amour \"wanted to write almost from the time I could talk.\" After developing a widespread following for his many frontiers and adventure stories written for fiction magazines, Mr. L\\'Amour published his first full length novel, Hondo, in the United States in 1953. Every one of his more than 120 books is in print; there are more than 300 million copies of his books in print worldwide, making him one of the bestselling authors in modern literary history. His books have been translated into twenty languages, and more than forty-five of his novels and stories have been made into feature films and television movies.', \"The recipient of many great honor and awards, in 1983 Mr. L'Amour became the first novelist to ever to be awarded the Congressional Gold Medal by the United States Congress in honor of his life's work. In 1984 he was also awarded the Medal of Freedom by President Reagan.\", \"Louis L'Amour died on June 10, 1988. His wife, Kathy, and their two children, Beau and Angelique, carry the L'Amour publishing tradition forward with new books written by the author during his lifetime to be published by Bantam.\"]   \n",
       "4                                                                                                                                                                                                                                                            ['Isaac Asimov (/ˈaɪzᵻk ˈæzᵻmɒv/; born Isaak Yudovich Ozimov; circa January 2, 1920 – April 6, 1992) was an American author and professor of biochemistry at Boston University, best known for his works of science fiction and for his popular science books. Asimov was prolific and wrote or edited more than 500 books and an estimated 90,000 letters and postcards. His books have been published in 9 of the 10 major categories of the Dewey Decimal Classification.', 'Asimov wrote hard science fiction and, along with Robert A. Heinlein and Arthur C. Clarke, he was considered one of the \"Big Three\" science fiction writers during his lifetime. Asimov\\'s most famous work is the Foundation Series; his other major series are the Galactic Empire series and the Robot series. The Galactic Empire novels are explicitly set in earlier history of the same fictional universe as the Foundation series. Later, beginning with Foundation\\'s Edge, he linked this distant future to the Robot and Spacer stories, creating a unified \"future history\" for his stories much like those pioneered by Robert A. Heinlein and previously produced by Cordwainer Smith and Poul Anderson. He wrote hundreds of short stories, including the social science fiction \"Nightfall\", which in 1964 was voted by the Science Fiction Writers of America the best short science fiction story of all time. Asimov wrote the Lucky Starr series of juvenile science-fiction novels using the pen name Paul French.', \"Asimov also wrote mysteries and fantasy, as well as much nonfiction. Most of his popular science books explain scientific concepts in a historical way, going as far back as possible to a time when the science in question was at its simplest stage. He often provides nationalities, birth dates, and death dates for the scientists he mentions, as well as etymologies and pronunciation guides for technical terms. Examples include Guide to Science, the three-volume set Understanding Physics, and Asimov's Chronology of Science and Discovery, as well as works on astronomy, mathematics, history, William Shakespeare's writing, and chemistry.\", 'Asimov was a long-time member and vice president of Mensa International, albeit reluctantly; he described some members of that organization as \"brain-proud and aggressive about their IQs\". He took more joy in being president of the American Humanist Association. The asteroid 5020 Asimov, a crater on the planet Mars, a Brooklyn elementary school, and a literary award are named in his honor.', 'Bio from Wikipedia, the free encyclopedia. Photo by Phillip Leonian from New York World-Telegram & Sun [Public domain], via Wikimedia Commons.']   \n",
       "\n",
       "                                                                                       author_avatar  \n",
       "0                                        https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg  \n",
       "1                                                                                                NaN  \n",
       "2  https://m.media-amazon.com/images/S/amzn-author-media-prod/an3i0pthoa0nsmk8mnn25ndb7n._SY600_.jpg  \n",
       "3                                        https://m.media-amazon.com/images/I/31pdVqK+eZL._SY600_.jpg  \n",
       "4  https://m.media-amazon.com/images/S/amzn-author-media-prod/6ce4rnjgpci8m81tu3aibtlf5r._SY600_.jpg  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Has Nested Columns**\n",
    "\n",
    "One of the biggest issues in this file (ahead of goodreads2017 metadata likewise) is the nested columns product of the file being originally in JSON format. These inner columns (json format key: value can take up allow of names, being by their own right individual main columns. However, a large portion of them are incredibly sparse in the dataset, meaning not all rows have all values in these columns (would be NaN). \n",
    "\n",
    "Therefore turning all \"json keys\" into columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Null Values**\n",
    "**Number and Percentage of Missing data points per column**\n",
    "\n",
    "- Replaced empty strings (or strings with only whitespace) with np.nan\n",
    "- Replaced empty lists with np.nan using Series.map for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: \n",
      " main_category         183\n",
      "title                   0\n",
      "subtitle            56630\n",
      "average_rating          0\n",
      "rating_number           0\n",
      "features            58780\n",
      "description        146653\n",
      "price               83306\n",
      "images              83641\n",
      "videos             265809\n",
      "store               10062\n",
      "categories          18717\n",
      "details                 0\n",
      "parent_asin             0\n",
      "bought_together    280338\n",
      "author_name         50333\n",
      "author_about        50333\n",
      "author_avatar       50330\n",
      "dtype: int64\n",
      "\n",
      "Percentage: \n",
      " main_category        0.065278\n",
      "title                0.000000\n",
      "subtitle            20.200615\n",
      "average_rating       0.000000\n",
      "rating_number        0.000000\n",
      "features            20.967546\n",
      "description         52.312922\n",
      "price               29.716271\n",
      "images              29.835770\n",
      "videos              94.817328\n",
      "store                3.589239\n",
      "categories           6.676583\n",
      "details              0.000000\n",
      "parent_asin          0.000000\n",
      "bought_together    100.000000\n",
      "author_name         17.954398\n",
      "author_about        17.954398\n",
      "author_avatar       17.953328\n"
     ]
    }
   ],
   "source": [
    "# Replace empty strings or strings with only whitespace with np.nan\n",
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "# Replace empty lists with np.nan using Series.map for each column\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].replace(\"[]\", np.nan)\n",
    "\n",
    "\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(\"Number: \\n\", missing_values_count)\n",
    "\n",
    "nan_percentage = df.isna().mean() * 100\n",
    "print(\"\\nPercentage: \\n\",nan_percentage.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"bought_together\" column is all NaN, so we dropped it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('bought_together', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['main_category', 'title', 'subtitle', 'average_rating', 'rating_number',\n",
       "       'features', 'description', 'price', 'images', 'videos', 'store',\n",
       "       'categories', 'details', 'parent_asin', 'author_name', 'author_about',\n",
       "       'author_avatar'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Let's Start Cleaning up and Standardizing some of the Columns Atributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cardinality—that is, the number of unique (non-null) values—for each column in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cardinality(df):\n",
    "    \"\"\"\n",
    "    Calculate the cardinality (number of unique non-null values) for each column in the DataFrame.\n",
    "    Requires:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    Returns:\n",
    "    pd.Series: A series where the index is the column names and the values are the cardinality of each column.\n",
    "    \"\"\"\n",
    "    return df.apply(lambda col: col.nunique(dropna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_category          6\n",
       "title             189601\n",
       "subtitle           47377\n",
       "average_rating        25\n",
       "rating_number      29259\n",
       "features          187274\n",
       "description       114554\n",
       "price               8622\n",
       "images            194013\n",
       "videos             13297\n",
       "store              95548\n",
       "categories          2036\n",
       "details           230805\n",
       "parent_asin       280338\n",
       "author_name        29078\n",
       "author_about       22269\n",
       "author_avatar      21453\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cardinality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Normalizing Nested JSON-like Data**\n",
    "Details contains several columns with nested data, indu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"details\" column**\n",
    "\n",
    "Fixed \"details\" column had JSON format within a list. Removed the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = df['details'].apply(pd.Series)\n",
    "\n",
    "# Optionally, drop the original 'details' column and join the new columns\n",
    "df = df.drop('details', axis=1).join(details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0: \"details\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"details\" column is stored as a string (rather than as a native Python dict, even though it has that formating).\n",
    "\n",
    "Let's see how many inner columns it has:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Item Weight</th>\n",
       "      <th>ISBN 13</th>\n",
       "      <th>ISBN 10</th>\n",
       "      <th>Hardcover</th>\n",
       "      <th>Mass Market Paperback</th>\n",
       "      <th>Paperback</th>\n",
       "      <th>File size</th>\n",
       "      <th>Text to Speech</th>\n",
       "      <th>Word Wise</th>\n",
       "      <th>Enhanced typesetting</th>\n",
       "      <th>Screen Reader</th>\n",
       "      <th>X Ray</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Sticky notes</th>\n",
       "      <th>Print length</th>\n",
       "      <th>MP3 CD</th>\n",
       "      <th>Reading age</th>\n",
       "      <th>Page numbers source ISBN</th>\n",
       "      <th>Grade level</th>\n",
       "      <th>Audio CD</th>\n",
       "      <th>Lexile measure</th>\n",
       "      <th>Novelty Book</th>\n",
       "      <th>Imitation Leather</th>\n",
       "      <th>Library Binding</th>\n",
       "      <th>Simultaneous device usage</th>\n",
       "      <th>Board book</th>\n",
       "      <th>Sheet music</th>\n",
       "      <th>Audio Cassette</th>\n",
       "      <th>Product Dimensions</th>\n",
       "      <th>Is Discontinued By Manufacturer</th>\n",
       "      <th>Country of Origin</th>\n",
       "      <th>Release date</th>\n",
       "      <th>Spiral bound</th>\n",
       "      <th>Date First Available</th>\n",
       "      <th>File Size</th>\n",
       "      <th>Loose Leaf</th>\n",
       "      <th>School Library Binding</th>\n",
       "      <th>Package Dimensions</th>\n",
       "      <th>Leather Bound</th>\n",
       "      <th>Bonded Leather</th>\n",
       "      <th>Pamphlet</th>\n",
       "      <th>Plastic Comb</th>\n",
       "      <th>Unknown Binding</th>\n",
       "      <th>Run time</th>\n",
       "      <th>Cards</th>\n",
       "      <th>CD ROM</th>\n",
       "      <th>Misc Supplies</th>\n",
       "      <th>Comic</th>\n",
       "      <th>Pocket Book</th>\n",
       "      <th>Perfect Paperback</th>\n",
       "      <th>Item model number</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Binding</th>\n",
       "      <th>Ruling Type</th>\n",
       "      <th>Color</th>\n",
       "      <th>Sheet Size</th>\n",
       "      <th>Cover Material</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Number of Items</th>\n",
       "      <th>Special Feature</th>\n",
       "      <th>Calendar</th>\n",
       "      <th>Product Bundle</th>\n",
       "      <th>Flexibound</th>\n",
       "      <th>Roughcut</th>\n",
       "      <th>Misc</th>\n",
       "      <th>Vinyl Bound</th>\n",
       "      <th>X Ray for textbooks</th>\n",
       "      <th>Diary</th>\n",
       "      <th>Hardcover spiral</th>\n",
       "      <th>Paperback Shinsho</th>\n",
       "      <th>Pop Up</th>\n",
       "      <th>Map</th>\n",
       "      <th>Format</th>\n",
       "      <th>Contributor</th>\n",
       "      <th>Manufacturer recommended age</th>\n",
       "      <th>Domestic Shipping</th>\n",
       "      <th>Best Sellers Rank</th>\n",
       "      <th>International Shipping</th>\n",
       "      <th>Department</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Paperback Bunko</th>\n",
       "      <th>Tankobon Hardcover</th>\n",
       "      <th>Card Book</th>\n",
       "      <th>Ring bound</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Type of item</th>\n",
       "      <th>Pricing</th>\n",
       "      <th>Poster</th>\n",
       "      <th>Bookmark</th>\n",
       "      <th>Staple Bound</th>\n",
       "      <th>Toy</th>\n",
       "      <th>Digital Audiobook</th>\n",
       "      <th>DVD</th>\n",
       "      <th>Turtleback</th>\n",
       "      <th>Single Issue Magazine</th>\n",
       "      <th>Bath Book</th>\n",
       "      <th>Wall Chart</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Unbound</th>\n",
       "      <th>Printed Access Code</th>\n",
       "      <th>Print on Demand Paperback</th>\n",
       "      <th>Textbook Binding</th>\n",
       "      <th>Game</th>\n",
       "      <th>Audio CD Library Binding</th>\n",
       "      <th>Edition</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Item Dimensions LxWxH</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Other display features</th>\n",
       "      <th>Preloaded Digital Audio Player</th>\n",
       "      <th>Batteries Required?</th>\n",
       "      <th>Hardware Platform</th>\n",
       "      <th>Rag Book</th>\n",
       "      <th>Item Package Quantity</th>\n",
       "      <th>Batteries Included?</th>\n",
       "      <th>Stationery</th>\n",
       "      <th>Blu ray</th>\n",
       "      <th>Digital</th>\n",
       "      <th>Hardcover Comic</th>\n",
       "      <th>Size</th>\n",
       "      <th>Manufacturer Part Number</th>\n",
       "      <th>Part Number</th>\n",
       "      <th>DVD ROM</th>\n",
       "      <th>Item Package Dimensions L x W x H</th>\n",
       "      <th>Item Dimensions  LxWxH</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Style</th>\n",
       "      <th>Suggested Users</th>\n",
       "      <th>Package Weight</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Batteries</th>\n",
       "      <th>Mook</th>\n",
       "      <th>Notebook</th>\n",
       "      <th>Print Magazine</th>\n",
       "      <th>Diskette</th>\n",
       "      <th>Sports Apparel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5.5 x 2.15 x 5.75 inches</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>1.05 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>6 x 1 x 9 inches</td>\n",
       "      <td>Thorndike Press Large Print; Large Print edition (May 18, 2016)</td>\n",
       "      <td>1.2 pounds</td>\n",
       "      <td>978-1410490117</td>\n",
       "      <td>9781410490117</td>\n",
       "      <td>422 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bantam (January 1, 1981)</td>\n",
       "      <td>3.2 ounces</td>\n",
       "      <td>978-0553230888</td>\n",
       "      <td>0553230883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>5.3 x 0.6 x 7.4 inches</td>\n",
       "      <td>Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)</td>\n",
       "      <td>2.61 ounces</td>\n",
       "      <td>978-1400154234</td>\n",
       "      <td>1400154235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280333</th>\n",
       "      <td>English</td>\n",
       "      <td>6.42 x 1.4 x 9.52 inches</td>\n",
       "      <td>Doubleday (September 13, 2011)</td>\n",
       "      <td>1.64 pounds</td>\n",
       "      <td>978-0385534635</td>\n",
       "      <td>0385534639</td>\n",
       "      <td>400 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>950L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280334</th>\n",
       "      <td>English</td>\n",
       "      <td>5.02 x 0.85 x 5.94 inches</td>\n",
       "      <td>Macmillan Audio; Unabridged edition (November 4, 2014)</td>\n",
       "      <td>8.6 ounces</td>\n",
       "      <td>978-1427252098</td>\n",
       "      <td>1427252092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scribner; 31685th edition (January 1, 1994)</td>\n",
       "      <td>1 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280336</th>\n",
       "      <td>English</td>\n",
       "      <td>6 x 0.25 x 9 inches</td>\n",
       "      <td>Cloud Forest Press (March 3, 2020)</td>\n",
       "      <td>6.7 ounces</td>\n",
       "      <td>978-1646081639</td>\n",
       "      <td>1646081633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280337</th>\n",
       "      <td>English</td>\n",
       "      <td>5.75 x 1.4 x 8.63 inches</td>\n",
       "      <td>VIZ Media LLC (December 2, 2008)</td>\n",
       "      <td>2.05 pounds</td>\n",
       "      <td>978-1421520667</td>\n",
       "      <td>1421520664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>560 pages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12 - 17 years</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 - 6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280338 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language                 Dimensions  \\\n",
       "0       English   5.5 x 2.15 x 5.75 inches   \n",
       "1           NaN                        NaN   \n",
       "2       English           6 x 1 x 9 inches   \n",
       "3       English                        NaN   \n",
       "4       English     5.3 x 0.6 x 7.4 inches   \n",
       "...         ...                        ...   \n",
       "280333  English   6.42 x 1.4 x 9.52 inches   \n",
       "280334  English  5.02 x 0.85 x 5.94 inches   \n",
       "280335      NaN                        NaN   \n",
       "280336  English        6 x 0.25 x 9 inches   \n",
       "280337  English   5.75 x 1.4 x 8.63 inches   \n",
       "\n",
       "                                                              Publisher  \\\n",
       "0                    Hachette Audio; Una Rei edition (November 3, 2010)   \n",
       "1                                                                   NaN   \n",
       "2       Thorndike Press Large Print; Large Print edition (May 18, 2016)   \n",
       "3                                              Bantam (January 1, 1981)   \n",
       "4             Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)   \n",
       "...                                                                 ...   \n",
       "280333                                   Doubleday (September 13, 2011)   \n",
       "280334           Macmillan Audio; Unabridged edition (November 4, 2014)   \n",
       "280335                      Scribner; 31685th edition (January 1, 1994)   \n",
       "280336                               Cloud Forest Press (March 3, 2020)   \n",
       "280337                                 VIZ Media LLC (December 2, 2008)   \n",
       "\n",
       "        Item Weight         ISBN 13        ISBN 10  Hardcover  \\\n",
       "0       1.05 pounds             NaN            NaN        NaN   \n",
       "1               NaN             NaN            NaN        NaN   \n",
       "2        1.2 pounds  978-1410490117  9781410490117  422 pages   \n",
       "3        3.2 ounces  978-0553230888     0553230883        NaN   \n",
       "4       2.61 ounces  978-1400154234     1400154235        NaN   \n",
       "...             ...             ...            ...        ...   \n",
       "280333  1.64 pounds  978-0385534635     0385534639  400 pages   \n",
       "280334   8.6 ounces  978-1427252098     1427252092        NaN   \n",
       "280335     1 pounds             NaN            NaN        NaN   \n",
       "280336   6.7 ounces  978-1646081639     1646081633        NaN   \n",
       "280337  2.05 pounds  978-1421520667     1421520664        NaN   \n",
       "\n",
       "       Mass Market Paperback  Paperback File size Text to Speech Word Wise  \\\n",
       "0                        NaN        NaN       NaN            NaN       NaN   \n",
       "1                        NaN        NaN       NaN            NaN       NaN   \n",
       "2                        NaN        NaN       NaN            NaN       NaN   \n",
       "3                    0 pages        NaN       NaN            NaN       NaN   \n",
       "4                        NaN        NaN       NaN            NaN       NaN   \n",
       "...                      ...        ...       ...            ...       ...   \n",
       "280333                   NaN        NaN       NaN            NaN       NaN   \n",
       "280334                   NaN        NaN       NaN            NaN       NaN   \n",
       "280335                   NaN        NaN       NaN            NaN       NaN   \n",
       "280336                   NaN  108 pages       NaN            NaN       NaN   \n",
       "280337                   NaN  560 pages       NaN            NaN       NaN   \n",
       "\n",
       "       Enhanced typesetting Screen Reader X Ray Publication date Sticky notes  \\\n",
       "0                       NaN           NaN   NaN              NaN          NaN   \n",
       "1                       NaN           NaN   NaN              NaN          NaN   \n",
       "2                       NaN           NaN   NaN              NaN          NaN   \n",
       "3                       NaN           NaN   NaN              NaN          NaN   \n",
       "4                       NaN           NaN   NaN              NaN          NaN   \n",
       "...                     ...           ...   ...              ...          ...   \n",
       "280333                  NaN           NaN   NaN              NaN          NaN   \n",
       "280334                  NaN           NaN   NaN              NaN          NaN   \n",
       "280335                  NaN           NaN   NaN              NaN          NaN   \n",
       "280336                  NaN           NaN   NaN              NaN          NaN   \n",
       "280337                  NaN           NaN   NaN              NaN          NaN   \n",
       "\n",
       "       Print length MP3 CD    Reading age Page numbers source ISBN  \\\n",
       "0               NaN    NaN            NaN                      NaN   \n",
       "1               NaN    NaN            NaN                      NaN   \n",
       "2               NaN    NaN            NaN                      NaN   \n",
       "3               NaN    NaN            NaN                      NaN   \n",
       "4               NaN    NaN            NaN                      NaN   \n",
       "...             ...    ...            ...                      ...   \n",
       "280333          NaN    NaN            NaN                      NaN   \n",
       "280334          NaN    NaN            NaN                      NaN   \n",
       "280335          NaN    NaN            NaN                      NaN   \n",
       "280336          NaN    NaN            NaN                      NaN   \n",
       "280337          NaN    NaN  12 - 17 years                      NaN   \n",
       "\n",
       "       Grade level Audio CD Lexile measure Novelty Book Imitation Leather  \\\n",
       "0              NaN      NaN            NaN          NaN               NaN   \n",
       "1              NaN      NaN            NaN          NaN               NaN   \n",
       "2              NaN      NaN            NaN          NaN               NaN   \n",
       "3              NaN      NaN            NaN          NaN               NaN   \n",
       "4              NaN      NaN            NaN          NaN               NaN   \n",
       "...            ...      ...            ...          ...               ...   \n",
       "280333         NaN      NaN           950L          NaN               NaN   \n",
       "280334         NaN      NaN            NaN          NaN               NaN   \n",
       "280335         NaN      NaN            NaN          NaN               NaN   \n",
       "280336         NaN      NaN            NaN          NaN               NaN   \n",
       "280337       4 - 6      NaN            NaN          NaN               NaN   \n",
       "\n",
       "       Library Binding Simultaneous device usage Board book Sheet music  \\\n",
       "0                  NaN                       NaN        NaN         NaN   \n",
       "1                  NaN                       NaN        NaN         NaN   \n",
       "2                  NaN                       NaN        NaN         NaN   \n",
       "3                  NaN                       NaN        NaN         NaN   \n",
       "4                  NaN                       NaN        NaN         NaN   \n",
       "...                ...                       ...        ...         ...   \n",
       "280333             NaN                       NaN        NaN         NaN   \n",
       "280334             NaN                       NaN        NaN         NaN   \n",
       "280335             NaN                       NaN        NaN         NaN   \n",
       "280336             NaN                       NaN        NaN         NaN   \n",
       "280337             NaN                       NaN        NaN         NaN   \n",
       "\n",
       "       Audio Cassette Product Dimensions Is Discontinued By Manufacturer  \\\n",
       "0                 NaN                NaN                             NaN   \n",
       "1                 NaN                NaN                             NaN   \n",
       "2                 NaN                NaN                             NaN   \n",
       "3                 NaN                NaN                             NaN   \n",
       "4                 NaN                NaN                             NaN   \n",
       "...               ...                ...                             ...   \n",
       "280333            NaN                NaN                             NaN   \n",
       "280334            NaN                NaN                             NaN   \n",
       "280335            NaN                NaN                             NaN   \n",
       "280336            NaN                NaN                             NaN   \n",
       "280337            NaN                NaN                             NaN   \n",
       "\n",
       "       Country of Origin Release date Spiral bound Date First Available  \\\n",
       "0                    NaN          NaN          NaN                  NaN   \n",
       "1                    NaN          NaN          NaN                  NaN   \n",
       "2                    NaN          NaN          NaN                  NaN   \n",
       "3                    NaN          NaN          NaN                  NaN   \n",
       "4                    NaN          NaN          NaN                  NaN   \n",
       "...                  ...          ...          ...                  ...   \n",
       "280333               NaN          NaN          NaN                  NaN   \n",
       "280334               NaN          NaN          NaN                  NaN   \n",
       "280335               NaN          NaN          NaN                  NaN   \n",
       "280336               NaN          NaN          NaN                  NaN   \n",
       "280337               NaN          NaN          NaN                  NaN   \n",
       "\n",
       "       File Size Loose Leaf School Library Binding Package Dimensions  \\\n",
       "0            NaN        NaN                    NaN                NaN   \n",
       "1            NaN        NaN                    NaN                NaN   \n",
       "2            NaN        NaN                    NaN                NaN   \n",
       "3            NaN        NaN                    NaN                NaN   \n",
       "4            NaN        NaN                    NaN                NaN   \n",
       "...          ...        ...                    ...                ...   \n",
       "280333       NaN        NaN                    NaN                NaN   \n",
       "280334       NaN        NaN                    NaN                NaN   \n",
       "280335       NaN        NaN                    NaN                NaN   \n",
       "280336       NaN        NaN                    NaN                NaN   \n",
       "280337       NaN        NaN                    NaN                NaN   \n",
       "\n",
       "       Leather Bound Bonded Leather Pamphlet Plastic Comb Unknown Binding  \\\n",
       "0                NaN            NaN      NaN          NaN             NaN   \n",
       "1                NaN            NaN      NaN          NaN             NaN   \n",
       "2                NaN            NaN      NaN          NaN             NaN   \n",
       "3                NaN            NaN      NaN          NaN             NaN   \n",
       "4                NaN            NaN      NaN          NaN             NaN   \n",
       "...              ...            ...      ...          ...             ...   \n",
       "280333           NaN            NaN      NaN          NaN             NaN   \n",
       "280334           NaN            NaN      NaN          NaN             NaN   \n",
       "280335           NaN            NaN      NaN          NaN             NaN   \n",
       "280336           NaN            NaN      NaN          NaN             NaN   \n",
       "280337           NaN            NaN      NaN          NaN             NaN   \n",
       "\n",
       "       Run time Cards CD ROM Misc Supplies Comic Pocket Book  \\\n",
       "0           NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "1           NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "2           NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "3           NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "4           NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "...         ...   ...    ...           ...   ...         ...   \n",
       "280333      NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "280334      NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "280335      NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "280336      NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "280337      NaN   NaN    NaN           NaN   NaN         NaN   \n",
       "\n",
       "       Perfect Paperback Item model number Pages Binding Ruling Type Color  \\\n",
       "0                    NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "1                    NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "2                    NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "3                    NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "4                    NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "...                  ...               ...   ...     ...         ...   ...   \n",
       "280333               NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "280334               NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "280335               NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "280336               NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "280337               NaN               NaN   NaN     NaN         NaN   NaN   \n",
       "\n",
       "       Sheet Size Cover Material Theme Number of Items Special Feature  \\\n",
       "0             NaN            NaN   NaN             NaN             NaN   \n",
       "1             NaN            NaN   NaN             NaN             NaN   \n",
       "2             NaN            NaN   NaN             NaN             NaN   \n",
       "3             NaN            NaN   NaN             NaN             NaN   \n",
       "4             NaN            NaN   NaN             NaN             NaN   \n",
       "...           ...            ...   ...             ...             ...   \n",
       "280333        NaN            NaN   NaN             NaN             NaN   \n",
       "280334        NaN            NaN   NaN             NaN             NaN   \n",
       "280335        NaN            NaN   NaN             NaN             NaN   \n",
       "280336        NaN            NaN   NaN             NaN             NaN   \n",
       "280337        NaN            NaN   NaN             NaN             NaN   \n",
       "\n",
       "       Calendar Product Bundle Flexibound Roughcut Misc Vinyl Bound  \\\n",
       "0           NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "1           NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "2           NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "3           NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "4           NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "...         ...            ...        ...      ...  ...         ...   \n",
       "280333      NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "280334      NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "280335      NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "280336      NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "280337      NaN            NaN        NaN      NaN  NaN         NaN   \n",
       "\n",
       "       X Ray for textbooks Diary Hardcover spiral Paperback Shinsho Pop Up  \\\n",
       "0                      NaN   NaN              NaN               NaN    NaN   \n",
       "1                      NaN   NaN              NaN               NaN    NaN   \n",
       "2                      NaN   NaN              NaN               NaN    NaN   \n",
       "3                      NaN   NaN              NaN               NaN    NaN   \n",
       "4                      NaN   NaN              NaN               NaN    NaN   \n",
       "...                    ...   ...              ...               ...    ...   \n",
       "280333                 NaN   NaN              NaN               NaN    NaN   \n",
       "280334                 NaN   NaN              NaN               NaN    NaN   \n",
       "280335                 NaN   NaN              NaN               NaN    NaN   \n",
       "280336                 NaN   NaN              NaN               NaN    NaN   \n",
       "280337                 NaN   NaN              NaN               NaN    NaN   \n",
       "\n",
       "        Map Format Contributor Manufacturer recommended age Domestic Shipping  \\\n",
       "0       NaN    NaN         NaN                          NaN               NaN   \n",
       "1       NaN    NaN         NaN                          NaN               NaN   \n",
       "2       NaN    NaN         NaN                          NaN               NaN   \n",
       "3       NaN    NaN         NaN                          NaN               NaN   \n",
       "4       NaN    NaN         NaN                          NaN               NaN   \n",
       "...     ...    ...         ...                          ...               ...   \n",
       "280333  NaN    NaN         NaN                          NaN               NaN   \n",
       "280334  NaN    NaN         NaN                          NaN               NaN   \n",
       "280335  NaN    NaN         NaN                          NaN               NaN   \n",
       "280336  NaN    NaN         NaN                          NaN               NaN   \n",
       "280337  NaN    NaN         NaN                          NaN               NaN   \n",
       "\n",
       "       Best Sellers Rank International Shipping Department Manufacturer  \\\n",
       "0                    NaN                    NaN        NaN          NaN   \n",
       "1                    NaN                    NaN        NaN          NaN   \n",
       "2                    NaN                    NaN        NaN          NaN   \n",
       "3                    NaN                    NaN        NaN          NaN   \n",
       "4                    NaN                    NaN        NaN          NaN   \n",
       "...                  ...                    ...        ...          ...   \n",
       "280333               NaN                    NaN        NaN          NaN   \n",
       "280334               NaN                    NaN        NaN          NaN   \n",
       "280335               NaN                    NaN        NaN          NaN   \n",
       "280336               NaN                    NaN        NaN          NaN   \n",
       "280337               NaN                    NaN        NaN          NaN   \n",
       "\n",
       "       Paperback Bunko Tankobon Hardcover Card Book Ring bound Journal  \\\n",
       "0                  NaN                NaN       NaN        NaN     NaN   \n",
       "1                  NaN                NaN       NaN        NaN     NaN   \n",
       "2                  NaN                NaN       NaN        NaN     NaN   \n",
       "3                  NaN                NaN       NaN        NaN     NaN   \n",
       "4                  NaN                NaN       NaN        NaN     NaN   \n",
       "...                ...                ...       ...        ...     ...   \n",
       "280333             NaN                NaN       NaN        NaN     NaN   \n",
       "280334             NaN                NaN       NaN        NaN     NaN   \n",
       "280335             NaN                NaN       NaN        NaN     NaN   \n",
       "280336             NaN                NaN       NaN        NaN     NaN   \n",
       "280337             NaN                NaN       NaN        NaN     NaN   \n",
       "\n",
       "       Type of item Pricing Poster Bookmark Staple Bound  Toy  \\\n",
       "0               NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "1               NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "2               NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "3               NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "4               NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "...             ...     ...    ...      ...          ...  ...   \n",
       "280333          NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "280334          NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "280335          NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "280336          NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "280337          NaN     NaN    NaN      NaN          NaN  NaN   \n",
       "\n",
       "       Digital Audiobook  DVD Turtleback Single Issue Magazine Bath Book  \\\n",
       "0                    NaN  NaN        NaN                   NaN       NaN   \n",
       "1                    NaN  NaN        NaN                   NaN       NaN   \n",
       "2                    NaN  NaN        NaN                   NaN       NaN   \n",
       "3                    NaN  NaN        NaN                   NaN       NaN   \n",
       "4                    NaN  NaN        NaN                   NaN       NaN   \n",
       "...                  ...  ...        ...                   ...       ...   \n",
       "280333               NaN  NaN        NaN                   NaN       NaN   \n",
       "280334               NaN  NaN        NaN                   NaN       NaN   \n",
       "280335               NaN  NaN        NaN                   NaN       NaN   \n",
       "280336               NaN  NaN        NaN                   NaN       NaN   \n",
       "280337               NaN  NaN        NaN                   NaN       NaN   \n",
       "\n",
       "       Wall Chart Accessory Unbound Printed Access Code  \\\n",
       "0             NaN       NaN     NaN                 NaN   \n",
       "1             NaN       NaN     NaN                 NaN   \n",
       "2             NaN       NaN     NaN                 NaN   \n",
       "3             NaN       NaN     NaN                 NaN   \n",
       "4             NaN       NaN     NaN                 NaN   \n",
       "...           ...       ...     ...                 ...   \n",
       "280333        NaN       NaN     NaN                 NaN   \n",
       "280334        NaN       NaN     NaN                 NaN   \n",
       "280335        NaN       NaN     NaN                 NaN   \n",
       "280336        NaN       NaN     NaN                 NaN   \n",
       "280337        NaN       NaN     NaN                 NaN   \n",
       "\n",
       "       Print on Demand Paperback Textbook Binding Game  \\\n",
       "0                            NaN              NaN  NaN   \n",
       "1                            NaN              NaN  NaN   \n",
       "2                            NaN              NaN  NaN   \n",
       "3                            NaN              NaN  NaN   \n",
       "4                            NaN              NaN  NaN   \n",
       "...                          ...              ...  ...   \n",
       "280333                       NaN              NaN  NaN   \n",
       "280334                       NaN              NaN  NaN   \n",
       "280335                       NaN              NaN  NaN   \n",
       "280336                       NaN              NaN  NaN   \n",
       "280337                       NaN              NaN  NaN   \n",
       "\n",
       "       Audio CD Library Binding Edition Brand Publication Date  \\\n",
       "0                           NaN     NaN   NaN              NaN   \n",
       "1                           NaN     NaN   NaN              NaN   \n",
       "2                           NaN     NaN   NaN              NaN   \n",
       "3                           NaN     NaN   NaN              NaN   \n",
       "4                           NaN     NaN   NaN              NaN   \n",
       "...                         ...     ...   ...              ...   \n",
       "280333                      NaN     NaN   NaN              NaN   \n",
       "280334                      NaN     NaN   NaN              NaN   \n",
       "280335                      NaN     NaN   NaN              NaN   \n",
       "280336                      NaN     NaN   NaN              NaN   \n",
       "280337                      NaN     NaN   NaN              NaN   \n",
       "\n",
       "       Item Dimensions LxWxH Genre Other display features  \\\n",
       "0                        NaN   NaN                    NaN   \n",
       "1                        NaN   NaN                    NaN   \n",
       "2                        NaN   NaN                    NaN   \n",
       "3                        NaN   NaN                    NaN   \n",
       "4                        NaN   NaN                    NaN   \n",
       "...                      ...   ...                    ...   \n",
       "280333                   NaN   NaN                    NaN   \n",
       "280334                   NaN   NaN                    NaN   \n",
       "280335                   NaN   NaN                    NaN   \n",
       "280336                   NaN   NaN                    NaN   \n",
       "280337                   NaN   NaN                    NaN   \n",
       "\n",
       "       Preloaded Digital Audio Player Batteries Required? Hardware Platform  \\\n",
       "0                                 NaN                 NaN               NaN   \n",
       "1                                 NaN                 NaN               NaN   \n",
       "2                                 NaN                 NaN               NaN   \n",
       "3                                 NaN                 NaN               NaN   \n",
       "4                                 NaN                 NaN               NaN   \n",
       "...                               ...                 ...               ...   \n",
       "280333                            NaN                 NaN               NaN   \n",
       "280334                            NaN                 NaN               NaN   \n",
       "280335                            NaN                 NaN               NaN   \n",
       "280336                            NaN                 NaN               NaN   \n",
       "280337                            NaN                 NaN               NaN   \n",
       "\n",
       "       Rag Book Item Package Quantity Batteries Included? Stationery Blu ray  \\\n",
       "0           NaN                   NaN                 NaN        NaN     NaN   \n",
       "1           NaN                   NaN                 NaN        NaN     NaN   \n",
       "2           NaN                   NaN                 NaN        NaN     NaN   \n",
       "3           NaN                   NaN                 NaN        NaN     NaN   \n",
       "4           NaN                   NaN                 NaN        NaN     NaN   \n",
       "...         ...                   ...                 ...        ...     ...   \n",
       "280333      NaN                   NaN                 NaN        NaN     NaN   \n",
       "280334      NaN                   NaN                 NaN        NaN     NaN   \n",
       "280335      NaN                   NaN                 NaN        NaN     NaN   \n",
       "280336      NaN                   NaN                 NaN        NaN     NaN   \n",
       "280337      NaN                   NaN                 NaN        NaN     NaN   \n",
       "\n",
       "       Digital Hardcover Comic Size Manufacturer Part Number Part Number  \\\n",
       "0          NaN             NaN  NaN                      NaN         NaN   \n",
       "1          NaN             NaN  NaN                      NaN         NaN   \n",
       "2          NaN             NaN  NaN                      NaN         NaN   \n",
       "3          NaN             NaN  NaN                      NaN         NaN   \n",
       "4          NaN             NaN  NaN                      NaN         NaN   \n",
       "...        ...             ...  ...                      ...         ...   \n",
       "280333     NaN             NaN  NaN                      NaN         NaN   \n",
       "280334     NaN             NaN  NaN                      NaN         NaN   \n",
       "280335     NaN             NaN  NaN                      NaN         NaN   \n",
       "280336     NaN             NaN  NaN                      NaN         NaN   \n",
       "280337     NaN             NaN  NaN                      NaN         NaN   \n",
       "\n",
       "       DVD ROM Item Package Dimensions L x W x H Item Dimensions  LxWxH  \\\n",
       "0          NaN                               NaN                    NaN   \n",
       "1          NaN                               NaN                    NaN   \n",
       "2          NaN                               NaN                    NaN   \n",
       "3          NaN                               NaN                    NaN   \n",
       "4          NaN                               NaN                    NaN   \n",
       "...        ...                               ...                    ...   \n",
       "280333     NaN                               NaN                    NaN   \n",
       "280334     NaN                               NaN                    NaN   \n",
       "280335     NaN                               NaN                    NaN   \n",
       "280336     NaN                               NaN                    NaN   \n",
       "280337     NaN                               NaN                    NaN   \n",
       "\n",
       "       Brand Name Model Year Style Suggested Users Package Weight Runtime  \\\n",
       "0             NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "1             NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "2             NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "3             NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "4             NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "...           ...        ...   ...             ...            ...     ...   \n",
       "280333        NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "280334        NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "280335        NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "280336        NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "280337        NaN        NaN   NaN             NaN            NaN     NaN   \n",
       "\n",
       "       Batteries Mook Notebook Print Magazine Diskette Sports Apparel  \n",
       "0            NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "1            NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "2            NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "3            NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "4            NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "...          ...  ...      ...            ...      ...            ...  \n",
       "280333       NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "280334       NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "280335       NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "280336       NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "280337       NaN  NaN      NaN            NaN      NaN            NaN  \n",
       "\n",
       "[280338 rows x 141 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "df[\"details\"] = df[\"details\"].apply(ast.literal_eval)\n",
    "details_expanded = df[\"details\"].apply(pd.Series)\n",
    "details_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Values in \"details\"' nested Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage: \n",
      " Language                             19.686236\n",
      "Dimensions                           34.149848\n",
      "Publisher                            21.550771\n",
      "Item Weight                          26.822265\n",
      "ISBN 13                              30.399018\n",
      "ISBN 10                              32.670919\n",
      "Hardcover                            84.411318\n",
      "Mass Market Paperback                95.490087\n",
      "Paperback                            66.483317\n",
      "File size                            92.290021\n",
      "Text to Speech                       92.290735\n",
      "Word Wise                            92.292875\n",
      "Enhanced typesetting                 92.291805\n",
      "Screen Reader                        92.562906\n",
      "X Ray                                92.405239\n",
      "Publication date                     91.757450\n",
      "Sticky notes                         92.289308\n",
      "Print length                         92.323552\n",
      "MP3 CD                               99.848041\n",
      "Reading age                          89.027174\n",
      "Page numbers source ISBN             98.513580\n",
      "Grade level                          94.037911\n",
      "Audio CD                             98.963037\n",
      "Lexile measure                       94.813047\n",
      "Novelty Book                         99.990725\n",
      "Imitation Leather                    99.818433\n",
      "Library Binding                      99.407501\n",
      "Simultaneous device usage            98.638786\n",
      "Board book                           99.476346\n",
      "Sheet music                          99.997503\n",
      "Audio Cassette                       99.870870\n",
      "Product Dimensions                   99.455300\n",
      "Is Discontinued By Manufacturer      99.906898\n",
      "Country of Origin                    99.799171\n",
      "Release date                         99.499176\n",
      "Spiral bound                         99.760289\n",
      "Date First Available                 99.951844\n",
      "File Size                            99.960048\n",
      "Loose Leaf                           99.985018\n",
      "School Library Binding               99.777412\n",
      "Package Dimensions                   99.866233\n",
      "Leather Bound                        99.854105\n",
      "Bonded Leather                       99.968609\n",
      "Pamphlet                             99.973960\n",
      "Plastic Comb                         99.991439\n",
      "Unknown Binding                      99.884425\n",
      "Run time                             99.905471\n",
      "Cards                                99.885495\n",
      "CD ROM                               99.995719\n",
      "Misc Supplies                        99.985375\n",
      "Comic                                99.984305\n",
      "Pocket Book                          99.919383\n",
      "Perfect Paperback                    99.966112\n",
      "Item model number                    99.979311\n",
      "Pages                                99.998930\n",
      "Binding                              99.998930\n",
      "Ruling Type                          99.999643\n",
      "Color                                99.996790\n",
      "Sheet Size                           99.999643\n",
      "Cover Material                       99.999287\n",
      "Theme                                99.998930\n",
      "Number of Items                      99.997503\n",
      "Special Feature                      99.999287\n",
      "Calendar                             99.931155\n",
      "Product Bundle                       99.984305\n",
      "Flexibound                           99.977527\n",
      "Roughcut                             99.998216\n",
      "Misc                                 99.999643\n",
      "Vinyl Bound                          99.999287\n",
      "X Ray for textbooks                  99.998216\n",
      "Diary                                99.983591\n",
      "Hardcover spiral                     99.988942\n",
      "Paperback Shinsho                    99.999643\n",
      "Pop Up                               99.998930\n",
      "Map                                  99.994293\n",
      "Format                               99.999287\n",
      "Contributor                          99.998930\n",
      "Manufacturer recommended age         99.995719\n",
      "Domestic Shipping                    99.993579\n",
      "Best Sellers Rank                    99.987515\n",
      "International Shipping               99.993579\n",
      "Department                           99.996790\n",
      "Manufacturer                         99.987158\n",
      "Paperback Bunko                      99.997503\n",
      "Tankobon Hardcover                   99.998216\n",
      "Card Book                            99.998573\n",
      "Ring bound                           99.993579\n",
      "Journal                              99.999287\n",
      "Type of item                         99.998216\n",
      "Pricing                              99.998216\n",
      "Poster                               99.998573\n",
      "Bookmark                             99.997146\n",
      "Staple Bound                         99.995719\n",
      "Toy                                  99.998930\n",
      "Digital Audiobook                    99.999643\n",
      "DVD                                  99.999643\n",
      "Turtleback                           99.998573\n",
      "Single Issue Magazine                99.997503\n",
      "Bath Book                            99.998573\n",
      "Wall Chart                           99.998930\n",
      "Accessory                            99.998930\n",
      "Unbound                              99.999287\n",
      "Printed Access Code                  99.998930\n",
      "Print on Demand Paperback            99.998216\n",
      "Textbook Binding                     99.999643\n",
      "Game                                 99.999287\n",
      "Audio CD Library Binding             99.999287\n",
      "Edition                              99.999643\n",
      "Brand                                99.997146\n",
      "Publication Date                     99.999287\n",
      "Item Dimensions LxWxH                99.999287\n",
      "Genre                                99.998573\n",
      "Other display features               99.998216\n",
      "Preloaded Digital Audio Player       99.998216\n",
      "Batteries Required?                  99.998573\n",
      "Hardware Platform                    99.999643\n",
      "Rag Book                             99.998930\n",
      "Item Package Quantity                99.998930\n",
      "Batteries Included?                  99.999287\n",
      "Stationery                           99.998930\n",
      "Blu ray                              99.999643\n",
      "Digital                              99.998930\n",
      "Hardcover Comic                      99.999643\n",
      "Size                                 99.999287\n",
      "Manufacturer Part Number             99.998573\n",
      "Part Number                          99.999287\n",
      "DVD ROM                              99.999287\n",
      "Item Package Dimensions L x W x H    99.999643\n",
      "Item Dimensions  LxWxH               99.999643\n",
      "Brand Name                           99.999643\n",
      "Model Year                           99.999643\n",
      "Style                                99.999643\n",
      "Suggested Users                      99.999643\n",
      "Package Weight                       99.999643\n",
      "Runtime                              99.999643\n",
      "Batteries                            99.999287\n",
      "Mook                                 99.999643\n",
      "Notebook                             99.999643\n",
      "Print Magazine                       99.999643\n",
      "Diskette                             99.999643\n",
      "Sports Apparel                       99.999643\n"
     ]
    }
   ],
   "source": [
    "details_expanded\n",
    "nan_percentage = details_expanded.isna().mean() * 100\n",
    "print(\"\\nPercentage: \\n\",nan_percentage.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the extreme sparsity observed in most columns—and given that the Amazon metadata table primarily represents general titles rather than specific editions or formats—we have chosen, for now, to retain only those columns that facilitate the integration of this table's entities with other datasets (it's child products). These columns are Language, Dimensions, Publisher, Item Weight, ISBN 13, and ISBN 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fields = [\"Language\", \"Dimensions\", \"Publisher\", \"Item Weight\", \n",
    "                 \"ISBN 13\", \"ISBN 10\"]\n",
    "\n",
    "# Copy only available target fields from details_expanded into df\n",
    "available_fields = [col for col in details_expanded.columns if col in target_fields]\n",
    "df[available_fields] = details_expanded[available_fields]\n",
    "\n",
    "# Find all columns in details_expanded whose name contains \"date\" (case-insensitive)\n",
    "date_columns = [col for col in details_expanded.columns if 'date' in col.lower()]\n",
    "\n",
    "# Create a unified PublicationDate column by taking the first non-null value across the date columns\n",
    "if date_columns:\n",
    "    df['PublicationDate'] = details_expanded[date_columns].bfill(axis=1).iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_about</th>\n",
       "      <th>author_avatar</th>\n",
       "      <th>details</th>\n",
       "      <th>Language</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Item Weight</th>\n",
       "      <th>ISBN 13</th>\n",
       "      <th>ISBN 10</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>found_unit</th>\n",
       "      <th>weight_oz</th>\n",
       "      <th>iso_language</th>\n",
       "      <th>main_category_real</th>\n",
       "      <th>PublicationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Swan Thieves</td>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1559</td>\n",
       "      <td>[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>B0062GL89I</td>\n",
       "      <td>Elizabeth Kostova</td>\n",
       "      <td>[\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a l...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg</td>\n",
       "      <td>{'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...</td>\n",
       "      <td>English</td>\n",
       "      <td>5.5 x 2.15 x 5.75 inches</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>1.05 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>5.75</td>\n",
       "      <td>pound</td>\n",
       "      <td>16.8</td>\n",
       "      <td>en</td>\n",
       "      <td>Books</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audiobooks</td>\n",
       "      <td>Death in a White Tie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1033</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...</td>\n",
       "      <td>[\"Books\",\"Mystery, Thriller &amp; Suspense\",\"Mystery\",\"Traditional Detectives\"]</td>\n",
       "      <td>B001F1ZPDK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Audiobooks</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category                 title  \\\n",
       "0         Books      The Swan Thieves   \n",
       "1    Audiobooks  Death in a White Tie   \n",
       "\n",
       "                                     subtitle  average_rating  rating_number  \\\n",
       "0  Audio CD – Bargain Price, November 3, 2010             4.2           1559   \n",
       "1                                         NaN             4.5           1033   \n",
       "\n",
       "                                                                                              features  \\\n",
       "0  [\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...   \n",
       "1                                                                                                   []   \n",
       "\n",
       "  description  price images videos  \\\n",
       "0          []    NaN     []     []   \n",
       "1          []    NaN     []     []   \n",
       "\n",
       "                                                                                                 store  \\\n",
       "0  Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...   \n",
       "1  Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...   \n",
       "\n",
       "                                                                    categories  \\\n",
       "0                             [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "1  [\"Books\",\"Mystery, Thriller & Suspense\",\"Mystery\",\"Traditional Detectives\"]   \n",
       "\n",
       "  parent_asin        author_name  \\\n",
       "0  B0062GL89I  Elizabeth Kostova   \n",
       "1  B001F1ZPDK                NaN   \n",
       "\n",
       "                                                                                          author_about  \\\n",
       "0  [\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a l...   \n",
       "1                                                                                                  NaN   \n",
       "\n",
       "                                                 author_avatar  \\\n",
       "0  https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg   \n",
       "1                                                          NaN   \n",
       "\n",
       "                                                                                               details  \\\n",
       "0  {'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...   \n",
       "1                                                                                                   {}   \n",
       "\n",
       "  Language                Dimensions  \\\n",
       "0  English  5.5 x 2.15 x 5.75 inches   \n",
       "1      NaN                       NaN   \n",
       "\n",
       "                                            Publisher  Item Weight ISBN 13  \\\n",
       "0  Hachette Audio; Una Rei edition (November 3, 2010)  1.05 pounds     NaN   \n",
       "1                                                 NaN          NaN     NaN   \n",
       "\n",
       "  ISBN 10  length  width  height found_unit  weight_oz iso_language  \\\n",
       "0     NaN     5.5   2.15    5.75      pound       16.8           en   \n",
       "1     NaN     NaN    NaN     NaN       None        NaN         None   \n",
       "\n",
       "  main_category_real PublicationDate  \n",
       "0              Books             NaN  \n",
       "1         Audiobooks             NaN  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "main_category          0.065278\n",
       "title                  0.000000\n",
       "subtitle              20.200615\n",
       "average_rating         0.000000\n",
       "rating_number          0.000000\n",
       "features               0.000000\n",
       "description            0.000000\n",
       "price                 29.716271\n",
       "images                 0.000000\n",
       "videos                 0.000000\n",
       "store                  3.589239\n",
       "categories             0.000000\n",
       "parent_asin            0.000000\n",
       "author_name           17.954398\n",
       "author_about          17.954398\n",
       "author_avatar         17.953328\n",
       "details                0.000000\n",
       "Language              19.686236\n",
       "Dimensions            34.149848\n",
       "Publisher             21.550771\n",
       "Item Weight           26.822265\n",
       "ISBN 13               30.399018\n",
       "ISBN 10               32.670919\n",
       "length                34.149848\n",
       "width                 34.149848\n",
       "height                34.265779\n",
       "found_unit            26.822621\n",
       "weight_oz             26.822265\n",
       "iso_language          19.888135\n",
       "main_category_real     0.065278\n",
       "PublicationDate       19.160442\n",
       "dtype: float64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_percentage = df.isna().mean() * 100\n",
    "print(\"\\nPercentage: \\n\")\n",
    "nan_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Dimensions\" Column**\n",
    "\n",
    "Dimensions values of books should probably be numeric, not text.\n",
    "On Amazon, product dimensions are typically presented in the standard format of Length x Width x Height (LxWxH). Producing those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_about</th>\n",
       "      <th>author_avatar</th>\n",
       "      <th>details</th>\n",
       "      <th>Language</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Item Weight</th>\n",
       "      <th>ISBN 13</th>\n",
       "      <th>ISBN 10</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Swan Thieves</td>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1559</td>\n",
       "      <td>[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>B0062GL89I</td>\n",
       "      <td>Elizabeth Kostova</td>\n",
       "      <td>[\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a l...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg</td>\n",
       "      <td>{'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...</td>\n",
       "      <td>English</td>\n",
       "      <td>5.5 x 2.15 x 5.75 inches</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>1.05 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audible Audiobooks</td>\n",
       "      <td>Death in a White Tie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1033</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...</td>\n",
       "      <td>[\"Books\",\"Mystery, Thriller &amp; Suspense\",\"Mystery\",\"Traditional Detectives\"]</td>\n",
       "      <td>B001F1ZPDK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        main_category                 title  \\\n",
       "0               Books      The Swan Thieves   \n",
       "1  Audible Audiobooks  Death in a White Tie   \n",
       "\n",
       "                                     subtitle  average_rating  rating_number  \\\n",
       "0  Audio CD – Bargain Price, November 3, 2010             4.2           1559   \n",
       "1                                         NaN             4.5           1033   \n",
       "\n",
       "                                                                                              features  \\\n",
       "0  [\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...   \n",
       "1                                                                                                   []   \n",
       "\n",
       "  description  price images videos  \\\n",
       "0          []    NaN     []     []   \n",
       "1          []    NaN     []     []   \n",
       "\n",
       "                                                                                                 store  \\\n",
       "0  Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...   \n",
       "1  Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...   \n",
       "\n",
       "                                                                    categories  \\\n",
       "0                             [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "1  [\"Books\",\"Mystery, Thriller & Suspense\",\"Mystery\",\"Traditional Detectives\"]   \n",
       "\n",
       "  parent_asin        author_name  \\\n",
       "0  B0062GL89I  Elizabeth Kostova   \n",
       "1  B001F1ZPDK                NaN   \n",
       "\n",
       "                                                                                          author_about  \\\n",
       "0  [\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a l...   \n",
       "1                                                                                                  NaN   \n",
       "\n",
       "                                                 author_avatar  \\\n",
       "0  https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg   \n",
       "1                                                          NaN   \n",
       "\n",
       "                                                                                               details  \\\n",
       "0  {'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...   \n",
       "1                                                                                                   {}   \n",
       "\n",
       "  Language                Dimensions  \\\n",
       "0  English  5.5 x 2.15 x 5.75 inches   \n",
       "1      NaN                       NaN   \n",
       "\n",
       "                                            Publisher  Item Weight ISBN 13  \\\n",
       "0  Hachette Audio; Una Rei edition (November 3, 2010)  1.05 pounds     NaN   \n",
       "1                                                 NaN          NaN     NaN   \n",
       "\n",
       "  ISBN 10  length  width  height  \n",
       "0     NaN     5.5   2.15    5.75  \n",
       "1     NaN     NaN    NaN     NaN  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"length\", \"width\", \"height\"]] = (\n",
    "    df[\"Dimensions\"]\n",
    "    .str.replace(\" inches\", \"\", regex=False)    # remove \" inches\"\n",
    "    .str.split(\"x\", expand=True)                # split by 'x'\n",
    "    .apply(lambda col: col.str.strip())         # strip spaces around each part\n",
    "    .astype(float)                              # convert to float\n",
    ")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Item Weight\" Column**\n",
    "\n",
    "Check how many distinct formats exist (and how often each occurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item Weight\n",
       "NaN             75193\n",
       "1 pounds         5445\n",
       "8 ounces         5152\n",
       "9.6 ounces       5067\n",
       "6.4 ounces       4846\n",
       "                ...  \n",
       "233 pounds          1\n",
       "10.41 pounds        1\n",
       "4.51 pounds         1\n",
       "12.57 pounds        1\n",
       "5.56 pounds         1\n",
       "Name: count, Length: 1142, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Item Weight\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there other units than pounds and ounces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_unit\n",
      "ounce    137883\n",
      "None      75194\n",
      "pound     67261\n",
      "Name: count, dtype: int64\n",
      "163563    1.41 Grams\n",
      "Name: Item Weight, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r'(pound|pounds|ounce|ounces)', re.IGNORECASE)\n",
    "\n",
    "def find_unit(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = pattern.search(str(text))\n",
    "    if match:\n",
    "        return match.group(1).lower()  # e.g. \"pound\", \"pounds\", etc.\n",
    "    return None\n",
    "\n",
    "# Suppose df[\"Item Weight\"] is your column\n",
    "df[\"found_unit\"] = df[\"Item Weight\"].apply(find_unit)\n",
    "\n",
    "# Now see how many times each recognized unit is found:\n",
    "print(df[\"found_unit\"].value_counts(dropna=False))\n",
    "\n",
    "# Next, let's see the raw strings that did NOT match (found_unit is None)\n",
    "unmatched = df[df[\"found_unit\"].isna() & df[\"Item Weight\"].notna()]\n",
    "print(unmatched[\"Item Weight\"].head(50))  # or show how many, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units are ounce, pound and at least one entry with Grams. Let's convert eveything to ounces/ oz (other datasets are primarly american too so let's have a US unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex capturing:\n",
    "#   - numeric part (\\d+(\\.\\d+)?)\n",
    "#   - optional space\n",
    "#   - unit group (pound/pounds/ounce/ounces/gram/grams)\n",
    "pattern = re.compile(r'(?P<value>\\d+(\\.\\d+)?)(\\s+)?(?P<unit>pound|pounds|ounce|ounces|gram|grams)', re.IGNORECASE)\n",
    "\n",
    "def parse_weight_to_oz(text):\n",
    "    if pd.isna(text):\n",
    "        return np.nan  # Nothing to parse\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    match = pattern.search(text_lower)\n",
    "    if match:\n",
    "        val_str = match.group('value')   # e.g. \"1.41\"\n",
    "        unit_str = match.group('unit')   # e.g. \"grams\"\n",
    "        \n",
    "        # Convert numeric part to float\n",
    "        val = float(val_str)\n",
    "        \n",
    "        # Convert everything into ounces\n",
    "        if unit_str in (\"pound\", \"pounds\"):\n",
    "            return val * 16\n",
    "        elif unit_str in (\"ounce\", \"ounces\"):\n",
    "            return val\n",
    "        elif unit_str in (\"gram\", \"grams\"):\n",
    "            return val * 0.035274\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan  # not recognized\n",
    "\n",
    "# Apply to your DataFrame:\n",
    "df[\"weight_oz\"] = df[\"Item Weight\"].apply(parse_weight_to_oz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to have worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item Weight</th>\n",
       "      <th>weight_oz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.05 pounds</td>\n",
       "      <td>16.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2 pounds</td>\n",
       "      <td>19.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.2 ounces</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.61 ounces</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.59 pounds</td>\n",
       "      <td>25.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.6 ounces</td>\n",
       "      <td>5.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.79 pounds</td>\n",
       "      <td>28.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.72 ounces</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.6 ounces</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.6 ounces</td>\n",
       "      <td>9.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5 ounces</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.8 ounces</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.2 ounces</td>\n",
       "      <td>5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Item Weight  weight_oz\n",
       "0   1.05 pounds      16.80\n",
       "1           NaN        NaN\n",
       "2    1.2 pounds      19.20\n",
       "3    3.2 ounces       3.20\n",
       "4   2.61 ounces       2.61\n",
       "5   1.59 pounds      25.44\n",
       "6           NaN        NaN\n",
       "7    5.6 ounces       5.60\n",
       "8   1.79 pounds      28.64\n",
       "9   2.72 ounces       2.72\n",
       "10  12.6 ounces      12.60\n",
       "11   9.6 ounces       9.60\n",
       "12          NaN        NaN\n",
       "13          NaN        NaN\n",
       "14   3.5 ounces       3.50\n",
       "15          NaN        NaN\n",
       "16          NaN        NaN\n",
       "17   8.8 ounces       8.80\n",
       "18   5.2 ounces       5.20\n",
       "19          NaN        NaN"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Item Weight\", \"weight_oz\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Language\" Column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so weird entries with multiple languages but they are a minority.\n",
    "\n",
    "Lets parse the entries with singular values into ISO language codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language\n",
       "English                                                                                                                                                                                                                                                                                                              219180\n",
       "NaN                                                                                                                                                                                                                                                                                                                   55188\n",
       "Spanish                                                                                                                                                                                                                                                                                                                3034\n",
       "French                                                                                                                                                                                                                                                                                                                  882\n",
       "German                                                                                                                                                                                                                                                                                                                  844\n",
       "                                                                                                                                                                                                                                                                                                                      ...  \n",
       "Batak                                                                                                                                                                                                                                                                                                                     1\n",
       "Multilingual, English, French                                                                                                                                                                                                                                                                                             1\n",
       "English, German, Italian, Portuguese, Japanese, Spanish, Chinese, English, Czech, French, German, Danish, Dutch, Finnish, Japanese, Greek, Icelandic, Polish, Portuguese, Korean, Hungarian, Spanish, Norwegian, Russian, Turkish, English, French, Swedish, German, Italian, Thai, Japanese, Portuguese, Spanish         1\n",
       "Efik                                                                                                                                                                                                                                                                                                                      1\n",
       "English, French, German, Italian, Japanese, Portuguese, Russian, Spanish, Dutch, Danish, Finnish, Norwegian, Swedish, Icelandic                                                                                                                                                                                           1\n",
       "Name: count, Length: 116, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Language\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "def convert_language_to_iso(language_value):\n",
    "    \"\"\"\n",
    "    Convert a language name to its corresponding ISO 639-1 code.\n",
    "    Uses pycountry to perform the lookup.\n",
    "    \"\"\"\n",
    "    if pd.isnull(language_value):\n",
    "        return None\n",
    "    try:\n",
    "        # pycountry.languages.lookup() can handle common language names.\n",
    "        language = pycountry.languages.lookup(language_value)\n",
    "        # Check if the language has an alpha_2 attribute.\n",
    "        if hasattr(language, 'alpha_2'):\n",
    "            return language.alpha_2\n",
    "        else:\n",
    "            # Fallback: return the alpha_3 code if available.\n",
    "            return language.alpha_3\n",
    "    except LookupError:\n",
    "        # Return None or a custom value if the language is not found.\n",
    "        return None\n",
    "\n",
    "# Apply the conversion function to the Language column.\n",
    "df['iso_language'] = df['Language'].apply(convert_language_to_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>iso_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280333</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280334</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280336</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280337</th>\n",
       "      <td>English</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280338 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language iso_language\n",
       "0       English           en\n",
       "1           NaN         None\n",
       "2       English           en\n",
       "3       English           en\n",
       "4       English           en\n",
       "...         ...          ...\n",
       "280333  English           en\n",
       "280334  English           en\n",
       "280335      NaN         None\n",
       "280336  English           en\n",
       "280337  English           en\n",
       "\n",
       "[280338 rows x 2 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Language\", \"iso_language\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"main_category\"**\n",
    "\n",
    "As expected, this dataset does not exclusively regard physical books, it also has digital and audio categories. \n",
    "\n",
    "Let's fix up the categorical values it takes and <ins>manually inspect the 3 low-count categories that might be misclassified or do not belong here</ins>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_category\n",
       "Books                  220653\n",
       "Audible Audiobooks      37106\n",
       "Buy a Kindle            22385\n",
       "Toys & Games                9\n",
       "Musical Instruments         1\n",
       "Amazon Home                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_main_category = df[\"main_category\"].value_counts()\n",
    "counts_main_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking these 11 entries, all seemed to be misclassified with their subcategories, meaning all are physical books that should be classified as such - Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_about</th>\n",
       "      <th>author_avatar</th>\n",
       "      <th>details</th>\n",
       "      <th>Language</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Item Weight</th>\n",
       "      <th>ISBN 13</th>\n",
       "      <th>ISBN 10</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>found_unit</th>\n",
       "      <th>weight_oz</th>\n",
       "      <th>iso_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6591</td>\n",
       "      <td>[\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...</td>\n",
       "      <td>[\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...</td>\n",
       "      <td>9.95</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Klutz</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Games\"]</td>\n",
       "      <td>B005K6VBW0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Release date': 'January 1, 1993', 'Manufacturer recommended age': '6 years and up', 'Item mode...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>8.10</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27594</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Melissa &amp; Doug Children's Book - Poke-A-Dot: The Wheels on the Bus Wild Safari (Board Book with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4206</td>\n",
       "      <td>[\"20-page interactive sturdy board book with buttons to press and “pop” on every page\",\"Sing alo...</td>\n",
       "      <td>[\"The dots in the book go Pop, Pop, Pop! Poke the irresistible click-to-count buttons to hear sa...</td>\n",
       "      <td>13.99</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51ltoJ24nwL._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[{\"user_id\":\"AFIXMSDC2WB7FEBQPBWPHMHQIAEQ\",\"title\":\"This is just so super cute!!!\",\"url\":\"https:...</td>\n",
       "      <td>Melissa &amp; Doug</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Literature &amp; Fiction\",\"Poetry\",\"Stories In Verse\"]</td>\n",
       "      <td>1601694202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Release date': 'July 31, 2019', 'Manufacturer recommended age': '3 years and up', 'Department'...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.6 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pound</td>\n",
       "      <td>41.60</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91601</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6410</td>\n",
       "      <td>[\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...</td>\n",
       "      <td>[\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...</td>\n",
       "      <td>9.95</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[{\"user_id\":\"\",\"title\":\"Sew Mini Treats\",\"url\":\"https://www.amazon.com/vdp/8fecce474818439eba2de...</td>\n",
       "      <td>Klutz</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Games\"]</td>\n",
       "      <td>B001BSKVSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'International Shipping': 'This item can be shipped to select countries outside of the U.S.  Le...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>8.10</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98566</th>\n",
       "      <td>Musical Instruments</td>\n",
       "      <td>Faber Piano Adventures Primer Level Learning Library Pack - Lesson, Theory, Performance, and Tec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2883</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"PIANO ADVENTURES PRIMER PACK\"]</td>\n",
       "      <td>34.92</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/41nAmGM1fQL._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Faber Piano Adventures</td>\n",
       "      <td>[\"Books\",\"Arts &amp; Photography\",\"Music\",\"Songbooks\",\"Piano\"]</td>\n",
       "      <td>B00A82TY52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Date First Available': 'November 15, 2012', 'Product Dimensions': '9 x 2 x 12 inches', 'Item W...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.53 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>3.53</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124869</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>GirlZone Arts and Crafts Unicorn and Mermaids Coloring Book for Girls 4 to 10 Years, Birthday Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2199</td>\n",
       "      <td>[\"114 beautifully illustrated calming and soothing pages for coloring.\",\"Exclusive designs inclu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>12.90</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/618UyKNfEiL._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[{\"user_id\":\"AH6RIL2VF2D34J22LILF6PQ3GNNA\",\"title\":\"You Tuber Sarah Cantwell unboxes the stunnin...</td>\n",
       "      <td>GirlZone</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Activity Books\",\"Coloring Books\"]</td>\n",
       "      <td>B075NKR4K9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Manufacturer recommended age': '5 years and up', 'Item model number': 'Colouring Book UK', 'Ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.76 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>1.76</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180876</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>DITTY BIRD The Wheels on The Bus Book | Nursery Rhymes for Babies | Sound Books for Toddlers 1-3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2654</td>\n",
       "      <td>[\"😀 INTERACTIVE MUSICAL SONG BOOK FOR BABY, TODDLER, 1-3 YEARS OLD: Read, listen and sing along ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>14.82</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51KmOb3dEvL._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>DITTY BIRD</td>\n",
       "      <td>[\"Books\",\"Children's Books\"]</td>\n",
       "      <td>0994606710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Release date': 'October 13, 2023', 'Manufacturer recommended age': '2 months and up', 'Languag...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.3 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>9.30</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208215</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6410</td>\n",
       "      <td>[\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...</td>\n",
       "      <td>[\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...</td>\n",
       "      <td>9.95</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[{\"user_id\":\"\",\"title\":\"Sew Mini Treats\",\"url\":\"https://www.amazon.com/vdp/8fecce474818439eba2de...</td>\n",
       "      <td>Klutz</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Games\"]</td>\n",
       "      <td>1878257536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'International Shipping': 'This item can be shipped to select countries outside of the U.S.  Le...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>8.10</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215184</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6650</td>\n",
       "      <td>[\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>9.95</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Klutz</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Games\"]</td>\n",
       "      <td>B000HZZ19S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Release date': 'January 1, 1993', 'Manufacturer recommended age': '6 years and up', 'Departmen...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>8.10</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229888</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6603</td>\n",
       "      <td>[\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...</td>\n",
       "      <td>[\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...</td>\n",
       "      <td>9.95</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Klutz</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Games\"]</td>\n",
       "      <td>B0049RNDOA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Release date': 'January 1, 1993', 'Manufacturer recommended age': '6 years and up', 'Item mode...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>8.10</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256923</th>\n",
       "      <td>Amazon Home</td>\n",
       "      <td>I Will Find You</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>24916</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"1\"]</td>\n",
       "      <td>26.85</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51qsS9YdWmL._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[]</td>\n",
       "      <td>CALOTO</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>1529135516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Date First Available': 'September 27, 2022', 'Language': 'English', 'Manufacturer': 'CENTURY',...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.14 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pound</td>\n",
       "      <td>18.24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267344</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Klutz Nail Style Studio Book Kit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1667</td>\n",
       "      <td>[\"Simple steps to painting 25 stunning designs\",\"Comes with 60 page instructions, practice polis...</td>\n",
       "      <td>[\"Product Description\",\"Nail Style Studio is the perfect big sister to our bestselling title Nai...</td>\n",
       "      <td>23.73</td>\n",
       "      <td>[{\"large\":\"https://m.media-amazon.com/images/I/51GVIDdmbeL._AC_.jpg\",\"thumb\":\"https://m.media-am...</td>\n",
       "      <td>[{\"user_id\":\"AGGSA7D2A5XO6KFVBCR5GOOUFGIQ\",\"title\":\"Horrible product!!!\",\"url\":\"https://www.amaz...</td>\n",
       "      <td>Klutz</td>\n",
       "      <td>[\"Books\",\"Children's Books\",\"Activities, Crafts &amp; Games\",\"Crafts &amp; Hobbies\"]</td>\n",
       "      <td>0545561639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'International Shipping': 'This item is not eligible for international shipping.  Learn More', ...</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01 ounces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ounce</td>\n",
       "      <td>1.01</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              main_category  \\\n",
       "15034          Toys & Games   \n",
       "27594          Toys & Games   \n",
       "91601          Toys & Games   \n",
       "98566   Musical Instruments   \n",
       "124869         Toys & Games   \n",
       "180876         Toys & Games   \n",
       "208215         Toys & Games   \n",
       "215184         Toys & Games   \n",
       "229888         Toys & Games   \n",
       "256923          Amazon Home   \n",
       "267344         Toys & Games   \n",
       "\n",
       "                                                                                                      title  \\\n",
       "15034                            Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height   \n",
       "27594   Melissa & Doug Children's Book - Poke-A-Dot: The Wheels on the Bus Wild Safari (Board Book with ...   \n",
       "91601                            Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height   \n",
       "98566   Faber Piano Adventures Primer Level Learning Library Pack - Lesson, Theory, Performance, and Tec...   \n",
       "124869  GirlZone Arts and Crafts Unicorn and Mermaids Coloring Book for Girls 4 to 10 Years, Birthday Gi...   \n",
       "180876  DITTY BIRD The Wheels on The Bus Book | Nursery Rhymes for Babies | Sound Books for Toddlers 1-3...   \n",
       "208215                           Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height   \n",
       "215184                           Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height   \n",
       "229888                           Cat's Cradle (Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height   \n",
       "256923                                                                                      I Will Find You   \n",
       "267344                                                                     Klutz Nail Style Studio Book Kit   \n",
       "\n",
       "       subtitle  average_rating  rating_number  \\\n",
       "15034       NaN             4.7           6591   \n",
       "27594       NaN             4.7           4206   \n",
       "91601       NaN             4.7           6410   \n",
       "98566       NaN             4.8           2883   \n",
       "124869      NaN             4.8           2199   \n",
       "180876      NaN             4.7           2654   \n",
       "208215      NaN             4.7           6410   \n",
       "215184      NaN             4.7           6650   \n",
       "229888      NaN             4.7           6603   \n",
       "256923      NaN             4.5          24916   \n",
       "267344      NaN             4.3           1667   \n",
       "\n",
       "                                                                                                   features  \\\n",
       "15034   [\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...   \n",
       "27594   [\"20-page interactive sturdy board book with buttons to press and “pop” on every page\",\"Sing alo...   \n",
       "91601   [\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...   \n",
       "98566                                                                                                    []   \n",
       "124869  [\"114 beautifully illustrated calming and soothing pages for coloring.\",\"Exclusive designs inclu...   \n",
       "180876  [\"😀 INTERACTIVE MUSICAL SONG BOOK FOR BABY, TODDLER, 1-3 YEARS OLD: Read, listen and sing along ...   \n",
       "208215  [\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...   \n",
       "215184  [\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...   \n",
       "229888  [\"Learn how to make The Cup and Saucer, The Witch's Broom and Jacob's Ladder\",\"Comes with loop o...   \n",
       "256923                                                                                                   []   \n",
       "267344  [\"Simple steps to painting 25 stunning designs\",\"Comes with 60 page instructions, practice polis...   \n",
       "\n",
       "                                                                                                description  \\\n",
       "15034   [\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...   \n",
       "27594   [\"The dots in the book go Pop, Pop, Pop! Poke the irresistible click-to-count buttons to hear sa...   \n",
       "91601   [\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...   \n",
       "98566                                                                      [\"PIANO ADVENTURES PRIMER PACK\"]   \n",
       "124869                                                                                                   []   \n",
       "180876                                                                                                   []   \n",
       "208215  [\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...   \n",
       "215184                                                                                                   []   \n",
       "229888  [\"In today's hi-tech world, people have completely forgotten how to make The Cup and Saucer, The...   \n",
       "256923                                                                                                [\"1\"]   \n",
       "267344  [\"Product Description\",\"Nail Style Studio is the perfect big sister to our bestselling title Nai...   \n",
       "\n",
       "        price  \\\n",
       "15034    9.95   \n",
       "27594   13.99   \n",
       "91601    9.95   \n",
       "98566   34.92   \n",
       "124869  12.90   \n",
       "180876  14.82   \n",
       "208215   9.95   \n",
       "215184   9.95   \n",
       "229888   9.95   \n",
       "256923  26.85   \n",
       "267344  23.73   \n",
       "\n",
       "                                                                                                     images  \\\n",
       "15034   [{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "27594   [{\"large\":\"https://m.media-amazon.com/images/I/51ltoJ24nwL._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "91601   [{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "98566   [{\"large\":\"https://m.media-amazon.com/images/I/41nAmGM1fQL._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "124869  [{\"large\":\"https://m.media-amazon.com/images/I/618UyKNfEiL._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "180876  [{\"large\":\"https://m.media-amazon.com/images/I/51KmOb3dEvL._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "208215  [{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "215184  [{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "229888  [{\"large\":\"https://m.media-amazon.com/images/I/51ADflGz2-L._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "256923  [{\"large\":\"https://m.media-amazon.com/images/I/51qsS9YdWmL._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "267344  [{\"large\":\"https://m.media-amazon.com/images/I/51GVIDdmbeL._AC_.jpg\",\"thumb\":\"https://m.media-am...   \n",
       "\n",
       "                                                                                                     videos  \\\n",
       "15034                                                                                                    []   \n",
       "27594   [{\"user_id\":\"AFIXMSDC2WB7FEBQPBWPHMHQIAEQ\",\"title\":\"This is just so super cute!!!\",\"url\":\"https:...   \n",
       "91601   [{\"user_id\":\"\",\"title\":\"Sew Mini Treats\",\"url\":\"https://www.amazon.com/vdp/8fecce474818439eba2de...   \n",
       "98566                                                                                                    []   \n",
       "124869  [{\"user_id\":\"AH6RIL2VF2D34J22LILF6PQ3GNNA\",\"title\":\"You Tuber Sarah Cantwell unboxes the stunnin...   \n",
       "180876                                                                                                   []   \n",
       "208215  [{\"user_id\":\"\",\"title\":\"Sew Mini Treats\",\"url\":\"https://www.amazon.com/vdp/8fecce474818439eba2de...   \n",
       "215184                                                                                                   []   \n",
       "229888                                                                                                   []   \n",
       "256923                                                                                                   []   \n",
       "267344  [{\"user_id\":\"AGGSA7D2A5XO6KFVBCR5GOOUFGIQ\",\"title\":\"Horrible product!!!\",\"url\":\"https://www.amaz...   \n",
       "\n",
       "                         store  \\\n",
       "15034                    Klutz   \n",
       "27594           Melissa & Doug   \n",
       "91601                    Klutz   \n",
       "98566   Faber Piano Adventures   \n",
       "124869                GirlZone   \n",
       "180876              DITTY BIRD   \n",
       "208215                   Klutz   \n",
       "215184                   Klutz   \n",
       "229888                   Klutz   \n",
       "256923                  CALOTO   \n",
       "267344                   Klutz   \n",
       "\n",
       "                                                                                         categories  \\\n",
       "15034                             [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Games\"]   \n",
       "27594               [\"Books\",\"Children's Books\",\"Literature & Fiction\",\"Poetry\",\"Stories In Verse\"]   \n",
       "91601                             [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Games\"]   \n",
       "98566                                    [\"Books\",\"Arts & Photography\",\"Music\",\"Songbooks\",\"Piano\"]   \n",
       "124869  [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Activity Books\",\"Coloring Books\"]   \n",
       "180876                                                                 [\"Books\",\"Children's Books\"]   \n",
       "208215                            [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Games\"]   \n",
       "215184                            [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Games\"]   \n",
       "229888                            [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Games\"]   \n",
       "256923                                             [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "267344                 [\"Books\",\"Children's Books\",\"Activities, Crafts & Games\",\"Crafts & Hobbies\"]   \n",
       "\n",
       "       parent_asin author_name author_about author_avatar  \\\n",
       "15034   B005K6VBW0         NaN          NaN           NaN   \n",
       "27594   1601694202         NaN          NaN           NaN   \n",
       "91601   B001BSKVSG         NaN          NaN           NaN   \n",
       "98566   B00A82TY52         NaN          NaN           NaN   \n",
       "124869  B075NKR4K9         NaN          NaN           NaN   \n",
       "180876  0994606710         NaN          NaN           NaN   \n",
       "208215  1878257536         NaN          NaN           NaN   \n",
       "215184  B000HZZ19S         NaN          NaN           NaN   \n",
       "229888  B0049RNDOA         NaN          NaN           NaN   \n",
       "256923  1529135516         NaN          NaN           NaN   \n",
       "267344  0545561639         NaN          NaN           NaN   \n",
       "\n",
       "                                                                                                    details  \\\n",
       "15034   {'Release date': 'January 1, 1993', 'Manufacturer recommended age': '6 years and up', 'Item mode...   \n",
       "27594   {'Release date': 'July 31, 2019', 'Manufacturer recommended age': '3 years and up', 'Department'...   \n",
       "91601   {'International Shipping': 'This item can be shipped to select countries outside of the U.S.  Le...   \n",
       "98566   {'Date First Available': 'November 15, 2012', 'Product Dimensions': '9 x 2 x 12 inches', 'Item W...   \n",
       "124869  {'Manufacturer recommended age': '5 years and up', 'Item model number': 'Colouring Book UK', 'Ma...   \n",
       "180876  {'Release date': 'October 13, 2023', 'Manufacturer recommended age': '2 months and up', 'Languag...   \n",
       "208215  {'International Shipping': 'This item can be shipped to select countries outside of the U.S.  Le...   \n",
       "215184  {'Release date': 'January 1, 1993', 'Manufacturer recommended age': '6 years and up', 'Departmen...   \n",
       "229888  {'Release date': 'January 1, 1993', 'Manufacturer recommended age': '6 years and up', 'Item mode...   \n",
       "256923  {'Date First Available': 'September 27, 2022', 'Language': 'English', 'Manufacturer': 'CENTURY',...   \n",
       "267344  {'International Shipping': 'This item is not eligible for international shipping.  Learn More', ...   \n",
       "\n",
       "       Language Dimensions Publisher  Item Weight ISBN 13 ISBN 10  length  \\\n",
       "15034   English        NaN       NaN   8.1 ounces     NaN     NaN     NaN   \n",
       "27594   English        NaN       NaN   2.6 pounds     NaN     NaN     NaN   \n",
       "91601   English        NaN       NaN   8.1 ounces     NaN     NaN     NaN   \n",
       "98566       NaN        NaN       NaN  3.53 ounces     NaN     NaN     NaN   \n",
       "124869      NaN        NaN       NaN  1.76 ounces     NaN     NaN     NaN   \n",
       "180876  English        NaN       NaN   9.3 ounces     NaN     NaN     NaN   \n",
       "208215  English        NaN       NaN   8.1 ounces     NaN     NaN     NaN   \n",
       "215184  English        NaN       NaN   8.1 ounces     NaN     NaN     NaN   \n",
       "229888  English        NaN       NaN   8.1 ounces     NaN     NaN     NaN   \n",
       "256923  English        NaN       NaN  1.14 pounds     NaN     NaN     NaN   \n",
       "267344  English        NaN       NaN  1.01 ounces     NaN     NaN     NaN   \n",
       "\n",
       "        width  height found_unit  weight_oz iso_language  \n",
       "15034     NaN     NaN      ounce       8.10           en  \n",
       "27594     NaN     NaN      pound      41.60           en  \n",
       "91601     NaN     NaN      ounce       8.10           en  \n",
       "98566     NaN     NaN      ounce       3.53         None  \n",
       "124869    NaN     NaN      ounce       1.76         None  \n",
       "180876    NaN     NaN      ounce       9.30           en  \n",
       "208215    NaN     NaN      ounce       8.10           en  \n",
       "215184    NaN     NaN      ounce       8.10           en  \n",
       "229888    NaN     NaN      ounce       8.10           en  \n",
       "256923    NaN     NaN      pound      18.24           en  \n",
       "267344    NaN     NaN      ounce       1.01           en  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[df['main_category'].isin([\"Toys & Games\", \"Musical Instruments\", \"Amazon Home\"])]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['main_category'].isin([\"Toys & Games\", \"Musical Instruments\", \"Amazon Home\"]), 'main_category'] = \"Books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_category\n",
       "Books                 220664\n",
       "Audible Audiobooks     37106\n",
       "Buy a Kindle           22385\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_main_category = df[\"main_category\"].value_counts()\n",
    "counts_main_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize other attributes categorical values into single words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_category'] = df['main_category'].replace({\n",
    "    \"Audible Audiobooks\": \"Audiobooks\",\n",
    "    \"Buy a Kindle\": \"Ebooks\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_category\n",
       "Books         220664\n",
       "Audiobooks     37106\n",
       "Ebooks         22385\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_main_category = df[\"main_category\"].value_counts()\n",
    "counts_main_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"title\"**\n",
    "\n",
    "This column and its attributes seem to be fairly clean. However, some, like those in the prior table, have additional unwanted info in them (i.e., Cat's Cradle <ins>(Klutz Activity Kit) 9.44\" Length x 0.5\" Width x 5.75\" Height</ins>).\n",
    "\n",
    "We are not sure how to remove this information efficiently without having it do it manually one by one. Will keep the titles has they are in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Publication Dates - on \"subtitle\" and \"Publisher\"**\n",
    "\n",
    "Most dates seem to be in the \"subtitle\" and \"details\". However, their format is very inconsistent though-out the entries.\n",
    "\n",
    "We will extract it to **ISO 8601**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtitle</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hardcover – Large Print, May 18, 2016</td>\n",
       "      <td>Thorndike Press Large Print; Large Print edition (May 18, 2016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mass Market Paperback – January 1, 1981</td>\n",
       "      <td>Bantam (January 1, 1981)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MP3 CD – Unabridged, July 15, 2007</td>\n",
       "      <td>Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280333</th>\n",
       "      <td>Hardcover – September 13, 2011</td>\n",
       "      <td>Doubleday (September 13, 2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280334</th>\n",
       "      <td>Audio CD – Unabridged, November 4, 2014</td>\n",
       "      <td>Macmillan Audio; Unabridged edition (November 4, 2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280335</th>\n",
       "      <td>Paperback – January 1, 1994</td>\n",
       "      <td>Scribner; 31685th edition (January 1, 1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280336</th>\n",
       "      <td>Paperback – March 3, 2020</td>\n",
       "      <td>Cloud Forest Press (March 3, 2020)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280337</th>\n",
       "      <td>Paperback – December 2, 2008</td>\n",
       "      <td>VIZ Media LLC (December 2, 2008)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280338 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          subtitle  \\\n",
       "0       Audio CD – Bargain Price, November 3, 2010   \n",
       "1                                              NaN   \n",
       "2            Hardcover – Large Print, May 18, 2016   \n",
       "3          Mass Market Paperback – January 1, 1981   \n",
       "4               MP3 CD – Unabridged, July 15, 2007   \n",
       "...                                            ...   \n",
       "280333              Hardcover – September 13, 2011   \n",
       "280334     Audio CD – Unabridged, November 4, 2014   \n",
       "280335                 Paperback – January 1, 1994   \n",
       "280336                   Paperback – March 3, 2020   \n",
       "280337                Paperback – December 2, 2008   \n",
       "\n",
       "                                                              Publisher  \n",
       "0                    Hachette Audio; Una Rei edition (November 3, 2010)  \n",
       "1                                                                   NaN  \n",
       "2       Thorndike Press Large Print; Large Print edition (May 18, 2016)  \n",
       "3                                              Bantam (January 1, 1981)  \n",
       "4             Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)  \n",
       "...                                                                 ...  \n",
       "280333                                   Doubleday (September 13, 2011)  \n",
       "280334           Macmillan Audio; Unabridged edition (November 4, 2014)  \n",
       "280335                      Scribner; 31685th edition (January 1, 1994)  \n",
       "280336                               Cloud Forest Press (March 3, 2020)  \n",
       "280337                                 VIZ Media LLC (December 2, 2008)  \n",
       "\n",
       "[280338 rows x 2 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"subtitle\", \"Publisher\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to extract the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(text):\n",
    "    \"\"\"\n",
    "    Attempt to extract a date from the provided text using fuzzy parsing.\n",
    "    Returns an ISO formatted date string (YYYY-MM-DD) or None if no date is found.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            dt = parser.parse(text, fuzzy=True)\n",
    "            return dt.date().isoformat()  # Returns date in 'YYYY-MM-DD' format\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def get_publication_date(row):\n",
    "    \"\"\"\n",
    "    Try to extract a date from the row by checking 'subtitle', then 'Publisher', then 'details'.\n",
    "    Returns the first valid date found, or None if none are found.\n",
    "    \"\"\"\n",
    "    for col in ['subtitle', 'Publisher', 'details']:\n",
    "        date_candidate = extract_date(row.get(col, ''))\n",
    "        if date_candidate:\n",
    "            return date_candidate\n",
    "    return None\n",
    "\n",
    "# Create the unified PublicationDate column by applying the function to each row.\n",
    "df['PublicationDate'] = df.apply(get_publication_date, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtitle</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>details</th>\n",
       "      <th>PublicationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>{'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...</td>\n",
       "      <td>2010-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hardcover – Large Print, May 18, 2016</td>\n",
       "      <td>Thorndike Press Large Print; Large Print edition (May 18, 2016)</td>\n",
       "      <td>{'ISBN 13': '978-1410490117', 'Language': 'English', 'ISBN 10': '9781410490117', 'Dimensions': '...</td>\n",
       "      <td>2016-05-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mass Market Paperback – January 1, 1981</td>\n",
       "      <td>Bantam (January 1, 1981)</td>\n",
       "      <td>{'ISBN 13': '978-0553230888', 'Language': 'English', 'Mass Market Paperback': '0 pages', 'ISBN 1...</td>\n",
       "      <td>1981-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MP3 CD – Unabridged, July 15, 2007</td>\n",
       "      <td>Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)</td>\n",
       "      <td>{'ISBN 13': '978-1400154234', 'Language': 'English', 'ISBN 10': '1400154235', 'Dimensions': '5.3...</td>\n",
       "      <td>2015-07-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     subtitle  \\\n",
       "0  Audio CD – Bargain Price, November 3, 2010   \n",
       "1                                         NaN   \n",
       "2       Hardcover – Large Print, May 18, 2016   \n",
       "3     Mass Market Paperback – January 1, 1981   \n",
       "4          MP3 CD – Unabridged, July 15, 2007   \n",
       "\n",
       "                                                         Publisher  \\\n",
       "0               Hachette Audio; Una Rei edition (November 3, 2010)   \n",
       "1                                                              NaN   \n",
       "2  Thorndike Press Large Print; Large Print edition (May 18, 2016)   \n",
       "3                                         Bantam (January 1, 1981)   \n",
       "4        Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)   \n",
       "\n",
       "                                                                                               details  \\\n",
       "0  {'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...   \n",
       "1                                                                                                   {}   \n",
       "2  {'ISBN 13': '978-1410490117', 'Language': 'English', 'ISBN 10': '9781410490117', 'Dimensions': '...   \n",
       "3  {'ISBN 13': '978-0553230888', 'Language': 'English', 'Mass Market Paperback': '0 pages', 'ISBN 1...   \n",
       "4  {'ISBN 13': '978-1400154234', 'Language': 'English', 'ISBN 10': '1400154235', 'Dimensions': '5.3...   \n",
       "\n",
       "  PublicationDate  \n",
       "0      2010-11-03  \n",
       "1            None  \n",
       "2      2016-05-18  \n",
       "3      1981-01-01  \n",
       "4      2015-07-03  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['subtitle', 'Publisher', 'details', 'PublicationDate']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Null percentages now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_category          0.065278\n",
       "title                  0.000000\n",
       "subtitle              20.200615\n",
       "average_rating         0.000000\n",
       "rating_number          0.000000\n",
       "features               0.000000\n",
       "description            0.000000\n",
       "price                 29.716271\n",
       "images                 0.000000\n",
       "videos                 0.000000\n",
       "store                  3.589239\n",
       "categories             0.000000\n",
       "parent_asin            0.000000\n",
       "author_name           17.954398\n",
       "author_about          17.954398\n",
       "author_avatar         17.953328\n",
       "details                0.000000\n",
       "Language              19.686236\n",
       "Dimensions            34.149848\n",
       "Publisher             21.550771\n",
       "Item Weight           26.822265\n",
       "ISBN 13               30.399018\n",
       "ISBN 10               32.670919\n",
       "length                34.149848\n",
       "width                 34.149848\n",
       "height                34.265779\n",
       "found_unit            26.822621\n",
       "weight_oz             26.822265\n",
       "iso_language          19.888135\n",
       "main_category_real     0.065278\n",
       "PublicationDate       19.160442\n",
       "Publisher_Clean       21.550771\n",
       "Edition               58.994856\n",
       "dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Publisher\" Column and edition**\n",
    "\n",
    "Cleaning the Publisher column, which contains inconsistent formatting and embedded publication dates. The goal is to separate this information for better data quality and analysis.\n",
    "\n",
    "- Cleaning the Publisher column: Many entries include extraneous details such as dates or edition notes. I'm standardizing this column to retain only the publisher names.\n",
    "\n",
    "- Creating an Edition column: Information about the edition, which typically appears between the publisher name and the date, is extracted into a new column named edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thorndike Press Large Print; Large Print edition (May 18, 2016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bantam (January 1, 1981)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Opal Reyne (June 18, 2023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Avon (November 1, 1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CreateSpace Independent Publishing Platform (August 1, 2018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Brilliance Audio; Unabridged edition (April 21, 2015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brilliance Audio; Unabridged edition (March 23, 2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lomic Books (January 19, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Transworld Digital; Reissue edition (August 10, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Brilliance Audio; Unabridged edition (December 30, 2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Arrow (April 1, 2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Crown; First Edition (August 15, 2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Publisher\n",
       "0                Hachette Audio; Una Rei edition (November 3, 2010)\n",
       "1                                                               NaN\n",
       "2   Thorndike Press Large Print; Large Print edition (May 18, 2016)\n",
       "3                                          Bantam (January 1, 1981)\n",
       "4         Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)\n",
       "5                                        Opal Reyne (June 18, 2023)\n",
       "6                                                               NaN\n",
       "7                                           Avon (November 1, 1990)\n",
       "8      CreateSpace Independent Publishing Platform (August 1, 2018)\n",
       "9             Brilliance Audio; Unabridged edition (April 21, 2015)\n",
       "10            Brilliance Audio; Unabridged edition (March 23, 2011)\n",
       "11                                   Lomic Books (January 19, 2019)\n",
       "12            Transworld Digital; Reissue edition (August 10, 2017)\n",
       "13                                                              NaN\n",
       "14         Brilliance Audio; Unabridged edition (December 30, 2014)\n",
       "15                                                              NaN\n",
       "16                                                              NaN\n",
       "17                                            Arrow (April 1, 2008)\n",
       "18                           Crown; First Edition (August 15, 2000)\n",
       "19                                                              NaN"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Publisher\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_publisher_and_edition(value):\n",
    "    \"\"\"\n",
    "    Parse a publisher string with expected formats:\n",
    "      \"PublisherName; Edition Info (Date)\" or \"PublisherName (Date)\"\n",
    "    Returns a Series with:\n",
    "      - Clean publisher name (without the date and edition details)\n",
    "      - Edition info (if available, otherwise None)\n",
    "    The date part is ignored.\n",
    "    \"\"\"\n",
    "    if pd.isnull(value):\n",
    "        return pd.Series([None, None])\n",
    "    \n",
    "    value = value.strip()\n",
    "    \n",
    "    # Regex pattern explanation:\n",
    "    #   ^(?P<publisher>[^;(]+)       : Capture everything until a semicolon or parenthesis as publisher name.\n",
    "    #   (?:;\\s*(?P<edition>[^()]+))?   : Optionally capture edition info after a semicolon.\n",
    "    #   \\s*\\([^)]*\\)$                : Ignore the date portion enclosed in parentheses.\n",
    "    pattern = re.compile(\n",
    "        r'^(?P<publisher>[^;(]+)(?:;\\s*(?P<edition>[^()]+))?\\s*\\([^)]*\\)$'\n",
    "    )\n",
    "    \n",
    "    match = pattern.match(value)\n",
    "    if match:\n",
    "        publisher_clean = match.group('publisher').strip() if match.group('publisher') else None\n",
    "        edition = match.group('edition').strip() if match.group('edition') else None\n",
    "        return pd.Series([publisher_clean, edition])\n",
    "    else:\n",
    "        # If no match is found, return the original value as publisher and None for edition.\n",
    "        return pd.Series([value, None])\n",
    "\n",
    "# Apply the parsing function to create new columns for a cleaned publisher name and edition info.\n",
    "df[['Publisher_Clean', 'Edition']] = df['Publisher'].apply(parse_publisher_and_edition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publisher_Clean</th>\n",
       "      <th>Edition</th>\n",
       "      <th>PublicationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>Hachette Audio</td>\n",
       "      <td>Una Rei edition</td>\n",
       "      <td>2010-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thorndike Press Large Print; Large Print edition (May 18, 2016)</td>\n",
       "      <td>Thorndike Press Large Print</td>\n",
       "      <td>Large Print edition</td>\n",
       "      <td>2016-05-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bantam (January 1, 1981)</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>None</td>\n",
       "      <td>1981-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)</td>\n",
       "      <td>Tantor Audio</td>\n",
       "      <td>MP3 - Unabridged CD edition</td>\n",
       "      <td>2015-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280333</th>\n",
       "      <td>Doubleday (September 13, 2011)</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280334</th>\n",
       "      <td>Macmillan Audio; Unabridged edition (November 4, 2014)</td>\n",
       "      <td>Macmillan Audio</td>\n",
       "      <td>Unabridged edition</td>\n",
       "      <td>2014-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280335</th>\n",
       "      <td>Scribner; 31685th edition (January 1, 1994)</td>\n",
       "      <td>Scribner</td>\n",
       "      <td>31685th edition</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280336</th>\n",
       "      <td>Cloud Forest Press (March 3, 2020)</td>\n",
       "      <td>Cloud Forest Press</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280337</th>\n",
       "      <td>VIZ Media LLC (December 2, 2008)</td>\n",
       "      <td>VIZ Media LLC</td>\n",
       "      <td>None</td>\n",
       "      <td>2008-12-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280338 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Publisher  \\\n",
       "0                    Hachette Audio; Una Rei edition (November 3, 2010)   \n",
       "1                                                                   NaN   \n",
       "2       Thorndike Press Large Print; Large Print edition (May 18, 2016)   \n",
       "3                                              Bantam (January 1, 1981)   \n",
       "4             Tantor Audio; MP3 - Unabridged CD edition (July 15, 2007)   \n",
       "...                                                                 ...   \n",
       "280333                                   Doubleday (September 13, 2011)   \n",
       "280334           Macmillan Audio; Unabridged edition (November 4, 2014)   \n",
       "280335                      Scribner; 31685th edition (January 1, 1994)   \n",
       "280336                               Cloud Forest Press (March 3, 2020)   \n",
       "280337                                 VIZ Media LLC (December 2, 2008)   \n",
       "\n",
       "                    Publisher_Clean                      Edition  \\\n",
       "0                    Hachette Audio              Una Rei edition   \n",
       "1                              None                         None   \n",
       "2       Thorndike Press Large Print          Large Print edition   \n",
       "3                            Bantam                         None   \n",
       "4                      Tantor Audio  MP3 - Unabridged CD edition   \n",
       "...                             ...                          ...   \n",
       "280333                    Doubleday                         None   \n",
       "280334              Macmillan Audio           Unabridged edition   \n",
       "280335                     Scribner              31685th edition   \n",
       "280336           Cloud Forest Press                         None   \n",
       "280337                VIZ Media LLC                         None   \n",
       "\n",
       "       PublicationDate  \n",
       "0           2010-11-03  \n",
       "1                 None  \n",
       "2           2016-05-18  \n",
       "3           1981-01-01  \n",
       "4           2015-07-03  \n",
       "...                ...  \n",
       "280333      2011-09-13  \n",
       "280334      2014-11-04  \n",
       "280335      1994-01-01  \n",
       "280336      2020-03-03  \n",
       "280337      2008-12-02  \n",
       "\n",
       "[280338 rows x 4 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Publisher','Publisher_Clean', 'Edition', 'PublicationDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['main_category', 'title', 'subtitle', 'avg_rating', 'num_ratings',\n",
       "       'features', 'description', 'price', 'images', 'videos', 'store',\n",
       "       'categories', 'parent_asin', 'author_name', 'author_bio',\n",
       "       'author_avatar', 'product_details', 'language', 'dimensions_raw',\n",
       "       'publisher_raw', 'item_weight_raw', 'isbn_13', 'isbn_10', 'length',\n",
       "       'width', 'height', 'dimension_unit', 'weight_oz', 'lang_iso',\n",
       "       'publication_date', 'publisher', 'edition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main_category', 'title', 'subtitle', 'avg_rating', 'num_ratings', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'parent_asin', 'author_name', 'author_bio', 'author_avatar', 'product_details', 'language', 'dimensions_raw', 'publisher_raw', 'item_weight_raw', 'isbn_13', 'isbn_10', 'length', 'width', 'height', 'dimension_unit', 'weight_oz', 'lang_iso', 'publication_date', 'publisher', 'edition']\n"
     ]
    }
   ],
   "source": [
    "rename_mapping = {\n",
    "    'main_category': 'main_category',\n",
    "    'title': 'title',\n",
    "    'subtitle': 'subtitle',\n",
    "    'average_rating': 'avg_rating',\n",
    "    'rating_number': 'num_ratings',\n",
    "    'features': 'features',\n",
    "    'description': 'description',\n",
    "    'price': 'price',\n",
    "    'images': 'images',\n",
    "    'videos': 'videos',\n",
    "    'store': 'store',\n",
    "    'categories': 'categories',\n",
    "    'parent_asin': 'parent_asin',\n",
    "    'author_name': 'author_name',\n",
    "    'author_about': 'author_bio',\n",
    "    'author_avatar': 'author_avatar',\n",
    "    'details': 'product_details',\n",
    "    'Language': 'language',\n",
    "    'Dimensions': 'dimensions_raw',\n",
    "    'Publisher': 'publisher_raw',\n",
    "    'Item Weight': 'item_weight_raw',\n",
    "    'ISBN 13': 'isbn_13',\n",
    "    'ISBN 10': 'isbn_10',\n",
    "    'length': 'length',\n",
    "    'width': 'width',\n",
    "    'height': 'height',\n",
    "    'found_unit': 'dimension_unit',\n",
    "    'weight_oz': 'weight_oz',\n",
    "    'iso_language': 'lang_iso',\n",
    "    'PublicationDate': 'publication_date',\n",
    "    'Publisher_Clean': 'publisher',\n",
    "    'Edition': 'edition'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Print the standardized column names\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/1_amazon/amazon_meta_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_bio</th>\n",
       "      <th>author_avatar</th>\n",
       "      <th>product_details</th>\n",
       "      <th>language</th>\n",
       "      <th>dimensions_raw</th>\n",
       "      <th>publisher_raw</th>\n",
       "      <th>item_weight_raw</th>\n",
       "      <th>isbn_13</th>\n",
       "      <th>isbn_10</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>dimension_unit</th>\n",
       "      <th>weight_oz</th>\n",
       "      <th>lang_iso</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>edition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books</td>\n",
       "      <td>The Swan Thieves</td>\n",
       "      <td>Audio CD – Bargain Price, November 3, 2010</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1559</td>\n",
       "      <td>[\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...</td>\n",
       "      <td>[\"Books\",\"Literature &amp; Fiction\",\"Genre Fiction\"]</td>\n",
       "      <td>B0062GL89I</td>\n",
       "      <td>Elizabeth Kostova</td>\n",
       "      <td>[\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a l...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg</td>\n",
       "      <td>{'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...</td>\n",
       "      <td>English</td>\n",
       "      <td>5.5 x 2.15 x 5.75 inches</td>\n",
       "      <td>Hachette Audio; Una Rei edition (November 3, 2010)</td>\n",
       "      <td>1.05 pounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>5.75</td>\n",
       "      <td>pound</td>\n",
       "      <td>16.8</td>\n",
       "      <td>en</td>\n",
       "      <td>2010-11-03</td>\n",
       "      <td>Hachette Audio</td>\n",
       "      <td>Una Rei edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audiobooks</td>\n",
       "      <td>Death in a White Tie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1033</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...</td>\n",
       "      <td>[\"Books\",\"Mystery, Thriller &amp; Suspense\",\"Mystery\",\"Traditional Detectives\"]</td>\n",
       "      <td>B001F1ZPDK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category                 title  \\\n",
       "0         Books      The Swan Thieves   \n",
       "1    Audiobooks  Death in a White Tie   \n",
       "\n",
       "                                     subtitle  avg_rating  num_ratings  \\\n",
       "0  Audio CD – Bargain Price, November 3, 2010         4.2         1559   \n",
       "1                                         NaN         4.5         1033   \n",
       "\n",
       "                                                                                              features  \\\n",
       "0  [\"Psychiatrist Andrew Marlowe has a perfectly ordered life--solitary, perhaps, but full of devot...   \n",
       "1                                                                                                   []   \n",
       "\n",
       "  description  price images videos  \\\n",
       "0          []    NaN     []     []   \n",
       "1          []    NaN     []     []   \n",
       "\n",
       "                                                                                                 store  \\\n",
       "0  Elizabeth Kostova (Author),  Treat Williams (Reader),  Anne Heche (Reader),  Erin Cottrell (Read...   \n",
       "1  Ngaio Marsh  (Author),     Benedict Cumberbatch  (Narrator),     Hachette Audio UK  (Publisher) ...   \n",
       "\n",
       "                                                                    categories  \\\n",
       "0                             [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"]   \n",
       "1  [\"Books\",\"Mystery, Thriller & Suspense\",\"Mystery\",\"Traditional Detectives\"]   \n",
       "\n",
       "  parent_asin        author_name  \\\n",
       "0  B0062GL89I  Elizabeth Kostova   \n",
       "1  B001F1ZPDK                NaN   \n",
       "\n",
       "                                                                                            author_bio  \\\n",
       "0  [\"Elizabeth Kostova's engrossing debut novel is the culmination of ten years of research and a l...   \n",
       "1                                                                                                  NaN   \n",
       "\n",
       "                                                 author_avatar  \\\n",
       "0  https://m.media-amazon.com/images/I/31E3SMTce5L._SY600_.jpg   \n",
       "1                                                          NaN   \n",
       "\n",
       "                                                                                       product_details  \\\n",
       "0  {'Language': 'English', 'Dimensions': '5.5 x 2.15 x 5.75 inches', 'Publisher': 'Hachette Audio; ...   \n",
       "1                                                                                                   {}   \n",
       "\n",
       "  language            dimensions_raw  \\\n",
       "0  English  5.5 x 2.15 x 5.75 inches   \n",
       "1      NaN                       NaN   \n",
       "\n",
       "                                        publisher_raw item_weight_raw isbn_13  \\\n",
       "0  Hachette Audio; Una Rei edition (November 3, 2010)     1.05 pounds     NaN   \n",
       "1                                                 NaN             NaN     NaN   \n",
       "\n",
       "  isbn_10  length  width  height dimension_unit  weight_oz lang_iso  \\\n",
       "0     NaN     5.5   2.15    5.75          pound       16.8       en   \n",
       "1     NaN     NaN    NaN     NaN            NaN        NaN      NaN   \n",
       "\n",
       "  publication_date       publisher          edition  \n",
       "0       2010-11-03  Hachette Audio  Una Rei edition  \n",
       "1              NaN             NaN              NaN  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Rows**\n",
    "\n",
    "It has 10.683.301 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/1_amazon/amazon_reviews_filtered.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10683301 entries, 0 to 10683300\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   user_id      object\n",
      " 1   parent_asin  object\n",
      " 2   rating       int64 \n",
      " 3   timestamp    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 326.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Value Lenghts**\n",
    "\n",
    "Min Length: The length of the shortest string in the column.\n",
    "\n",
    "Max Length: The length of the longest string in the column.\n",
    "\n",
    "Mean Length: The average length of all the string values in the column.\n",
    "\n",
    "Median Length: The middle string length when all lengths are sorted, indicating the 50th percentile.\n",
    "\n",
    "\"rating\" and  \"timestamp\" are null cuz the type is incompatible, aka contain no string values, so no string length statistics are computed (all values are None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String length statistics per column:\n",
      "user_id: {'min_length': 28, 'max_length': 36, 'mean_length': 28.001065962664537, 'median_length': 28.0}\n",
      "parent_asin: {'min_length': 10, 'max_length': 10, 'mean_length': 10.0, 'median_length': 10.0}\n",
      "rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "timestamp: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n"
     ]
    }
   ],
   "source": [
    "df_length = pd.DataFrame({\n",
    "    col: df[col].map(lambda x: len(x) if isinstance(x, str) else np.nan)\n",
    "    for col in df.columns\n",
    "})\n",
    "\n",
    "# Compute statistics for each column, only if there are any valid (non-NaN) string lengths.\n",
    "stats = {}\n",
    "for col in df_length.columns:\n",
    "    lengths = df_length[col].dropna()\n",
    "    if lengths.empty:\n",
    "        stats[col] = {\n",
    "            'min_length': None,\n",
    "            'max_length': None,\n",
    "            'mean_length': None,\n",
    "            'median_length': None\n",
    "        }\n",
    "    else:\n",
    "        stats[col] = {\n",
    "            'min_length': lengths.min(),\n",
    "            'max_length': lengths.max(),\n",
    "            'mean_length': lengths.mean(),\n",
    "            'median_length': lengths.median()\n",
    "        }\n",
    "\n",
    "print(\"String length statistics per column:\")\n",
    "for col, stat in stats.items():\n",
    "    print(f\"{col}: {stat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Null Percentages per Column:**\n",
    "\n",
    "No nulls in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id        0.0\n",
       "parent_asin    0.0\n",
       "rating         0.0\n",
       "timestamp      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinality**\n",
    "The number of distinct values per column is high on all columns as expect (not categoricals), ratings that is a categorical (1-5 value no decimals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         5095240\n",
       "parent_asin      280173\n",
       "rating                5\n",
       "timestamp      10562253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Uniqueness**\n",
    "\n",
    "The proportion of distinct values compared to the total number of values indicates how many values are unique.\n",
    "\n",
    "(For instance, a uniqueness ratio of 1.0 means every value is unique; a ratio of 0.5 means only half of the values are unique).\n",
    "\n",
    "as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id        47.693498\n",
       "parent_asin     2.622532\n",
       "rating          0.000047\n",
       "timestamp      98.866942\n",
       "dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: x.nunique() / len(x) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Goodreads - 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) i) Metadata books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Datasets/2_3_goodreads/goodreads_meta_books.json'\n",
    "\n",
    "df = pd.read_json(file_path, lines=True) # JSON Lines (JSONL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is much cleaner than the amazon 23 data set bye.\n",
    "\n",
    "There are 34.758 entries/books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34758 entries, 0 to 34757\n",
      "Data columns (total 29 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title_without_series  34758 non-null  object \n",
      " 1   title                 34758 non-null  object \n",
      " 2   work_id               34758 non-null  int64  \n",
      " 3   book_id               34758 non-null  int64  \n",
      " 4   publication_year      34758 non-null  int64  \n",
      " 5   num_pages             32950 non-null  float64\n",
      " 6   ratings_count         34758 non-null  int64  \n",
      " 7   kindle_asin           34758 non-null  object \n",
      " 8   publisher             34758 non-null  object \n",
      " 9   authors               34758 non-null  object \n",
      " 10  format                34758 non-null  object \n",
      " 11  country_code          34758 non-null  object \n",
      " 12  series                34758 non-null  object \n",
      " 13  average_rating        34758 non-null  float64\n",
      " 14  similar_books         34758 non-null  object \n",
      " 15  image_url             34758 non-null  object \n",
      " 16  isbn13                34758 non-null  object \n",
      " 17  is_ebook              34758 non-null  bool   \n",
      " 18  text_reviews_count    34758 non-null  int64  \n",
      " 19  language_code         34758 non-null  object \n",
      " 20  description           34758 non-null  object \n",
      " 21  link                  34758 non-null  object \n",
      " 22  url                   34758 non-null  object \n",
      " 23  asin                  34758 non-null  object \n",
      " 24  popular_shelves       34758 non-null  object \n",
      " 25  edition_information   34758 non-null  object \n",
      " 26  isbn                  34758 non-null  object \n",
      " 27  publication_day       33100 non-null  float64\n",
      " 28  publication_month     34006 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(5), object(19)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### **Investigate what attributes are actually unique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"work_id\"** - ingroups the same book title regardless of edition.\n",
    "- **\"book_id\"** - it is unique for each book type and edition.\n",
    "- **\"kindle_asin\"** - the same title but from different editions have the same kindle_asin, meaning the same ebook. However, there are 0 that should probably be taken as NaN.\n",
    "- **\"isbn13\"** - also has many zeros that should be NaN\n",
    "- **\"isbn\"** - also has many zeros that should be NaN. ***NEED TO CHANGE NAME TO isbn10***\n",
    "- **\"asin\"** - also has  many zeros that should be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41107568</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16827462</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8812783</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21825181</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17763198</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>25442975</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>594714</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>21861745</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>16091023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>23636994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2943 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       work_id  count\n",
       "0     41107568     20\n",
       "1     16827462     17\n",
       "2      8812783     17\n",
       "3     21825181     13\n",
       "4     17763198     13\n",
       "...        ...    ...\n",
       "2938  25442975      2\n",
       "2939    594714      2\n",
       "2940  21861745      2\n",
       "2941  16091023      2\n",
       "2942  23636994      2\n",
       "\n",
       "[2943 rows x 2 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'work_id'\n",
    "dup_work_id = (\n",
    "    df[\"work_id\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_work_id.columns = [\"work_id\", \"count\"]\n",
    "dup_work_id = dup_work_id[dup_work_id[\"count\"] > 1]\n",
    "dup_work_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [book_id, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'book_id'\n",
    "dup_book_id = (\n",
    "    df[\"book_id\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_book_id.columns = [\"book_id\", \"count\"]\n",
    "dup_book_id = dup_book_id[dup_book_id[\"count\"] > 1]\n",
    "dup_book_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kindle_asin</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>5374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00655U3WE</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00L9B7IKE</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B003XF1XOQ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B005ZOBNOI</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>B00637B2UQ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>B003I55BIK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>B00A9V1PKY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>B004KSQDEA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>B00DNEH2L0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     kindle_asin  count\n",
       "0                  5374\n",
       "1     B00655U3WE      9\n",
       "2     B00L9B7IKE      9\n",
       "3     B003XF1XOQ      9\n",
       "4     B005ZOBNOI      9\n",
       "...          ...    ...\n",
       "2454  B00637B2UQ      2\n",
       "2455  B003I55BIK      2\n",
       "2456  B00A9V1PKY      2\n",
       "2457  B004KSQDEA      2\n",
       "2458  B00DNEH2L0      2\n",
       "\n",
       "[2459 rows x 2 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'kindle_asin'\n",
    "dup_kindle_asin = (\n",
    "    df[\"kindle_asin\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_kindle_asin.columns = [\"kindle_asin\", \"count\"]\n",
    "dup_kindle_asin = dup_kindle_asin[dup_kindle_asin[\"count\"] > 1]\n",
    "dup_kindle_asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn13</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>10220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  isbn13  count\n",
       "0         10220"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'isbn13'\n",
    "dup_isbn13 = (\n",
    "    df[\"isbn13\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn13.columns = [\"isbn13\", \"count\"]\n",
    "dup_isbn13 = dup_isbn13[dup_isbn13[\"count\"] > 1]\n",
    "dup_isbn13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>27585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  asin  count\n",
       "0       27585"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'asin'\n",
    "dup_asin = (\n",
    "    df[\"asin\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_asin.columns = [\"asin\", \"count\"]\n",
    "dup_asin = dup_asin[dup_asin[\"count\"] > 1]\n",
    "dup_asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>11988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  isbn  count\n",
       "0       11988"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'isbn'\n",
    "dup_isbn = (\n",
    "    df[\"isbn\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"isbn\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>22911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>11847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_ebook  count\n",
       "0     False  22911\n",
       "1      True  11847"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'is_ebook'\n",
    "dup_isbn = (\n",
    "    df[\"is_ebook\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"is_ebook\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Null Values - Number of Missing data points per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title_without_series       0\n",
       "title                      0\n",
       "work_id                    0\n",
       "book_id                    0\n",
       "publication_year           0\n",
       "num_pages               1808\n",
       "ratings_count              0\n",
       "kindle_asin                0\n",
       "publisher                  0\n",
       "authors                    0\n",
       "format                     0\n",
       "country_code               0\n",
       "series                     0\n",
       "average_rating             0\n",
       "similar_books              0\n",
       "image_url                  0\n",
       "isbn13                     0\n",
       "is_ebook                   0\n",
       "text_reviews_count         0\n",
       "language_code              0\n",
       "description                0\n",
       "link                       0\n",
       "url                        0\n",
       "asin                       0\n",
       "popular_shelves            0\n",
       "edition_information        0\n",
       "isbn                       0\n",
       "publication_day         1658\n",
       "publication_month        752\n",
       "dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>This is misleading many entries have empty slots that are not being registered as null.</ins>\n",
    "\n",
    "The real NaN values should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title_without_series        0\n",
       "title                       0\n",
       "work_id                     0\n",
       "book_id                     0\n",
       "publication_year            0\n",
       "num_pages                1808\n",
       "ratings_count               0\n",
       "kindle_asin              5374\n",
       "publisher                3802\n",
       "authors                     0\n",
       "format                   2932\n",
       "country_code                0\n",
       "series                      0\n",
       "average_rating              0\n",
       "similar_books               0\n",
       "image_url                   0\n",
       "isbn13                  10220\n",
       "is_ebook                    0\n",
       "text_reviews_count          0\n",
       "language_code            4791\n",
       "description               417\n",
       "link                        0\n",
       "url                         0\n",
       "asin                    27585\n",
       "popular_shelves             0\n",
       "edition_information     31847\n",
       "isbn                    11988\n",
       "publication_day          1658\n",
       "publication_month         752\n",
       "dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace empty strings with NaN across the DataFrame\n",
    "df = df.replace(\"\", np.nan)\n",
    "df = df.replace(\" \", np.nan)\n",
    "\n",
    "# Replace 0 of no value with NaN across specific columns\n",
    "columns_to_replace = [\"kindle_asin\", \"asin\", \"isbn\", \"isbn13\"]\n",
    "df[columns_to_replace] = df[columns_to_replace].replace(0, np.nan)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_without_series</th>\n",
       "      <th>title</th>\n",
       "      <th>work_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>kindle_asin</th>\n",
       "      <th>publisher</th>\n",
       "      <th>authors</th>\n",
       "      <th>format</th>\n",
       "      <th>country_code</th>\n",
       "      <th>series</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>similar_books</th>\n",
       "      <th>image_url</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>language_code</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>url</th>\n",
       "      <th>asin</th>\n",
       "      <th>popular_shelves</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>isbn</th>\n",
       "      <th>publication_day</th>\n",
       "      <th>publication_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>171715</td>\n",
       "      <td>12479382</td>\n",
       "      <td>2011</td>\n",
       "      <td>464.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Voyager</td>\n",
       "      <td>[{'role': '', 'author_id': '25307'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>[146510, 607115, 212862, 576243]</td>\n",
       "      <td>4.14</td>\n",
       "      <td>[618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...</td>\n",
       "      <td>https://images.gr-assets.com/books/1406623170m/12479382.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>en-GB</td>\n",
       "      <td>Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>B005JE1K9M</td>\n",
       "      <td>[{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seven Eves</td>\n",
       "      <td>Seven Eves</td>\n",
       "      <td>42299347</td>\n",
       "      <td>29457915</td>\n",
       "      <td>2015</td>\n",
       "      <td>867.0</td>\n",
       "      <td>1047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>[{'role': '', 'author_id': '545'}]</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>US</td>\n",
       "      <td>[]</td>\n",
       "      <td>3.98</td>\n",
       "      <td>[23197269, 25659450, 23533039, 23168809, 23168817, 15985402, 9424053, 20821159, 19367093, 238480...</td>\n",
       "      <td>https://images.gr-assets.com/books/1463823772m/29457915.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>146</td>\n",
       "      <td>eng</td>\n",
       "      <td>What would happen if the world were ending?\\nA catastrophic event renders the earth a ticking ti...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29457915-seven-eves</td>\n",
       "      <td>https://www.goodreads.com/book/show/29457915-seven-eves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '25568'}, {'name': 'science-fiction', 'count': '1975'}, {'name': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9780008132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يا بعده</td>\n",
       "      <td>يا بعده</td>\n",
       "      <td>42464976</td>\n",
       "      <td>22894535</td>\n",
       "      <td>2014</td>\n",
       "      <td>293.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lqS@ l`rby@ - lkwyt</td>\n",
       "      <td>[{'role': '', 'author_id': '2743894'}]</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>US</td>\n",
       "      <td>[]</td>\n",
       "      <td>3.85</td>\n",
       "      <td>[18068554, 7156236, 5781288, 22035215, 28106811, 13454387, 6652543, 23264107, 17911031, 22067995...</td>\n",
       "      <td>https://images.gr-assets.com/books/1407760836m/22894535.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>235</td>\n",
       "      <td>ara</td>\n",
       "      <td>'n Hbk ldrj@ tnsyny Hzny,\\n'n Hbk ldrj@ tlhyny `n nZrt lHsr@ fy `ywn hly,\\n'n Hbk ldrj@ tmHw fkr...</td>\n",
       "      <td>https://www.goodreads.com/book/show/22894535</td>\n",
       "      <td>https://www.goodreads.com/book/show/22894535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '4396'}, {'name': 'currently-reading', 'count': '420'}, {'name': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noah (5th Street, #1)</td>\n",
       "      <td>Noah (5th Street, #1)</td>\n",
       "      <td>18573480</td>\n",
       "      <td>22913057</td>\n",
       "      <td>2012</td>\n",
       "      <td>352.0</td>\n",
       "      <td>2521</td>\n",
       "      <td>B007GEUY7W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'role': '', 'author_id': '499602'}]</td>\n",
       "      <td>ebook</td>\n",
       "      <td>US</td>\n",
       "      <td>[348923]</td>\n",
       "      <td>3.80</td>\n",
       "      <td>[18393683, 15776233, 12807448, 14290481, 15827220, 13509846, 15724170, 15990356, 18793095, 16581...</td>\n",
       "      <td>https://images.gr-assets.com/books/1407955134m/22913057.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>205</td>\n",
       "      <td>eng</td>\n",
       "      <td>Veronica Cruz has been through hell and back. After disconnecting with the world two years ago t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/22913057-noah</td>\n",
       "      <td>https://www.goodreads.com/book/show/22913057-noah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '9208'}, {'name': 'romance', 'count': '305'}, {'name': 'currently-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016 on Goodreads</td>\n",
       "      <td>2016 on Goodreads</td>\n",
       "      <td>53943483</td>\n",
       "      <td>33232571</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'role': '', 'author_id': '5481957'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>[996473]</td>\n",
       "      <td>4.25</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://images.gr-assets.com/books/1480784881m/33232571.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1225</td>\n",
       "      <td>eng</td>\n",
       "      <td>You've probably found your way to this page because you follow reviews here on goodreads. Whethe...</td>\n",
       "      <td>https://www.goodreads.com/book/show/33232571-2016-on-goodreads</td>\n",
       "      <td>https://www.goodreads.com/book/show/33232571-2016-on-goodreads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '561'}, {'name': 'year-in-review', 'count': '87'}, {'name': 'revie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title_without_series  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)   \n",
       "1                                   Seven Eves   \n",
       "2                                      يا بعده   \n",
       "3                        Noah (5th Street, #1)   \n",
       "4                            2016 on Goodreads   \n",
       "\n",
       "                                         title   work_id   book_id  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)    171715  12479382   \n",
       "1                                   Seven Eves  42299347  29457915   \n",
       "2                                      يا بعده  42464976  22894535   \n",
       "3                        Noah (5th Street, #1)  18573480  22913057   \n",
       "4                            2016 on Goodreads  53943483  33232571   \n",
       "\n",
       "   publication_year  num_pages  ratings_count kindle_asin  \\\n",
       "0              2011      464.0           2037         NaN   \n",
       "1              2015      867.0           1047         NaN   \n",
       "2              2014      293.0           2100         NaN   \n",
       "3              2012      352.0           2521  B007GEUY7W   \n",
       "4              2016        NaN           1167         NaN   \n",
       "\n",
       "             publisher                                 authors     format  \\\n",
       "0       Harper Voyager    [{'role': '', 'author_id': '25307'}]        NaN   \n",
       "1       Harper Collins      [{'role': '', 'author_id': '545'}]  Hardcover   \n",
       "2  lqS@ l`rby@ - lkwyt  [{'role': '', 'author_id': '2743894'}]  Paperback   \n",
       "3                  NaN   [{'role': '', 'author_id': '499602'}]      ebook   \n",
       "4                  NaN  [{'role': '', 'author_id': '5481957'}]        NaN   \n",
       "\n",
       "  country_code                            series  average_rating  \\\n",
       "0           US  [146510, 607115, 212862, 576243]            4.14   \n",
       "1           US                                []            3.98   \n",
       "2           US                                []            3.85   \n",
       "3           US                          [348923]            3.80   \n",
       "4           US                          [996473]            4.25   \n",
       "\n",
       "                                                                                         similar_books  \\\n",
       "0  [618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...   \n",
       "1  [23197269, 25659450, 23533039, 23168809, 23168817, 15985402, 9424053, 20821159, 19367093, 238480...   \n",
       "2  [18068554, 7156236, 5781288, 22035215, 28106811, 13454387, 6652543, 23264107, 17911031, 22067995...   \n",
       "3  [18393683, 15776233, 12807448, 14290481, 15827220, 13509846, 15724170, 15990356, 18793095, 16581...   \n",
       "4                                                                                                   []   \n",
       "\n",
       "                                                     image_url isbn13  \\\n",
       "0  https://images.gr-assets.com/books/1406623170m/12479382.jpg    NaN   \n",
       "1  https://images.gr-assets.com/books/1463823772m/29457915.jpg    NaN   \n",
       "2  https://images.gr-assets.com/books/1407760836m/22894535.jpg    NaN   \n",
       "3  https://images.gr-assets.com/books/1407955134m/22913057.jpg    NaN   \n",
       "4  https://images.gr-assets.com/books/1480784881m/33232571.jpg    NaN   \n",
       "\n",
       "   is_ebook  text_reviews_count language_code  \\\n",
       "0      True                 112         en-GB   \n",
       "1     False                 146           eng   \n",
       "2     False                 235           ara   \n",
       "3      True                 205           eng   \n",
       "4     False                1225           eng   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0  Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...   \n",
       "1  What would happen if the world were ending?\\nA catastrophic event renders the earth a ticking ti...   \n",
       "2  'n Hbk ldrj@ tnsyny Hzny,\\n'n Hbk ldrj@ tlhyny `n nZrt lHsr@ fy `ywn hly,\\n'n Hbk ldrj@ tmHw fkr...   \n",
       "3  Veronica Cruz has been through hell and back. After disconnecting with the world two years ago t...   \n",
       "4  You've probably found your way to this page because you follow reviews here on goodreads. Whethe...   \n",
       "\n",
       "                                                                 link  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "1             https://www.goodreads.com/book/show/29457915-seven-eves   \n",
       "2                        https://www.goodreads.com/book/show/22894535   \n",
       "3                   https://www.goodreads.com/book/show/22913057-noah   \n",
       "4      https://www.goodreads.com/book/show/33232571-2016-on-goodreads   \n",
       "\n",
       "                                                                  url  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "1             https://www.goodreads.com/book/show/29457915-seven-eves   \n",
       "2                        https://www.goodreads.com/book/show/22894535   \n",
       "3                   https://www.goodreads.com/book/show/22913057-noah   \n",
       "4      https://www.goodreads.com/book/show/33232571-2016-on-goodreads   \n",
       "\n",
       "         asin  \\\n",
       "0  B005JE1K9M   \n",
       "1         NaN   \n",
       "2         NaN   \n",
       "3         NaN   \n",
       "4         NaN   \n",
       "\n",
       "                                                                                       popular_shelves  \\\n",
       "0  [{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...   \n",
       "1  [{'name': 'to-read', 'count': '25568'}, {'name': 'science-fiction', 'count': '1975'}, {'name': '...   \n",
       "2  [{'name': 'to-read', 'count': '4396'}, {'name': 'currently-reading', 'count': '420'}, {'name': '...   \n",
       "3  [{'name': 'to-read', 'count': '9208'}, {'name': 'romance', 'count': '305'}, {'name': 'currently-...   \n",
       "4  [{'name': 'to-read', 'count': '561'}, {'name': 'year-in-review', 'count': '87'}, {'name': 'revie...   \n",
       "\n",
       "  edition_information        isbn  publication_day  publication_month  \n",
       "0                 NaN         NaN              NaN                NaN  \n",
       "1                 NaN  9780008132              NaN                NaN  \n",
       "2                 NaN         NaN              NaN                NaN  \n",
       "3                 NaN         NaN              NaN                NaN  \n",
       "4                 NaN         NaN              NaN                NaN  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dicts into a string, then strip the leading/trailing brackets\n",
    "df[\"authors\"] = df[\"authors\"].astype(str).str.strip(\"[]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_without_series</th>\n",
       "      <th>title</th>\n",
       "      <th>work_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>kindle_asin</th>\n",
       "      <th>publisher</th>\n",
       "      <th>authors</th>\n",
       "      <th>format</th>\n",
       "      <th>country_code</th>\n",
       "      <th>series</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>similar_books</th>\n",
       "      <th>image_url</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>language_code</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>url</th>\n",
       "      <th>asin</th>\n",
       "      <th>popular_shelves</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>isbn</th>\n",
       "      <th>publication_day</th>\n",
       "      <th>publication_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>171715</td>\n",
       "      <td>12479382</td>\n",
       "      <td>2011</td>\n",
       "      <td>464.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Voyager</td>\n",
       "      <td>{'role': '', 'author_id': '25307'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>[146510, 607115, 212862, 576243]</td>\n",
       "      <td>4.14</td>\n",
       "      <td>[618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...</td>\n",
       "      <td>https://images.gr-assets.com/books/1406623170m/12479382.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>en-GB</td>\n",
       "      <td>Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>B005JE1K9M</td>\n",
       "      <td>[{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title_without_series  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)   \n",
       "\n",
       "                                         title  work_id   book_id  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)   171715  12479382   \n",
       "\n",
       "   publication_year  num_pages  ratings_count kindle_asin       publisher  \\\n",
       "0              2011      464.0           2037         NaN  Harper Voyager   \n",
       "\n",
       "                              authors format country_code  \\\n",
       "0  {'role': '', 'author_id': '25307'}    NaN           US   \n",
       "\n",
       "                             series  average_rating  \\\n",
       "0  [146510, 607115, 212862, 576243]            4.14   \n",
       "\n",
       "                                                                                         similar_books  \\\n",
       "0  [618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...   \n",
       "\n",
       "                                                     image_url isbn13  \\\n",
       "0  https://images.gr-assets.com/books/1406623170m/12479382.jpg    NaN   \n",
       "\n",
       "   is_ebook  text_reviews_count language_code  \\\n",
       "0      True                 112         en-GB   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0  Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...   \n",
       "\n",
       "                                                                 link  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "\n",
       "                                                                  url  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "\n",
       "         asin  \\\n",
       "0  B005JE1K9M   \n",
       "\n",
       "                                                                                       popular_shelves  \\\n",
       "0  [{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...   \n",
       "\n",
       "  edition_information isbn  publication_day  publication_month  \n",
       "0                 NaN  NaN              NaN                NaN  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 34758\n"
     ]
    }
   ],
   "source": [
    "# 1. Number of rows\n",
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String length**\n",
    "\n",
    "String length statistics for each column (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "title_without_series: {'min_length': 1, 'max_length': 203, 'mean_length': 33.23890902813741, 'median_length': 32.0}\n",
      "title: {'min_length': 1, 'max_length': 203, 'mean_length': 33.23890902813741, 'median_length': 32.0}\n",
      "work_id: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "book_id: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "publication_year: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "num_pages: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "ratings_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "kindle_asin: {'min_length': 10, 'max_length': 10, 'mean_length': 10.0, 'median_length': 10.0}\n",
      "publisher: {'min_length': 2, 'max_length': 73, 'mean_length': 14.883059826851014, 'median_length': 14.0}\n",
      "authors: {'min_length': 30, 'max_length': 2448, 'mean_length': 47.60777374992807, 'median_length': 36.0}\n",
      "format: {'min_length': 4, 'max_length': 31, 'mean_length': 9.89250926915101, 'median_length': 9.0}\n",
      "country_code: {'min_length': 2, 'max_length': 2, 'mean_length': 2.0, 'median_length': 2.0}\n",
      "series: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "average_rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "similar_books: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "image_url: {'min_length': 53, 'max_length': 88, 'mean_length': 61.65265550376892, 'median_length': 59.0}\n",
      "isbn13: {'min_length': 10, 'max_length': 13, 'mean_length': 12.998206862825006, 'median_length': 13.0}\n",
      "is_ebook: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "text_reviews_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "language_code: {'min_length': 2, 'max_length': 5, 'mean_length': 3.364868021490306, 'median_length': 3.0}\n",
      "description: {'min_length': 1, 'max_length': 8593, 'mean_length': 970.2610873300137, 'median_length': 916.0}\n",
      "link: {'min_length': 43, 'max_length': 121, 'mean_length': 60.435381782611195, 'median_length': 60.0}\n",
      "url: {'min_length': 43, 'max_length': 121, 'mean_length': 60.435381782611195, 'median_length': 60.0}\n",
      "asin: {'min_length': 9, 'max_length': 10, 'mean_length': 9.9998605883173, 'median_length': 10.0}\n",
      "popular_shelves: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "edition_information: {'min_length': 1, 'max_length': 84, 'mean_length': 11.945379594641016, 'median_length': 11.0}\n",
      "isbn: {'min_length': 4, 'max_length': 10, 'mean_length': 9.999385155906895, 'median_length': 10.0}\n",
      "publication_day: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "publication_month: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n"
     ]
    }
   ],
   "source": [
    "def string_length_stats(series):\n",
    "    # Compute lengths only for string values; ignore non-string values\n",
    "    lengths = series.dropna().map(lambda x: len(x) if isinstance(x, str) else np.nan).dropna()\n",
    "    if lengths.empty:\n",
    "        return {\"min_length\": None, \"max_length\": None, \"mean_length\": None, \"median_length\": None}\n",
    "    else:\n",
    "        return {\n",
    "            \"min_length\": lengths.min(),\n",
    "            \"max_length\": lengths.max(),\n",
    "            \"mean_length\": lengths.mean(),\n",
    "            \"median_length\": lengths.median()\n",
    "        }\n",
    "\n",
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title_without_series     0.000000\n",
       "title                    0.000000\n",
       "work_id                  0.000000\n",
       "book_id                  0.000000\n",
       "publication_year         0.000000\n",
       "num_pages                5.201680\n",
       "ratings_count            0.000000\n",
       "kindle_asin             15.461189\n",
       "publisher               10.938489\n",
       "authors                  0.000000\n",
       "format                   8.435468\n",
       "country_code             0.000000\n",
       "series                   0.000000\n",
       "average_rating           0.000000\n",
       "similar_books            0.000000\n",
       "image_url                0.000000\n",
       "isbn13                  29.403303\n",
       "is_ebook                 0.000000\n",
       "text_reviews_count       0.000000\n",
       "language_code           13.783877\n",
       "description              1.199724\n",
       "link                     0.000000\n",
       "url                      0.000000\n",
       "asin                    79.363024\n",
       "popular_shelves          0.000000\n",
       "edition_information     91.624950\n",
       "isbn                    34.489902\n",
       "publication_day          4.770125\n",
       "publication_month        2.163531\n",
       "dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nNull values per column:\")\n",
    "df.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'title_without_series': 31314, 'title': 31314, 'work_id': 30573, 'book_id': 34758, 'publication_year': 12, 'num_pages': 973, 'ratings_count': 9959, 'kindle_asin': 26179, 'publisher': 4978, 'authors': 14462, 'format': 53, 'country_code': 1, 'series': 19334, 'average_rating': 226, 'similar_books': 29284, 'image_url': 31417, 'isbn13': 24538, 'is_ebook': 2, 'text_reviews_count': 3207, 'language_code': 49, 'description': 32923, 'link': 34758, 'url': 34758, 'asin': 7173, 'popular_shelves': 32551, 'edition_information': 639, 'isbn': 22770, 'publication_day': 31, 'publication_month': 12}\n"
     ]
    }
   ],
   "source": [
    "def make_hashable(x):\n",
    "    \"\"\"\n",
    "    Recursively convert unhashable types (lists, dicts, sets) into hashable types.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return tuple(make_hashable(e) for e in x)\n",
    "    elif isinstance(x, dict):\n",
    "        # Sort items by key for consistency\n",
    "        return tuple((k, make_hashable(v)) for k, v in sorted(x.items()))\n",
    "    elif isinstance(x, set):\n",
    "        return tuple(sorted(make_hashable(e) for e in x))\n",
    "    else:\n",
    "        try:\n",
    "            hash(x)\n",
    "            return x\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "\n",
    "def safe_nunique(series):\n",
    "    return series.map(lambda x: make_hashable(x)).nunique(dropna=True)\n",
    "\n",
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title_without_series': 90.09148972898325,\n",
       " 'title': 90.09148972898325,\n",
       " 'work_id': 87.95960642154324,\n",
       " 'book_id': 100.0,\n",
       " 'publication_year': 0.03452442603141723,\n",
       " 'num_pages': 2.7993555440474136,\n",
       " 'ratings_count': 28.65239657057368,\n",
       " 'kindle_asin': 75.3179124230393,\n",
       " 'publisher': 14.321882732032915,\n",
       " 'authors': 41.607687438862996,\n",
       " 'format': 0.15248288163875942,\n",
       " 'country_code': 0.0028770355026181025,\n",
       " 'series': 55.62460440761839,\n",
       " 'average_rating': 0.6502100235916911,\n",
       " 'similar_books': 84.25110765866852,\n",
       " 'image_url': 90.38782438575292,\n",
       " 'isbn13': 70.596697163243,\n",
       " 'is_ebook': 0.005754071005236205,\n",
       " 'text_reviews_count': 9.226652856896255,\n",
       " 'language_code': 0.140974739628287,\n",
       " 'description': 94.72063985269578,\n",
       " 'link': 100.0,\n",
       " 'url': 100.0,\n",
       " 'asin': 20.636975660279646,\n",
       " 'popular_shelves': 93.65038264572185,\n",
       " 'edition_information': 1.8384256861729675,\n",
       " 'isbn': 65.51009839461419,\n",
       " 'publication_day': 0.08918810058116117,\n",
       " 'publication_month': 0.03452442603141723}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Standardizing Dates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming date attributes (\"publication_year\", \"publication_month\" and \"publication_day\") into one attribute - \"publication_date\"**\n",
    "\n",
    "- **Using ISO Standard (YYYY-MM-DD)**, instead of UNIX to be more universally understandable regardless of background.\n",
    "- We opted to assign missing publication months and days a default value of 1 (January and the first day, respectively), knowing it would be difficult to get this data. Still tagged them with \"publication_date_estimate\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_without_series</th>\n",
       "      <th>title</th>\n",
       "      <th>work_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>kindle_asin</th>\n",
       "      <th>publisher</th>\n",
       "      <th>authors</th>\n",
       "      <th>format</th>\n",
       "      <th>country_code</th>\n",
       "      <th>series</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>similar_books</th>\n",
       "      <th>image_url</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>language_code</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>url</th>\n",
       "      <th>asin</th>\n",
       "      <th>popular_shelves</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>isbn</th>\n",
       "      <th>publication_day</th>\n",
       "      <th>publication_month</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_date_estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>171715</td>\n",
       "      <td>12479382</td>\n",
       "      <td>2011</td>\n",
       "      <td>464.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Voyager</td>\n",
       "      <td>{'role': '', 'author_id': '25307'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>[146510, 607115, 212862, 576243]</td>\n",
       "      <td>4.14</td>\n",
       "      <td>[618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...</td>\n",
       "      <td>https://images.gr-assets.com/books/1406623170m/12479382.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>en-GB</td>\n",
       "      <td>Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>B005JE1K9M</td>\n",
       "      <td>[{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seven Eves</td>\n",
       "      <td>Seven Eves</td>\n",
       "      <td>42299347</td>\n",
       "      <td>29457915</td>\n",
       "      <td>2015</td>\n",
       "      <td>867.0</td>\n",
       "      <td>1047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>{'role': '', 'author_id': '545'}</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>US</td>\n",
       "      <td>[]</td>\n",
       "      <td>3.98</td>\n",
       "      <td>[23197269, 25659450, 23533039, 23168809, 23168817, 15985402, 9424053, 20821159, 19367093, 238480...</td>\n",
       "      <td>https://images.gr-assets.com/books/1463823772m/29457915.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>146</td>\n",
       "      <td>eng</td>\n",
       "      <td>What would happen if the world were ending?\\nA catastrophic event renders the earth a ticking ti...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29457915-seven-eves</td>\n",
       "      <td>https://www.goodreads.com/book/show/29457915-seven-eves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '25568'}, {'name': 'science-fiction', 'count': '1975'}, {'name': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9780008132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يا بعده</td>\n",
       "      <td>يا بعده</td>\n",
       "      <td>42464976</td>\n",
       "      <td>22894535</td>\n",
       "      <td>2014</td>\n",
       "      <td>293.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lqS@ l`rby@ - lkwyt</td>\n",
       "      <td>{'role': '', 'author_id': '2743894'}</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>US</td>\n",
       "      <td>[]</td>\n",
       "      <td>3.85</td>\n",
       "      <td>[18068554, 7156236, 5781288, 22035215, 28106811, 13454387, 6652543, 23264107, 17911031, 22067995...</td>\n",
       "      <td>https://images.gr-assets.com/books/1407760836m/22894535.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>235</td>\n",
       "      <td>ara</td>\n",
       "      <td>'n Hbk ldrj@ tnsyny Hzny,\\n'n Hbk ldrj@ tlhyny `n nZrt lHsr@ fy `ywn hly,\\n'n Hbk ldrj@ tmHw fkr...</td>\n",
       "      <td>https://www.goodreads.com/book/show/22894535</td>\n",
       "      <td>https://www.goodreads.com/book/show/22894535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '4396'}, {'name': 'currently-reading', 'count': '420'}, {'name': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noah (5th Street, #1)</td>\n",
       "      <td>Noah (5th Street, #1)</td>\n",
       "      <td>18573480</td>\n",
       "      <td>22913057</td>\n",
       "      <td>2012</td>\n",
       "      <td>352.0</td>\n",
       "      <td>2521</td>\n",
       "      <td>B007GEUY7W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'role': '', 'author_id': '499602'}</td>\n",
       "      <td>ebook</td>\n",
       "      <td>US</td>\n",
       "      <td>[348923]</td>\n",
       "      <td>3.80</td>\n",
       "      <td>[18393683, 15776233, 12807448, 14290481, 15827220, 13509846, 15724170, 15990356, 18793095, 16581...</td>\n",
       "      <td>https://images.gr-assets.com/books/1407955134m/22913057.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>205</td>\n",
       "      <td>eng</td>\n",
       "      <td>Veronica Cruz has been through hell and back. After disconnecting with the world two years ago t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/22913057-noah</td>\n",
       "      <td>https://www.goodreads.com/book/show/22913057-noah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '9208'}, {'name': 'romance', 'count': '305'}, {'name': 'currently-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016 on Goodreads</td>\n",
       "      <td>2016 on Goodreads</td>\n",
       "      <td>53943483</td>\n",
       "      <td>33232571</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'role': '', 'author_id': '5481957'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>[996473]</td>\n",
       "      <td>4.25</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://images.gr-assets.com/books/1480784881m/33232571.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1225</td>\n",
       "      <td>eng</td>\n",
       "      <td>You've probably found your way to this page because you follow reviews here on goodreads. Whethe...</td>\n",
       "      <td>https://www.goodreads.com/book/show/33232571-2016-on-goodreads</td>\n",
       "      <td>https://www.goodreads.com/book/show/33232571-2016-on-goodreads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '561'}, {'name': 'year-in-review', 'count': '87'}, {'name': 'revie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title_without_series  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)   \n",
       "1                                   Seven Eves   \n",
       "2                                      يا بعده   \n",
       "3                        Noah (5th Street, #1)   \n",
       "4                            2016 on Goodreads   \n",
       "\n",
       "                                         title   work_id   book_id  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)    171715  12479382   \n",
       "1                                   Seven Eves  42299347  29457915   \n",
       "2                                      يا بعده  42464976  22894535   \n",
       "3                        Noah (5th Street, #1)  18573480  22913057   \n",
       "4                            2016 on Goodreads  53943483  33232571   \n",
       "\n",
       "   publication_year  num_pages  ratings_count kindle_asin  \\\n",
       "0              2011      464.0           2037         NaN   \n",
       "1              2015      867.0           1047         NaN   \n",
       "2              2014      293.0           2100         NaN   \n",
       "3              2012      352.0           2521  B007GEUY7W   \n",
       "4              2016        NaN           1167         NaN   \n",
       "\n",
       "             publisher                               authors     format  \\\n",
       "0       Harper Voyager    {'role': '', 'author_id': '25307'}        NaN   \n",
       "1       Harper Collins      {'role': '', 'author_id': '545'}  Hardcover   \n",
       "2  lqS@ l`rby@ - lkwyt  {'role': '', 'author_id': '2743894'}  Paperback   \n",
       "3                  NaN   {'role': '', 'author_id': '499602'}      ebook   \n",
       "4                  NaN  {'role': '', 'author_id': '5481957'}        NaN   \n",
       "\n",
       "  country_code                            series  average_rating  \\\n",
       "0           US  [146510, 607115, 212862, 576243]            4.14   \n",
       "1           US                                []            3.98   \n",
       "2           US                                []            3.85   \n",
       "3           US                          [348923]            3.80   \n",
       "4           US                          [996473]            4.25   \n",
       "\n",
       "                                                                                         similar_books  \\\n",
       "0  [618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...   \n",
       "1  [23197269, 25659450, 23533039, 23168809, 23168817, 15985402, 9424053, 20821159, 19367093, 238480...   \n",
       "2  [18068554, 7156236, 5781288, 22035215, 28106811, 13454387, 6652543, 23264107, 17911031, 22067995...   \n",
       "3  [18393683, 15776233, 12807448, 14290481, 15827220, 13509846, 15724170, 15990356, 18793095, 16581...   \n",
       "4                                                                                                   []   \n",
       "\n",
       "                                                     image_url isbn13  \\\n",
       "0  https://images.gr-assets.com/books/1406623170m/12479382.jpg    NaN   \n",
       "1  https://images.gr-assets.com/books/1463823772m/29457915.jpg    NaN   \n",
       "2  https://images.gr-assets.com/books/1407760836m/22894535.jpg    NaN   \n",
       "3  https://images.gr-assets.com/books/1407955134m/22913057.jpg    NaN   \n",
       "4  https://images.gr-assets.com/books/1480784881m/33232571.jpg    NaN   \n",
       "\n",
       "   is_ebook  text_reviews_count language_code  \\\n",
       "0      True                 112         en-GB   \n",
       "1     False                 146           eng   \n",
       "2     False                 235           ara   \n",
       "3      True                 205           eng   \n",
       "4     False                1225           eng   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0  Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...   \n",
       "1  What would happen if the world were ending?\\nA catastrophic event renders the earth a ticking ti...   \n",
       "2  'n Hbk ldrj@ tnsyny Hzny,\\n'n Hbk ldrj@ tlhyny `n nZrt lHsr@ fy `ywn hly,\\n'n Hbk ldrj@ tmHw fkr...   \n",
       "3  Veronica Cruz has been through hell and back. After disconnecting with the world two years ago t...   \n",
       "4  You've probably found your way to this page because you follow reviews here on goodreads. Whethe...   \n",
       "\n",
       "                                                                 link  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "1             https://www.goodreads.com/book/show/29457915-seven-eves   \n",
       "2                        https://www.goodreads.com/book/show/22894535   \n",
       "3                   https://www.goodreads.com/book/show/22913057-noah   \n",
       "4      https://www.goodreads.com/book/show/33232571-2016-on-goodreads   \n",
       "\n",
       "                                                                  url  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "1             https://www.goodreads.com/book/show/29457915-seven-eves   \n",
       "2                        https://www.goodreads.com/book/show/22894535   \n",
       "3                   https://www.goodreads.com/book/show/22913057-noah   \n",
       "4      https://www.goodreads.com/book/show/33232571-2016-on-goodreads   \n",
       "\n",
       "         asin  \\\n",
       "0  B005JE1K9M   \n",
       "1         NaN   \n",
       "2         NaN   \n",
       "3         NaN   \n",
       "4         NaN   \n",
       "\n",
       "                                                                                       popular_shelves  \\\n",
       "0  [{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...   \n",
       "1  [{'name': 'to-read', 'count': '25568'}, {'name': 'science-fiction', 'count': '1975'}, {'name': '...   \n",
       "2  [{'name': 'to-read', 'count': '4396'}, {'name': 'currently-reading', 'count': '420'}, {'name': '...   \n",
       "3  [{'name': 'to-read', 'count': '9208'}, {'name': 'romance', 'count': '305'}, {'name': 'currently-...   \n",
       "4  [{'name': 'to-read', 'count': '561'}, {'name': 'year-in-review', 'count': '87'}, {'name': 'revie...   \n",
       "\n",
       "  edition_information        isbn  publication_day  publication_month  \\\n",
       "0                 NaN         NaN              NaN                NaN   \n",
       "1                 NaN  9780008132              NaN                NaN   \n",
       "2                 NaN         NaN              NaN                NaN   \n",
       "3                 NaN         NaN              NaN                NaN   \n",
       "4                 NaN         NaN              NaN                NaN   \n",
       "\n",
       "  publication_date  publication_date_estimate  \n",
       "0       2011-01-01                       True  \n",
       "1       2015-01-01                       True  \n",
       "2       2014-01-01                       True  \n",
       "3       2012-01-01                       True  \n",
       "4       2016-01-01                       True  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_pub_date_and_estimate(row):\n",
    "    year = row['publication_year']\n",
    "    # Check if month or day are missing\n",
    "    month_missing = pd.isnull(row['publication_month'])\n",
    "    day_missing = pd.isnull(row['publication_day'])\n",
    "    # Default values if missing\n",
    "    month = row['publication_month'] if not month_missing else 1\n",
    "    day = row['publication_day'] if not day_missing else 1\n",
    "    # Create formatted date string\n",
    "    date_str = f\"{int(year):04d}-{int(month):02d}-{int(day):02d}\"\n",
    "    # Tag as estimate if a default was used for month or day\n",
    "    estimate = month_missing or day_missing\n",
    "    return pd.Series([date_str, estimate])\n",
    "\n",
    "# Create two new columns: publication_date and publication_date_estimate\n",
    "df[['publication_date', 'publication_date_estimate']] = df.apply(create_pub_date_and_estimate, axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date_estimate</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>33100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publication_date_estimate  count\n",
       "0                      False  33100\n",
       "1                       True   1658"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_isbn = (\n",
    "    df[\"publication_date_estimate\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"publication_date_estimate\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **CATEGORICAL CONSISTENCY (String Attributes Values' Standardization of Synonyms)** \n",
    "\n",
    "Overall it seems to be pretty clean now, but let's see if there are string values that should be the same (Donald Duck and D. Duck should be the same value for instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **RapidFuzz - String Similarity Algorithm** \n",
    "\n",
    "RapidFuzz is built upon efficient implementations of **string similarity algorithms**, much like Levenshtein distance, which measures the minimal number of single-character edits required to transform one string into another. \n",
    "\n",
    "The algorithm computes a **similarity score by quantifying the cost of character substitutions, insertions, and deletions**, then **normalises this value into a ratio between 0 and 100**, where a <ins>higher score indicates greater similarity</ins>. \n",
    "\n",
    "This approach not only ensures speed and scalability when comparing large numbers of strings but also maintains the accuracy of approximate matching, making RapidFuzz particularly effective for tasks like deduplication, clustering, and standardising text data.\n",
    "\n",
    "However, as you can see in the output ahead it has issues when the synonyms have different character lengths, especially when the difference comes to number of words/abbreviations. Some nuance understanding might be needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of ratios:\n",
    "\n",
    "- The **simple token ratio** measures the similarity between two strings by <ins>comparing the total number of matching characters against the total characters in both strings</ins>. It provides a percentage score indicating how closely the two strings match overall, even if minor differences like extra spaces exist.\n",
    "  `fuzz.ratio()`\n",
    "\n",
    "- The **partial token ratio**, on the other hand, **looks at substrings**. It finds the <ins>best-matching substring within the longer string compared to the shorter one</ins>. This is <ins>particularly useful when one string is embedded within another, as it can recognise a perfect match even if additional words or details are present</ins>. `fuzz.partial_ratio()`\n",
    "\n",
    "- The **token sort ratio** first <ins>splits the strings into individual words (tokens), sorts these tokens alphabetically, and then compares the sorted versions</ins>. This method neutralises the effect of different word orders, ensuring that strings containing the same words in a different sequence are still recognised as identical. `fuzz.token_sort_ratio()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_groups(df, column, threshold=90, scorer=fuzz.ratio):\n",
    "    \"\"\"\n",
    "    Groups similar strings from a DataFrame column based on a similarity threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The column name containing the strings.\n",
    "        threshold (int): Similarity threshold (0-100) for fuzzy matching. Default is 98.\n",
    "        scorer (function): Scoring function from rapidfuzz (default: fuzz.ratio).\n",
    "        \n",
    "    Returns:\n",
    "        List[Set[str]]: A list of sets, where each set contains similar strings.\n",
    "    \"\"\"\n",
    "    unique_values = list(df[column].dropna().unique())\n",
    "    \n",
    "    similar_groups = {}\n",
    "    for item in unique_values:\n",
    "        matches = process.extract(item, unique_values, scorer=scorer)\n",
    "        # Keep only matches with a score >= threshold.\n",
    "        similar = sorted({match for match, score, _ in matches if score >= threshold})\n",
    "        similar_groups[item] = similar\n",
    "\n",
    "    def merge_groups(groups):\n",
    "        merged = []\n",
    "        for group in groups.values():\n",
    "            group_set = set(group)\n",
    "            added = False\n",
    "            for i, m in enumerate(merged):\n",
    "                if group_set & m:  # If there's any overlap, merge the groups.\n",
    "                    merged[i] = m | group_set\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                merged.append(group_set)\n",
    "        return merged\n",
    "\n",
    "    merged_groups = merge_groups(similar_groups)\n",
    "    return merged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - **\"publisher\" Column**\n",
    "\n",
    "Some entries are also in a foreign encryption type not UTF-8, but we are mostly working with USA in this project so it might not be the biggest issue.\n",
    "\n",
    "The matching isn't perfect but it seemed to be the best with the simple token ratio.\n",
    "\n",
    "Some publisher seems to have strange irregularities suggesting that encoding or extraction issues are more likely at play (ie:'n llnshr wltwzy`, lrwq llnshr w ltwzy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'HarperVogager', 'Harper/Voyager', 'Harper Voyager', 'HarperVoyager'}\n",
      "{'HaperCollins', 'Harpercollins', 'HarperCollins', 'Harper Collins'}\n",
      "{'lqS@ l`rby@ - lkwyt'}\n",
      "{'mdrk'}\n",
      "{'Fireweed Publishing', 'Fireweed Publishing Ltd'}\n",
      "{'Can Yayinlari'}\n",
      "{'Jennifer Estep'}\n",
      "{'RosettaBooks', 'Rosetta Books'}\n",
      "{'dr `Syr lktb llnshr wltwzy`', '`Syr lktb llnshr wltwzy`'}\n",
      "{'J.C. Isabella, via kindle'}\n",
      "{\"mw'ss@ lslm lywm\"}\n",
      "{'Lyons Publishing Limited', 'Lyons Publishing Ltd'}\n",
      "{'Bantam Books'}\n",
      "{'mktb@ jryr'}\n",
      "{'Weinstein Books'}\n",
      "{'Meredith Wild LLC'}\n",
      "{'Grand Central Publishing'}\n",
      "{'Headline Publishing Group'}\n",
      "{'Bestseller Publishing'}\n",
      "{\"lmw'ss@ l`rby@ lldrst w lnshr\", \"lmw'ss@ l`rby@ lldrst wlnshr\"}\n",
      "{'lslm lywm'}\n",
      "{'Disney-Hyperion Books', 'Disney/Hyperion Book', 'Disney Hyperion Books', 'Disney - Hyperion Books'}\n",
      "{'ldr lmSry@ llbnny@'}\n",
      "{'ldr l`rby@ ll`lwm-nshrwn', 'ldr l`rby@ ll`lwm nshrwn'}\n",
      "{'Algonquin Books'}\n",
      "{'The Great Courses'}\n",
      "{'mnshwrt mdwn@ nynr'}\n",
      "{'Faber and Faber Crime'}\n",
      "{'Orbit'}\n",
      "{'dr lmn~'}\n",
      "{'DreamBuilders'}\n",
      "{'S.L. Jennings'}\n",
      "{'dr lshrwq'}\n",
      "{\"Scholastic Children's Books\"}\n",
      "{\"mktb@ l`bykn , hyy'@ 'bwZby llthqf@ wltrth\"}\n",
      "{'ntshrt rh mn'}\n",
      "{'headline'}\n",
      "{'Wattpad'}\n",
      "{'Swoon Romance'}\n",
      "{'Grove'}\n",
      "{\"dr 'thr\"}\n",
      "{'n llnshr wltwzy`', 'lrwq llnshr w ltwzy`', 'drk llnshr wltwzy`', 'dr nsn llnshr w ltwzy`', \"dr 'ktb llnshr w ltwzy`\", 'dr ktb llnshr wltwzy`', 'dr jdwl llnshr w ltwzy`', \"dr 'thr llnshr w ltwzy`\", 'dr dwWin llnshr wltwzy`', 'dr klmt llnshr w ltwzy`', 'mdrk llnshr wltwzy`', 'bysn llnshr wltwzy`', 'kyn llnshr wltwzy`', 'nwn llnshr wltwzy`', 'dr klmt llnshr wltwzy`', \"dr 'ktb llnshr wltwzy`\", 'dr twy llnshr wltwzy`', 'dr ljndy llnshr wltwzy`', 'lrwq llnshr wltwzy`', 'dr nsn llnshr wltwzy`', 'dr ktb llnshr w ltwzy`', 'dr rwq llnshr wltwzy`', 'dr nwn llnshr wltwzy`', 'dr dwn llnshr w ltwzy`', 'dr n llnshr wltwzy`'}\n",
      "{'dr lfkr'}\n",
      "{'Riverhead'}\n",
      "{'VIVA PSICOM'}\n",
      "{\"'Tls llnshr wl'ntj l'`lm~ - mSr\"}\n",
      "{'Simon Pulse', 'SimonPulse', 'Simon Pluse'}\n",
      "{'Chanda Hahan', 'Chanda Hahn'}\n",
      "{'mHmwd frjmy'}\n",
      "{'Gagas Media', 'GagasMedia'}\n"
     ]
    }
   ],
   "source": [
    "unique_publishers = list(df['publisher'].dropna().unique())\n",
    "\n",
    "merged_groups = get_standardized_groups(df, 'publisher', threshold=90, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:50]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To make this task more efficient, we need to use the most recurring/ frequent name has the canonical one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized publisher names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>publisher_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harper Voyager</td>\n",
       "      <td>Harper Voyager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>HarperCollins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lqS@ l`rby@ - lkwyt</td>\n",
       "      <td>lqS@ l`rby@ - lkwyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mdrk</td>\n",
       "      <td>mdrk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fireweed Publishing</td>\n",
       "      <td>Fireweed Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Can Yayinlari</td>\n",
       "      <td>Can Yayinlari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jennifer Estep</td>\n",
       "      <td>Jennifer Estep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             publisher   publisher_standard\n",
       "0       Harper Voyager       Harper Voyager\n",
       "1       Harper Collins        HarperCollins\n",
       "2  lqS@ l`rby@ - lkwyt  lqS@ l`rby@ - lkwyt\n",
       "3                  NaN                  NaN\n",
       "4                  NaN                  NaN\n",
       "5                 mdrk                 mdrk\n",
       "6  Fireweed Publishing  Fireweed Publishing\n",
       "7        Can Yayinlari        Can Yayinlari\n",
       "8       Jennifer Estep       Jennifer Estep\n",
       "9                  NaN                  NaN"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency for each publisher\n",
    "freq = df['publisher'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    # Choose the publisher with the highest frequency in this group\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "# If it alone, it keeps its name obvi\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "\n",
    "# standardized publisher column in the DataFrame using the mapping, add not removing \n",
    "df['publisher_standard'] = df['publisher'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized publisher names added to DataFrame:\")\n",
    "df[['publisher', 'publisher_standard']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"title_without_series\" and \"title\"** not good candidates\n",
    "\n",
    "As we can see in the output, this method is not recommended because even small changes—such as replacing a '1' with a '2'—can lead to significant differences in titles. For instance, a minor alteration might indicate that one title is a sequel while the others belong to entirely different books. Consider this example:\n",
    "\n",
    "- {'Fantasy of Flight (The Tainted Accords, #2)', 'Fantasy of Frost (The Tainted Accords, #1)', 'Fantasy of Fire (The Tainted Accords, #3)'}\n",
    "\n",
    "- higher threshold would produce individual groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_publishers = list(df['title_without_series'].dropna().unique())\n",
    "\n",
    "#merged_groups = get_standardized_groups(df, 'title_without_series', threshold=90)\n",
    "#print(\"Potential standardized groups:\")\n",
    "#for group in merged_groups:\n",
    "    #print(group)\n",
    "\n",
    "# DO NOT RUN OUTPUT IS REAL LARGE AND MESSES WITH THE NOTEBOOK UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"country_code\"** \n",
    "\n",
    "All from the US, as expected. It's using the most commonly used standard is the ISO 3166-1 alpha-2 code, which uses two-letter abbreviations (e.g., US, GB, FR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US']\n"
     ]
    }
   ],
   "source": [
    "unique_country_codes = df['country_code'].dropna().unique()\n",
    "print(unique_country_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"language_code\"** \n",
    "\n",
    "Seems to be clean but let's further check. They are not in ISO 639-1 like our prior dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en-GB' 'eng' 'ara' 'tur' 'fil' 'rum' 'en-US' 'per' 'ind' 'tam' 'kor'\n",
      " 'hun' 'en' 'vie' 'spa' 'en-CA' 'fre' 'ger' 'rus' 'cze' 'slo' 'pol' 'por'\n",
      " 'ita' 'nl' 'swe' 'fin' 'bul' 'gre' 'urd' 'heb' 'msa' 'kat' 'lit' 'isl'\n",
      " 'jpn' 'mal' 'nor' 'lav' 'mar' 'dan' 'ben' 'es-MX' 'cat' 'nob' 'afr' 'tgl'\n",
      " 'frs' 'zho']\n"
     ]
    }
   ],
   "source": [
    "unique_language_code = df['language_code'].dropna().unique()\n",
    "print(unique_language_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confirming if they are valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique language codes from data: ['en-GB' 'eng' 'ara' 'tur' 'fil' 'rum' 'en-US' 'per' 'ind' 'tam' 'kor'\n",
      " 'hun' 'en' 'vie' 'spa' 'en-CA' 'fre' 'ger' 'rus' 'cze' 'slo' 'pol' 'por'\n",
      " 'ita' 'nl' 'swe' 'fin' 'bul' 'gre' 'urd' 'heb' 'msa' 'kat' 'lit' 'isl'\n",
      " 'jpn' 'mal' 'nor' 'lav' 'mar' 'dan' 'ben' 'es-MX' 'cat' 'nob' 'afr' 'tgl'\n",
      " 'frs' 'zho']\n",
      "Valid ISO language codes: ['eng', 'ara', 'tur', 'fil', 'ind', 'tam', 'kor', 'hun', 'en', 'vie', 'spa', 'rus', 'pol', 'por', 'ita', 'nl', 'swe', 'fin', 'bul', 'urd', 'heb', 'msa', 'kat', 'lit', 'isl', 'jpn', 'mal', 'nor', 'lav', 'mar', 'dan', 'ben', 'cat', 'nob', 'afr', 'tgl', 'frs', 'zho']\n",
      "Invalid ISO language codes: ['en-GB', 'rum', 'en-US', 'per', 'en-CA', 'fre', 'ger', 'cze', 'slo', 'gre', 'es-MX']\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "# Get unique language codes from the DataFrame (drop null values)\n",
    "unique_language_codes = df['language_code'].dropna().unique()\n",
    "print(\"Unique language codes from data:\", unique_language_codes)\n",
    "\n",
    "# Check each code to see if it is a valid ISO 3166-1 alpha-2 language code.\n",
    "valid_codes = []\n",
    "invalid_codes = []\n",
    "\n",
    "for code in unique_language_codes:\n",
    "    # Attempt to retrieve language information using alpha-2 code\n",
    "    language = pycountry.languages.get(alpha_2=code)\n",
    "    if language is None:\n",
    "        # If not found using alpha-2, try with alpha_3 as a fallback (if you expect 3-letter codes)\n",
    "        language = pycountry.languages.get(alpha_3=code)\n",
    "    if language:\n",
    "        valid_codes.append(code)\n",
    "    else:\n",
    "        invalid_codes.append(code)\n",
    "\n",
    "print(\"Valid ISO language codes:\", valid_codes)\n",
    "print(\"Invalid ISO language codes:\", invalid_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid ISO language codes after correction: [None]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_raw</th>\n",
       "      <th>language_iso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en-GB</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ara</td>\n",
       "      <td>ara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language_raw language_iso\n",
       "0        en-GB           en\n",
       "1          eng          eng\n",
       "2          ara          ara\n",
       "3          eng          eng\n",
       "4          eng          eng"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "\n",
    "# Assume df is already loaded and contains a column \"language_code\"\n",
    "# Rename \"language_code\" to \"language_raw\" to preserve the original values\n",
    "df.rename(columns={'language_code': 'language_raw'}, inplace=True)\n",
    "\n",
    "# Define a mapping for problematic codes to their ISO 639-1 equivalents\n",
    "corrections = {\n",
    "    'en-GB': 'en',\n",
    "    'en-US': 'en',\n",
    "    'en-CA': 'en',\n",
    "    'rum': 'ro',   # Romanian\n",
    "    'per': 'fa',   # Persian/Farsi\n",
    "    'fre': 'fr',   # French\n",
    "    'ger': 'de',   # German\n",
    "    'cze': 'cs',   # Czech\n",
    "    'slo': 'sk',   # Slovak\n",
    "    'gre': 'el',   # Greek\n",
    "    'es-MX': 'es'  # Mexican Spanish → generic Spanish\n",
    "}\n",
    "\n",
    "def correct_language_code(code):\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    # If the code contains a hyphen, use only the base language part\n",
    "    if '-' in code:\n",
    "        code = code.split('-')[0]\n",
    "    # Apply corrections if the code is in our mapping\n",
    "    return corrections.get(code, code)\n",
    "\n",
    "# Create a new column \"language_iso\" with the corrected ISO codes\n",
    "df['language_iso'] = df['language_raw'].apply(correct_language_code)\n",
    "\n",
    "# Optionally, validate the new codes using pycountry\n",
    "def validate_iso(code):\n",
    "    if not code:\n",
    "        return False\n",
    "    language = pycountry.languages.get(alpha_2=code)\n",
    "    if language is None:\n",
    "        language = pycountry.languages.get(alpha_3=code)\n",
    "    return language is not None\n",
    "\n",
    "invalid_iso = df.loc[~df['language_iso'].apply(validate_iso), 'language_iso'].unique()\n",
    "print(\"Invalid ISO language codes after correction:\", invalid_iso)\n",
    "\n",
    "# Display the original and corrected language columns\n",
    "df[['language_raw', 'language_iso']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"format\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hardcover</td>\n",
       "      <td>10588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paperback</td>\n",
       "      <td>9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>6138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebook</td>\n",
       "      <td>3918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mass Market Paperback</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Audio CD</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Audiobook</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trade Paperback</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Audio</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Audible Audio</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nook</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Library Binding</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>komitsuku</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Leather Bound</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Online Fiction</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unknown Binding</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Board Book</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Comic</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Comic Book</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MP3 CD</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Klappenbroschur</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Broche</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Poche</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gebunden</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hardback</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Board book</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Free Online Read</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Free Read Online</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Free online read</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Wattpad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   format  count\n",
       "0               Hardcover  10588\n",
       "1               Paperback   9616\n",
       "2          Kindle Edition   6138\n",
       "3                   ebook   3918\n",
       "4                     NaN   2932\n",
       "5   Mass Market Paperback   1094\n",
       "6                Audio CD    165\n",
       "7               Audiobook     68\n",
       "8         Trade Paperback     41\n",
       "9                   Audio     36\n",
       "10          Audible Audio     32\n",
       "11                   Nook     21\n",
       "12        Library Binding     10\n",
       "13              komitsuku      9\n",
       "14          Leather Bound      9\n",
       "15         Online Fiction      8\n",
       "16        Unknown Binding      7\n",
       "17             Board Book      5\n",
       "18                  Comic      5\n",
       "19             Comic Book      4\n",
       "20                 MP3 CD      4\n",
       "21        Klappenbroschur      4\n",
       "22                 Broche      3\n",
       "23                  Poche      3\n",
       "24               Gebunden      3\n",
       "25               Hardback      2\n",
       "26             Board book      2\n",
       "27       Free Online Read      2\n",
       "28       Free Read Online      2\n",
       "29       Free online read      2\n",
       "30                Wattpad      2"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'format'\n",
    "dup_isbn = (\n",
    "    df[\"format\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"format\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the **\"category\"** categorical values are not standardised and in multiple languages are included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for standard formats\n",
    "format_mapping = {\n",
    "    # Hardcover category\n",
    "    \"Hardcover\": \"Hardcover\",\n",
    "    \"Hardback\": \"Hardcover\",\n",
    "    \"Leather Bound\": \"Hardcover\",\n",
    "    \"gebunden\": \"Hardcover\",        # hardcover in german\n",
    "    \"Gebunden\": \"Hardcover\",\n",
    "    \"Library Binding\": \"Hardcover\",\n",
    "    \"Unknown Binding\": \"Hardcover\",\n",
    "    \n",
    "    # Paperback category\n",
    "    \"Paperback\": \"Paperback\",\n",
    "    \"Mass Market Paperback\": \"Paperback\",\n",
    "    \"Trade Paperback\": \"Paperback\",\n",
    "    \"Board Book\": \"Paperback\",\n",
    "    \"Board book\": \"Paperback\",\n",
    "    \"Comic\": \"Paperback\",\n",
    "    \"Comic Book\": \"Paperback\",\n",
    "    \"Klappenbroschur\": \"Paperback\",     # paperback in german\n",
    "    \"Broche\": \"Paperback\",              # Livre broché = paperback in french\n",
    "    \"Poche\": \"Paperback\",               # paperback in italian\n",
    "    \n",
    "    # Ebook category\n",
    "    \"Kindle Edition\": \"Ebook\",\n",
    "    \"ebook\": \"Ebook\",\n",
    "    \"Nook\": \"Ebook\",\n",
    "    \"Online Fiction\": \"Ebook\",\n",
    "    \"Free Online Read\": \"Ebook\",\n",
    "    \"Free Read Online\": \"Ebook\",\n",
    "    \"Free online read\": \"Ebook\",\n",
    "    \"Wattpad\": \"Ebook\",\n",
    "    \n",
    "    # Audio category\n",
    "    \"Audio CD\": \"Audio\",\n",
    "    \"Audiobook\": \"Audio\",\n",
    "    \"Audio\": \"Audio\",\n",
    "    \"Audible Audio\": \"Audio\",\n",
    "    \"MP3 CD\": \"Audio\"\n",
    "}\n",
    "\n",
    "# Create a new column 'standard_format' by mapping the values in the 'format' column\n",
    "df['standard_format'] = df['format'].map(format_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_format</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paperback</td>\n",
       "      <td>10777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hardcover</td>\n",
       "      <td>10619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ebook</td>\n",
       "      <td>10093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  standard_format  count\n",
       "0       Paperback  10777\n",
       "1       Hardcover  10619\n",
       "2           Ebook  10093\n",
       "3             NaN   2964\n",
       "4           Audio    305"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'standard_format'\n",
    "dup_isbn = (\n",
    "    df[\"standard_format\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"standard_format\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"is_ebook\"** can fix up some NaN values in **\"standard_format\"** by just matching them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['is_ebook'] == True, 'standard_format'] = \"Ebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_format</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ebook</td>\n",
       "      <td>11863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paperback</td>\n",
       "      <td>10777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hardcover</td>\n",
       "      <td>10619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  standard_format  count\n",
       "0           Ebook  11863\n",
       "1       Paperback  10777\n",
       "2       Hardcover  10619\n",
       "3             NaN   1194\n",
       "4           Audio    305"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'standard_format'\n",
    "dup_isbn = (\n",
    "    df[\"standard_format\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"standard_format\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_without_series</th>\n",
       "      <th>title</th>\n",
       "      <th>work_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>kindle_asin</th>\n",
       "      <th>publisher</th>\n",
       "      <th>authors</th>\n",
       "      <th>format</th>\n",
       "      <th>country_code</th>\n",
       "      <th>series</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>similar_books</th>\n",
       "      <th>image_url</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>language_raw</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>url</th>\n",
       "      <th>asin</th>\n",
       "      <th>popular_shelves</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>isbn</th>\n",
       "      <th>publication_day</th>\n",
       "      <th>publication_month</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_date_estimate</th>\n",
       "      <th>publisher_standard</th>\n",
       "      <th>language_iso</th>\n",
       "      <th>standard_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>Assassin's Apprentice (Farseer Trilogy, #1)</td>\n",
       "      <td>171715</td>\n",
       "      <td>12479382</td>\n",
       "      <td>2011</td>\n",
       "      <td>464.0</td>\n",
       "      <td>2037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Voyager</td>\n",
       "      <td>{'role': '', 'author_id': '25307'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>[146510, 607115, 212862, 576243]</td>\n",
       "      <td>4.14</td>\n",
       "      <td>[618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...</td>\n",
       "      <td>https://images.gr-assets.com/books/1406623170m/12479382.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>en-GB</td>\n",
       "      <td>Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>https://www.goodreads.com/book/show/12479382-assassin-s-apprentice</td>\n",
       "      <td>B005JE1K9M</td>\n",
       "      <td>[{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>Harper Voyager</td>\n",
       "      <td>en</td>\n",
       "      <td>Ebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seven Eves</td>\n",
       "      <td>Seven Eves</td>\n",
       "      <td>42299347</td>\n",
       "      <td>29457915</td>\n",
       "      <td>2015</td>\n",
       "      <td>867.0</td>\n",
       "      <td>1047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harper Collins</td>\n",
       "      <td>{'role': '', 'author_id': '545'}</td>\n",
       "      <td>Hardcover</td>\n",
       "      <td>US</td>\n",
       "      <td>[]</td>\n",
       "      <td>3.98</td>\n",
       "      <td>[23197269, 25659450, 23533039, 23168809, 23168817, 15985402, 9424053, 20821159, 19367093, 238480...</td>\n",
       "      <td>https://images.gr-assets.com/books/1463823772m/29457915.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>146</td>\n",
       "      <td>eng</td>\n",
       "      <td>What would happen if the world were ending?\\nA catastrophic event renders the earth a ticking ti...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29457915-seven-eves</td>\n",
       "      <td>https://www.goodreads.com/book/show/29457915-seven-eves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'name': 'to-read', 'count': '25568'}, {'name': 'science-fiction', 'count': '1975'}, {'name': '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9780008132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>HarperCollins</td>\n",
       "      <td>eng</td>\n",
       "      <td>Hardcover</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title_without_series  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)   \n",
       "1                                   Seven Eves   \n",
       "\n",
       "                                         title   work_id   book_id  \\\n",
       "0  Assassin's Apprentice (Farseer Trilogy, #1)    171715  12479382   \n",
       "1                                   Seven Eves  42299347  29457915   \n",
       "\n",
       "   publication_year  num_pages  ratings_count kindle_asin       publisher  \\\n",
       "0              2011      464.0           2037         NaN  Harper Voyager   \n",
       "1              2015      867.0           1047         NaN  Harper Collins   \n",
       "\n",
       "                              authors     format country_code  \\\n",
       "0  {'role': '', 'author_id': '25307'}        NaN           US   \n",
       "1    {'role': '', 'author_id': '545'}  Hardcover           US   \n",
       "\n",
       "                             series  average_rating  \\\n",
       "0  [146510, 607115, 212862, 576243]            4.14   \n",
       "1                                []            3.98   \n",
       "\n",
       "                                                                                         similar_books  \\\n",
       "0  [618196, 74275, 558681, 12527544, 104089, 91981, 96278, 590349, 61886, 618177, 811161, 589979, 3...   \n",
       "1  [23197269, 25659450, 23533039, 23168809, 23168817, 15985402, 9424053, 20821159, 19367093, 238480...   \n",
       "\n",
       "                                                     image_url isbn13  \\\n",
       "0  https://images.gr-assets.com/books/1406623170m/12479382.jpg    NaN   \n",
       "1  https://images.gr-assets.com/books/1463823772m/29457915.jpg    NaN   \n",
       "\n",
       "   is_ebook  text_reviews_count language_raw  \\\n",
       "0      True                 112        en-GB   \n",
       "1     False                 146          eng   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0  Young Fitz is the bastard son of the noble Prince Chivalry, raised in the shadow of the royal co...   \n",
       "1  What would happen if the world were ending?\\nA catastrophic event renders the earth a ticking ti...   \n",
       "\n",
       "                                                                 link  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "1             https://www.goodreads.com/book/show/29457915-seven-eves   \n",
       "\n",
       "                                                                  url  \\\n",
       "0  https://www.goodreads.com/book/show/12479382-assassin-s-apprentice   \n",
       "1             https://www.goodreads.com/book/show/29457915-seven-eves   \n",
       "\n",
       "         asin  \\\n",
       "0  B005JE1K9M   \n",
       "1         NaN   \n",
       "\n",
       "                                                                                       popular_shelves  \\\n",
       "0  [{'name': 'to-read', 'count': '1934'}, {'name': 'epic-fantasy', 'count': '492'}, {'name': 'high-...   \n",
       "1  [{'name': 'to-read', 'count': '25568'}, {'name': 'science-fiction', 'count': '1975'}, {'name': '...   \n",
       "\n",
       "  edition_information        isbn  publication_day  publication_month  \\\n",
       "0                 NaN         NaN              NaN                NaN   \n",
       "1                 NaN  9780008132              NaN                NaN   \n",
       "\n",
       "  publication_date  publication_date_estimate publisher_standard language_iso  \\\n",
       "0       2011-01-01                       True     Harper Voyager           en   \n",
       "1       2015-01-01                       True      HarperCollins          eng   \n",
       "\n",
       "  standard_format  \n",
       "0           Ebook  \n",
       "1       Hardcover  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title_without_series', 'title', 'work_id', 'book_id',\n",
       "       'publication_year', 'num_pages', 'ratings_count', 'kindle_asin',\n",
       "       'publisher', 'authors', 'format', 'country_code', 'series',\n",
       "       'average_rating', 'similar_books', 'image_url', 'isbn13', 'is_ebook',\n",
       "       'text_reviews_count', 'language_raw', 'description', 'link', 'url',\n",
       "       'asin', 'popular_shelves', 'edition_information', 'isbn',\n",
       "       'publication_day', 'publication_month', 'publication_date',\n",
       "       'publication_date_estimate', 'publisher_standard', 'language_iso',\n",
       "       'standard_format'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title_without_series', 'title', 'work_id', 'book_id', 'publication_year', 'num_pages', 'ratings_count', 'kindle_asin', 'publisher_raw', 'authors', 'format_raw', 'country_code_raw', 'series', 'avg_rating', 'similar_books', 'image_url', 'isbn_13', 'is_ebook', 'text_reviews_count', 'language_raw', 'description', 'link', 'url', 'child_asin', 'popular_shelves', 'edition', 'isbn_10', 'publication_day', 'publication_month', 'publication_date', 'publication_estimate', 'publisher', 'lang_iso', 'standard_format']\n"
     ]
    }
   ],
   "source": [
    "rename_mapping = {\n",
    "    'title_without_series': 'title_without_series',\n",
    "    'title': 'title',\n",
    "    'work_id': 'work_id',\n",
    "    'book_id': 'book_id',\n",
    "    'publication_year': 'publication_year',\n",
    "    'num_pages': 'num_pages',\n",
    "    'ratings_count': 'ratings_count',\n",
    "    'kindle_asin': 'kindle_asin',\n",
    "    'publisher': 'publisher_raw',\n",
    "    'authors': 'authors',\n",
    "    'format': 'format_raw',\n",
    "    'country_code': 'country_code',\n",
    "    'series': 'series',\n",
    "    'average_rating': 'avg_rating',\n",
    "    'similar_books': 'similar_books',\n",
    "    'image_url': 'image_url',\n",
    "    'isbn13': 'isbn_13',\n",
    "    'is_ebook': 'is_ebook',\n",
    "    'text_reviews_count': 'text_reviews_count',\n",
    "    'language_raw': 'language_raw',\n",
    "    'description': 'description',\n",
    "    'link': 'link',\n",
    "    'url': 'url',\n",
    "    'asin': 'child_asin',\n",
    "    'popular_shelves': 'popular_shelves',\n",
    "    'edition_information': 'edition',\n",
    "    'isbn': 'isbn_10',\n",
    "    'publication_day': 'publication_day',\n",
    "    'publication_month': 'publication_month',\n",
    "    'publication_date': 'publication_date',\n",
    "    'publication_date_estimate': 'publication_estimate',\n",
    "    'publisher_standard': 'publisher',\n",
    "    'language_iso': 'lang_iso',\n",
    "    'standard_format': 'standard_format'\n",
    "}\n",
    "\n",
    "# Apply the renaming mapping\n",
    "df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Check the new standardized column names\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the standardized DataFrame to a CSV file\n",
    "df.to_csv('./Datasets/2_3_goodreads/goodreads_meta_books_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) ii) Books Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Datasets/2_3_goodreads/goodreads_reviews_filtered.json\" \n",
    "\n",
    "df = pd.read_json(file_path, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_sentences</th>\n",
       "      <th>has_spoiler</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8f615d0cfe10cd485217472dca7de9b</td>\n",
       "      <td>929783</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'text': 'A near-future dystopia, Jack London's The Iron Heel (1908) tells of the transition fr...</td>\n",
       "      <td>False</td>\n",
       "      <td>2008-06-19</td>\n",
       "      <td>bd828f0c290ce2375b452d43052169d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>444b4bf21d4cb4099dcd025b0d803150</td>\n",
       "      <td>61898</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'text': 'I've always liked Bujold, and this one is no exception.', 'flag': '0'}, {'text': 'Her...</td>\n",
       "      <td>False</td>\n",
       "      <td>2009-03-12</td>\n",
       "      <td>59309b297bc476ce712ded8dd14f7d98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c853a5a3a34158c2c688ad6ee93abcb6</td>\n",
       "      <td>295649</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'text': 'I'm not a super fan of pirate fiction, but this was a fun romp (and, given the descri...</td>\n",
       "      <td>False</td>\n",
       "      <td>2009-07-16</td>\n",
       "      <td>f88674738523f5acd4e2e61a7960ede8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id  book_id  rating  \\\n",
       "0  b8f615d0cfe10cd485217472dca7de9b   929783       3   \n",
       "1  444b4bf21d4cb4099dcd025b0d803150    61898       4   \n",
       "2  c853a5a3a34158c2c688ad6ee93abcb6   295649       4   \n",
       "\n",
       "                                                                                      review_sentences  \\\n",
       "0  [{'text': 'A near-future dystopia, Jack London's The Iron Heel (1908) tells of the transition fr...   \n",
       "1  [{'text': 'I've always liked Bujold, and this one is no exception.', 'flag': '0'}, {'text': 'Her...   \n",
       "2  [{'text': 'I'm not a super fan of pirate fiction, but this was a fun romp (and, given the descri...   \n",
       "\n",
       "   has_spoiler  timestamp                           user_id  \n",
       "0        False 2008-06-19  bd828f0c290ce2375b452d43052169d9  \n",
       "1        False 2009-03-12  59309b297bc476ce712ded8dd14f7d98  \n",
       "2        False 2009-07-16  f88674738523f5acd4e2e61a7960ede8  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 262061\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "review_id: {'min_length': 32, 'max_length': 32, 'mean_length': 32.0, 'median_length': 32.0}\n",
      "book_id: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "review_sentences: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "has_spoiler: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "timestamp: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "user_id: {'min_length': 32, 'max_length': 32, 'mean_length': 32.0, 'median_length': 32.0}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "review_id           0.0\n",
       "book_id             0.0\n",
       "rating              0.0\n",
       "review_sentences    0.0\n",
       "has_spoiler         0.0\n",
       "timestamp           0.0\n",
       "user_id             0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nNull values per column:\")\n",
    "df.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'review_id': 262061, 'book_id': 12291, 'rating': 6, 'review_sentences': 259969, 'has_spoiler': 2, 'timestamp': 2905, 'user_id': 16864}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review_id': 100.0,\n",
       " 'book_id': 4.690129397354051,\n",
       " 'rating': 0.0022895432742758366,\n",
       " 'review_sentences': 99.20171257836915,\n",
       " 'has_spoiler': 0.0007631810914252788,\n",
       " 'timestamp': 1.1085205352952174,\n",
       " 'user_id': 6.435142962897951}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Info**\n",
    "\n",
    "Seems data types are already standardised. All timestamp dates are in the same format using **ISO Standard (YYYY-MM-DD)** as we formatted the metadata dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 262061 entries, 0 to 262060\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   review_id         262061 non-null  object        \n",
      " 1   book_id           262061 non-null  int64         \n",
      " 2   rating            262061 non-null  int64         \n",
      " 3   review_sentences  262061 non-null  object        \n",
      " 4   has_spoiler       262061 non-null  bool          \n",
      " 5   timestamp         262061 non-null  datetime64[ns]\n",
      " 6   user_id           262061 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates in this dataset**\n",
    "\n",
    "Reviews are clean - review_id's are completely unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_id, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate counts for 'review_id'\n",
    "dup_isbn = (\n",
    "    df[\"review_id\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    ")\n",
    "dup_isbn.columns = [\"review_id\", \"count\"]\n",
    "dup_isbn = dup_isbn[dup_isbn[\"count\"] > 1]\n",
    "dup_isbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are empty strings or 0 values in the DataFrame.\n",
      "review_id           False\n",
      "book_id             False\n",
      "rating               True\n",
      "review_sentences    False\n",
      "has_spoiler          True\n",
      "timestamp           False\n",
      "user_id             False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "mask = df.isin(['', ' ', 0])\n",
    "\n",
    "# Check if any cell in the DataFrame is either an empty string or 0\n",
    "if mask.any().any():\n",
    "    print(\"There are empty strings or 0 values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no empty strings or 0 values in the DataFrame.\")\n",
    "\n",
    "# To see which columns contain these values:\n",
    "print(mask.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No empty strings and 0s on relevant columns. On rating and has_spoiler, it was to be expected, so there isn't an issue.\n",
    "\n",
    "The rest of the columns would be expected to have some level of duplication (the same user posting multiple reviews, books having multiple reviews, reviews being published on the same date, etc...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_value_repeats(df, column):\n",
    "    \"\"\"\n",
    "    Counts how many times each unique value appears in the specified column of a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to analyze.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with two columns: the unique values and their repeat counts.\n",
    "    \"\"\"\n",
    "    counts = df[column].value_counts().reset_index()\n",
    "    counts.columns = [column, \"repeat_count\"]\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_spoiler</th>\n",
       "      <th>repeat_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>244316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>17745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_spoiler  repeat_count\n",
       "0        False        244316\n",
       "1         True         17745"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_value_repeats(df, \"has_spoiler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) iii) Book Authors\n",
    "\n",
    "Quering out authors that did not exist in the meta_books.\n",
    "before- 829 529 authors\n",
    "now - 16 570 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Datasets/2_3_goodreads/goodreads_book_authors.json\" \n",
    "\n",
    "df = pd.read_json(file_path, lines = True, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 16570\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "ratings_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "name: {'min_length': 3, 'max_length': 47, 'mean_length': 13.352866626433313, 'median_length': 13.0}\n",
      "text_reviews_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "author_id: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "average_rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ratings_count         0.0\n",
       "name                  0.0\n",
       "text_reviews_count    0.0\n",
       "author_id             0.0\n",
       "average_rating        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nNull values per column:\")\n",
    "df.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'ratings_count': 12379, 'name': 16563, 'text_reviews_count': 5691, 'author_id': 16570, 'average_rating': 205}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ratings_count': 74.70730235365117,\n",
       " 'name': 99.95775497887749,\n",
       " 'text_reviews_count': 34.345202172601084,\n",
       " 'author_id': 100.0,\n",
       " 'average_rating': 1.2371756185878093}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **NaN values**\n",
    "Apparently no NaN values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16570 entries, 0 to 16569\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ratings_count       16570 non-null  int64  \n",
      " 1   name                16570 non-null  object \n",
      " 2   text_reviews_count  16570 non-null  int64  \n",
      " 3   author_id           16570 non-null  int64  \n",
      " 4   average_rating      16570 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 647.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>name</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>author_id</th>\n",
       "      <th>average_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Vesna Velkovrh Bukilica</td>\n",
       "      <td>0</td>\n",
       "      <td>17287029</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>smyr Hydry</td>\n",
       "      <td>0</td>\n",
       "      <td>17209412</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tynrjby khmsy</td>\n",
       "      <td>0</td>\n",
       "      <td>16066657</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7174</td>\n",
       "      <td>Mariam T. Tennoe</td>\n",
       "      <td>12</td>\n",
       "      <td>13975984</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7174</td>\n",
       "      <td>Susan F. Henssonow</td>\n",
       "      <td>12</td>\n",
       "      <td>13975985</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings_count                     name  text_reviews_count  author_id  \\\n",
       "0              0  Vesna Velkovrh Bukilica                   0   17287029   \n",
       "1              0               smyr Hydry                   0   17209412   \n",
       "2              0            tynrjby khmsy                   0   16066657   \n",
       "3           7174         Mariam T. Tennoe                  12   13975984   \n",
       "4           7174       Susan F. Henssonow                  12   13975985   \n",
       "\n",
       "   average_rating  \n",
       "0            0.00  \n",
       "1            0.00  \n",
       "2            0.00  \n",
       "3            4.33  \n",
       "4            4.33  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author names seems to be already standardised and not repeating - its clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for duplicates - no duplicates were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Empty DataFrame\n",
      "Columns: [ratings_count, name, text_reviews_count, author_id, average_rating]\n",
      "Index: []\n",
      "0 rows are duplicates.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"Number of duplicate rows:\", duplicate_rows.shape[0])\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Alternatively, to get a boolean Series indicating duplicates:\n",
    "duplicates_bool = df.duplicated()\n",
    "print(duplicates_bool.sum(), \"rows are duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Goodreads - 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**goodreads_2019_2020.csv (~1.5MB)** \n",
    "\n",
    "There is an issue with the file's formating. <ins>Instead of only 12 fields some entries have 13. We have to look more closely to fix this issue</ins>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 12 fields in line 3350, saw 13\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[294], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Datasets/2_3_goodreads/goodreads_2019_2020.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 12 fields in line 3350, saw 13\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/2_3_goodreads/goodreads_2019_2020.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixing unmatching column number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 3350 has 13 fields: ['12224', 'Streetcar Suburbs: The Process of Growth in Boston  1870-1900', 'Sam Bass Warner', ' Jr./Sam B. Warner', '3.58', '0674842111', '9780674842113', 'en-US', '236', '61', '6', '4/20/2004', 'Harvard University Press']\n",
      "Line 4704 has 13 fields: ['16914', \"The Tolkien Fan's Medieval Reader\", 'David E. Smith (Turgon of TheOneRing.net', ' one of the founding members of this Tolkien website)/Verlyn Flieger/Turgon (=David E. Smith)', '3.58', '1593600119', '9781593600112', 'eng', '400', '26', '4', '4/6/2004', 'Cold Spring Press']\n",
      "Line 5879 has 13 fields: ['22128', 'Patriots (The Coming Collapse)', 'James Wesley', ' Rawles', '3.63', '156384155X', '9781563841552', 'eng', '342', '38', '4', '1/15/1999', 'Huntington House Publishers']\n",
      "Line 8981 has 13 fields: ['34889', \"Brown's Star Atlas: Showing All The Bright Stars With Full Instructions How To Find And Use Them For Navigational Purposes And Department Of Trade Examinations.\", 'Brown', ' Son & Ferguson', '0.00', '0851742718', '9780851742717', 'eng', '49', '0', '0', '5/1/1977', 'Brown Son & Ferguson Ltd.']\n"
     ]
    }
   ],
   "source": [
    "expected_num_columns = 12\n",
    "bad_entries = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, start=1):\n",
    "        # Split the line on commas\n",
    "        fields = line.strip().split(',')\n",
    "        if len(fields) != expected_num_columns:\n",
    "            bad_entries.append((line_num, len(fields), fields))\n",
    "\n",
    "# Print out the bad entries\n",
    "for entry in bad_entries:\n",
    "    line_num, field_count, fields = entry\n",
    "    print(f\"Line {line_num} has {field_count} fields: {fields}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file and check a specific problematic line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 3350 has 13 fields:\n",
      "Field 1: 12224\n",
      "Field 2: Streetcar Suburbs: The Process of Growth in Boston  1870-1900\n",
      "Field 3: Sam Bass Warner\n",
      "Field 4:  Jr./Sam B. Warner\n",
      "Field 5: 3.58\n",
      "Field 6: 0674842111\n",
      "Field 7: 9780674842113\n",
      "Field 8: en-US\n",
      "Field 9: 236\n",
      "Field 10: 61\n",
      "Field 11: 6\n",
      "Field 12: 4/20/2004\n",
      "Field 13: Harvard University Press\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        # Check a few rows for demonstration\n",
    "        if i == 3350:  # Based on the error message (line 3350)\n",
    "            fields = line.strip().split(',')\n",
    "            print(f\"Line {i} has {len(fields)} fields:\")\n",
    "            for idx, field in enumerate(fields, start=1):\n",
    "                print(f\"Field {idx}: {field}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The issue seems to arises because the author's name contains a comma** — \"Sam Bass Warner, Jr./Sam B. Warner\" — **but it isn’t enclosed in quotes**, <ins>so the CSV parser incorrectly splits it into two separate fields</ins>.\n",
    "\n",
    "This results in an extra column being detected (13 fields instead of the expected 12) because the comma in the name is interpreted as a field delimiter rather than as part of the data.\n",
    "\n",
    "**Fixing the Issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making new file to not redo this each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fixed rows: 4\n"
     ]
    }
   ],
   "source": [
    "input_file = \"./Datasets/2_3_goodreads/goodreads_2019_2020.csv\"\n",
    "output_file = \"./Datasets/2_3_goodreads/goodreads_2019_2020_fixed.csv\"\n",
    "\n",
    "expected_num_columns = 12  # the expected number of fields per row\n",
    "fixed_count = 0\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8', newline='') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) == expected_num_columns + 1:\n",
    "            # Increment the counter for bad rows\n",
    "            fixed_count += 1\n",
    "            # Merge the third and fourth fields (index 2 and 3)\n",
    "            merged_field = row[2].strip() + \", \" + row[3].strip()\n",
    "            fixed_row = row[:2] + [merged_field] + row[4:]\n",
    "            writer.writerow(fixed_row)\n",
    "        else:\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"Number of fixed rows:\", fixed_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!! Open fixed File - \"goodreads_2019_2020_fixed.csv\" !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry Potter  #6)</td>\n",
       "      <td>J.K. Rowling/Mary GrandPré</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0439785960</td>\n",
       "      <td>9780439785969</td>\n",
       "      <td>eng</td>\n",
       "      <td>652</td>\n",
       "      <td>2095690</td>\n",
       "      <td>27591</td>\n",
       "      <td>9/16/2006</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Harry Potter  #5)</td>\n",
       "      <td>J.K. Rowling/Mary GrandPré</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0439358078</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td>eng</td>\n",
       "      <td>870</td>\n",
       "      <td>2153167</td>\n",
       "      <td>29221</td>\n",
       "      <td>9/1/2004</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bookID                                                         title  \\\n",
       "0       1     Harry Potter and the Half-Blood Prince (Harry Potter  #6)   \n",
       "1       2  Harry Potter and the Order of the Phoenix (Harry Potter  #5)   \n",
       "\n",
       "                      authors  average_rating        isbn         isbn13  \\\n",
       "0  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
       "1  J.K. Rowling/Mary GrandPré            4.49  0439358078  9780439358071   \n",
       "\n",
       "  language_code    num_pages  ratings_count  text_reviews_count  \\\n",
       "0           eng          652        2095690               27591   \n",
       "1           eng          870        2153167               29221   \n",
       "\n",
       "  publication_date        publisher  \n",
       "0        9/16/2006  Scholastic Inc.  \n",
       "1         9/1/2004  Scholastic Inc.  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_file = \"./Datasets/2_3_goodreads/goodreads_2019_2020_fixed.csv\"\n",
    "\n",
    "df = pd.read_csv(fixed_file)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 11127\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "bookID: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "title: {'min_length': 2, 'max_length': 254, 'mean_length': 35.74916868877505, 'median_length': 31.0}\n",
      "authors: {'min_length': 3, 'max_length': 750, 'mean_length': 24.724364159252268, 'median_length': 17.0}\n",
      "average_rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "isbn: {'min_length': 9, 'max_length': 10, 'mean_length': 9.999910128516222, 'median_length': 10.0}\n",
      "isbn13: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "language_code: {'min_length': 2, 'max_length': 5, 'mean_length': 3.2928911656331445, 'median_length': 3.0}\n",
      "  num_pages: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "ratings_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "text_reviews_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "publication_date: {'min_length': 8, 'max_length': 10, 'mean_length': 8.723914801833379, 'median_length': 9.0}\n",
      "publisher: {'min_length': 2, 'max_length': 67, 'mean_length': 15.180192324975286, 'median_length': 14.0}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "bookID                0.0\n",
      "title                 0.0\n",
      "authors               0.0\n",
      "average_rating        0.0\n",
      "isbn                  0.0\n",
      "isbn13                0.0\n",
      "language_code         0.0\n",
      "  num_pages           0.0\n",
      "ratings_count         0.0\n",
      "text_reviews_count    0.0\n",
      "publication_date      0.0\n",
      "publisher             0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'bookID': 11127, 'title': 10352, 'authors': 6643, 'average_rating': 209, 'isbn': 11127, 'isbn13': 11127, 'language_code': 27, '  num_pages': 997, 'ratings_count': 5294, 'text_reviews_count': 1822, 'publication_date': 3679, 'publisher': 2292}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bookID': 100.0,\n",
       " 'title': 93.03496000718971,\n",
       " 'authors': 59.70162667385639,\n",
       " 'average_rating': 1.878314010964321,\n",
       " 'isbn': 100.0,\n",
       " 'isbn13': 100.0,\n",
       " 'language_code': 0.24265300620113237,\n",
       " '  num_pages': 8.960186932686259,\n",
       " 'ratings_count': 47.57796351217758,\n",
       " 'text_reviews_count': 16.37458434438753,\n",
       " 'publication_date': 33.063718881998746,\n",
       " 'publisher': 20.598544081962793}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apparently no NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11127 entries, 0 to 11126\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   bookID              11127 non-null  int64  \n",
      " 1   title               11127 non-null  object \n",
      " 2   authors             11127 non-null  object \n",
      " 3   average_rating      11127 non-null  float64\n",
      " 4   isbn                11127 non-null  object \n",
      " 5   isbn13              11127 non-null  int64  \n",
      " 6   language_code       11127 non-null  object \n",
      " 7     num_pages         11127 non-null  int64  \n",
      " 8   ratings_count       11127 non-null  int64  \n",
      " 9   text_reviews_count  11127 non-null  int64  \n",
      " 10  publication_date    11127 non-null  object \n",
      " 11  publisher           11127 non-null  object \n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Examining sample of the 'date' column to see its raw values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Format</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M/D/YYYY</td>\n",
       "      <td>11127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Format  Count\n",
       "0    M/D/YYYY  11127"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example function to identify the date format using regex patterns\n",
    "def identify_date_format(date_str):\n",
    "    # Define common patterns with their labels\n",
    "    patterns = {\n",
    "        'M/D/YYYY': r'^\\d{1,2}/\\d{1,2}/\\d{4}$',\n",
    "        'YYYY-MM-DD': r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "        # You can add more patterns here if you suspect other formats\n",
    "    }\n",
    "    for fmt, regex in patterns.items():\n",
    "        if re.match(regex, date_str):\n",
    "            return fmt\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "# Ensure the date column is treated as string\n",
    "df['publication_date'] = df['publication_date'].astype(str)\n",
    "\n",
    "# Apply the identification function to the 'publication_date' column\n",
    "df['date_format'] = df['publication_date'].apply(identify_date_format)\n",
    "\n",
    "# Count the occurrences of each date format\n",
    "format_counts = df['date_format'].value_counts().reset_index()\n",
    "format_counts.columns = ['Date Format', 'Count']\n",
    "\n",
    "format_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seeing if Dates are Valid**\n",
    "\n",
    "Checking the format of dates does not guarantee that the dates themselves are valid. A date may follow the M/D/YYYY format but still be nonsensical. For example, \"31/2/2002\" is invalid because there is no month 31, and \"2/31/2002\" is invalid because February does not have 31 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Date Validity:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is Valid Date</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>11125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Is Valid Date  Count\n",
       "0           True  11125\n",
       "1          False      2"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert publication_date to datetime objects, converting invalid dates to NaT.\n",
    "df['date_parsed'] = pd.to_datetime(df['publication_date'], errors='coerce')\n",
    "\n",
    "# Create a new column that indicates if a date is valid (True if valid, False if not)\n",
    "df['is_valid'] = df['date_parsed'].notnull()\n",
    "\n",
    "# Create and display a summary table of valid vs. invalid dates\n",
    "valid_counts = df['is_valid'].value_counts().reset_index()\n",
    "valid_counts.columns = ['Is Valid Date', 'Count']\n",
    "print(\"\\nSummary of Date Validity:\")\n",
    "valid_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing the two bad entries:**\n",
    "- 11/31/2000: November only has 30 days, so November 31st is not a valid date.\n",
    "- 6/31/1982: June has only 30 days, making June 31st invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date_format</th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8180</th>\n",
       "      <td>31373</td>\n",
       "      <td>In Pursuit of the Proper Sinner (Inspector Lynley  #10)</td>\n",
       "      <td>Elizabeth  George</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0553575104</td>\n",
       "      <td>9780553575101</td>\n",
       "      <td>eng</td>\n",
       "      <td>718</td>\n",
       "      <td>10608</td>\n",
       "      <td>295</td>\n",
       "      <td>11/31/2000</td>\n",
       "      <td>Bantam Books</td>\n",
       "      <td>M/D/YYYY</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098</th>\n",
       "      <td>45531</td>\n",
       "      <td>Montaillou  village occitan de 1294 à 1324</td>\n",
       "      <td>Emmanuel Le Roy Ladurie/Emmanuel Le Roy-Ladurie</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2070323285</td>\n",
       "      <td>9782070323289</td>\n",
       "      <td>fre</td>\n",
       "      <td>640</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>6/31/1982</td>\n",
       "      <td>Folio histoire</td>\n",
       "      <td>M/D/YYYY</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bookID                                                    title  \\\n",
       "8180    31373  In Pursuit of the Proper Sinner (Inspector Lynley  #10)   \n",
       "11098   45531               Montaillou  village occitan de 1294 à 1324   \n",
       "\n",
       "                                               authors  average_rating  \\\n",
       "8180                                 Elizabeth  George            4.10   \n",
       "11098  Emmanuel Le Roy Ladurie/Emmanuel Le Roy-Ladurie            3.96   \n",
       "\n",
       "             isbn         isbn13 language_code    num_pages  ratings_count  \\\n",
       "8180   0553575104  9780553575101           eng          718          10608   \n",
       "11098  2070323285  9782070323289           fre          640             15   \n",
       "\n",
       "       text_reviews_count publication_date       publisher date_format  \\\n",
       "8180                  295       11/31/2000    Bantam Books    M/D/YYYY   \n",
       "11098                   2        6/31/1982  Folio histoire    M/D/YYYY   \n",
       "\n",
       "      date_parsed  is_valid  \n",
       "8180          NaT     False  \n",
       "11098         NaT     False  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['date_parsed'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it was only two entries, we opted to fix them manually by searching their isbn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date_format</th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8180</th>\n",
       "      <td>31373</td>\n",
       "      <td>In Pursuit of the Proper Sinner (Inspector Lynley  #10)</td>\n",
       "      <td>Elizabeth  George</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0553575104</td>\n",
       "      <td>9780553575101</td>\n",
       "      <td>eng</td>\n",
       "      <td>718</td>\n",
       "      <td>10608</td>\n",
       "      <td>295</td>\n",
       "      <td>11/31/2000</td>\n",
       "      <td>Bantam Books</td>\n",
       "      <td>M/D/YYYY</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bookID                                                    title  \\\n",
       "8180   31373  In Pursuit of the Proper Sinner (Inspector Lynley  #10)   \n",
       "\n",
       "                authors  average_rating        isbn         isbn13  \\\n",
       "8180  Elizabeth  George             4.1  0553575104  9780553575101   \n",
       "\n",
       "     language_code    num_pages  ratings_count  text_reviews_count  \\\n",
       "8180           eng          718          10608                 295   \n",
       "\n",
       "     publication_date     publisher date_format date_parsed  is_valid  \n",
       "8180       11/31/2000  Bantam Books    M/D/YYYY         NaT     False  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"publication_date\"] == \"11/31/2000\"] # Octorber 31, 2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date_format</th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11098</th>\n",
       "      <td>45531</td>\n",
       "      <td>Montaillou  village occitan de 1294 à 1324</td>\n",
       "      <td>Emmanuel Le Roy Ladurie/Emmanuel Le Roy-Ladurie</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2070323285</td>\n",
       "      <td>9782070323289</td>\n",
       "      <td>fre</td>\n",
       "      <td>640</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>6/31/1982</td>\n",
       "      <td>Folio histoire</td>\n",
       "      <td>M/D/YYYY</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bookID                                       title  \\\n",
       "11098   45531  Montaillou  village occitan de 1294 à 1324   \n",
       "\n",
       "                                               authors  average_rating  \\\n",
       "11098  Emmanuel Le Roy Ladurie/Emmanuel Le Roy-Ladurie            3.96   \n",
       "\n",
       "             isbn         isbn13 language_code    num_pages  ratings_count  \\\n",
       "11098  2070323285  9782070323289           fre          640             15   \n",
       "\n",
       "       text_reviews_count publication_date       publisher date_format  \\\n",
       "11098                   2        6/31/1982  Folio histoire    M/D/YYYY   \n",
       "\n",
       "      date_parsed  is_valid  \n",
       "11098         NaT     False  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"publication_date\"] == \"6/31/1982\"] # June 1, 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new column \"date_parsed_iso\" in **ISO 8601 format**.\n",
    "\n",
    "ISO 8601 is an international standard for **representing dates and times in a clear, consistent, and unambiguous way**. \n",
    "\n",
    "- It uses a numeric format like `YYYY-MM-DD` for dates and extends to times by separating the date and time with a \"T\" (e.g., `YYYY-MM-DDTHH:MM:SS`).\n",
    "\n",
    "This format is widely adopted because it sorts naturally and minimizes confusion across different regions and systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publication_date to datetime objects; invalid dates become NaT.\n",
    "df['date_parsed'] = pd.to_datetime(df['publication_date'], errors='coerce')\n",
    "\n",
    "# Correct the invalid dates in the date_parsed column:\n",
    "df.loc[df[\"publication_date\"] == \"11/31/2000\", \"date_parsed\"] = pd.to_datetime(\"10/31/2000\")\n",
    "df.loc[df[\"publication_date\"] == \"6/31/1982\", \"date_parsed\"] = pd.to_datetime(\"6/1/2000\")\n",
    "\n",
    "# Create a new column with the ISO format (YYYY-MM-DD) of the valid dates\n",
    "df['date_parsed_iso'] = df['date_parsed'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Instead of filtering using strings, convert the list of dates to datetime objects:\n",
    "dates_to_check = pd.to_datetime([\"10/31/2000\", \"6/1/2000\"])\n",
    "\n",
    "# Filter rows where date_parsed matches one of these dates\n",
    "filtered_df = df[df[\"date_parsed\"].isin(dates_to_check)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date_parsed_iso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8180</th>\n",
       "      <td>31373</td>\n",
       "      <td>In Pursuit of the Proper Sinner (Inspector Lynley  #10)</td>\n",
       "      <td>Elizabeth  George</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0553575104</td>\n",
       "      <td>9780553575101</td>\n",
       "      <td>eng</td>\n",
       "      <td>718</td>\n",
       "      <td>10608</td>\n",
       "      <td>295</td>\n",
       "      <td>11/31/2000</td>\n",
       "      <td>Bantam Books</td>\n",
       "      <td>2000-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098</th>\n",
       "      <td>45531</td>\n",
       "      <td>Montaillou  village occitan de 1294 à 1324</td>\n",
       "      <td>Emmanuel Le Roy Ladurie/Emmanuel Le Roy-Ladurie</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2070323285</td>\n",
       "      <td>9782070323289</td>\n",
       "      <td>fre</td>\n",
       "      <td>640</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>6/31/1982</td>\n",
       "      <td>Folio histoire</td>\n",
       "      <td>2000-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bookID                                                    title  \\\n",
       "8180    31373  In Pursuit of the Proper Sinner (Inspector Lynley  #10)   \n",
       "11098   45531               Montaillou  village occitan de 1294 à 1324   \n",
       "\n",
       "                                               authors  average_rating  \\\n",
       "8180                                 Elizabeth  George            4.10   \n",
       "11098  Emmanuel Le Roy Ladurie/Emmanuel Le Roy-Ladurie            3.96   \n",
       "\n",
       "             isbn         isbn13 language_code    num_pages  ratings_count  \\\n",
       "8180   0553575104  9780553575101           eng          718          10608   \n",
       "11098  2070323285  9782070323289           fre          640             15   \n",
       "\n",
       "       text_reviews_count publication_date       publisher date_parsed_iso  \n",
       "8180                  295       11/31/2000    Bantam Books      2000-10-31  \n",
       "11098                   2        6/31/1982  Folio histoire      2000-06-01  "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['date_parsed', 'date_format', 'is_valid' ])\n",
    "\n",
    "df[df[\"bookID\"].isin([45531, 31373])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_parsed_iso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005-04-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2005-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2005-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2002-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2004-08-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_parsed_iso\n",
       "0      2006-09-16\n",
       "1      2004-09-01\n",
       "2      2003-11-01\n",
       "3      2004-05-01\n",
       "4      2004-09-13\n",
       "5      2005-04-26\n",
       "6      2005-09-12\n",
       "7      2005-11-01\n",
       "8      2002-04-30\n",
       "9      2004-08-03"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['date_parsed_iso']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"authors\"**\n",
    "Entries seem to be able to have multiple authors let's see if we can fix that.\n",
    "\n",
    "Some entries seem to have multiple authors split by \"/\". Let's fix that making \"authors\" have as values lists of author names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting \"authors\" column values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>language_code</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date_parsed_iso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry Potter  #6)</td>\n",
       "      <td>[J.K. Rowling, Mary GrandPré]</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0439785960</td>\n",
       "      <td>9780439785969</td>\n",
       "      <td>eng</td>\n",
       "      <td>652</td>\n",
       "      <td>2095690</td>\n",
       "      <td>27591</td>\n",
       "      <td>9/16/2006</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>2006-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Harry Potter  #5)</td>\n",
       "      <td>[J.K. Rowling, Mary GrandPré]</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0439358078</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td>eng</td>\n",
       "      <td>870</td>\n",
       "      <td>2153167</td>\n",
       "      <td>29221</td>\n",
       "      <td>9/1/2004</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>2004-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry Potter  #2)</td>\n",
       "      <td>[J.K. Rowling]</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0439554896</td>\n",
       "      <td>9780439554893</td>\n",
       "      <td>eng</td>\n",
       "      <td>352</td>\n",
       "      <td>6333</td>\n",
       "      <td>244</td>\n",
       "      <td>11/1/2003</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>2003-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harry Potter  #3)</td>\n",
       "      <td>[J.K. Rowling, Mary GrandPré]</td>\n",
       "      <td>4.56</td>\n",
       "      <td>043965548X</td>\n",
       "      <td>9780439655484</td>\n",
       "      <td>eng</td>\n",
       "      <td>435</td>\n",
       "      <td>2339585</td>\n",
       "      <td>36325</td>\n",
       "      <td>5/1/2004</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>2004-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bookID                                                         title  \\\n",
       "0       1     Harry Potter and the Half-Blood Prince (Harry Potter  #6)   \n",
       "1       2  Harry Potter and the Order of the Phoenix (Harry Potter  #5)   \n",
       "2       4    Harry Potter and the Chamber of Secrets (Harry Potter  #2)   \n",
       "3       5   Harry Potter and the Prisoner of Azkaban (Harry Potter  #3)   \n",
       "\n",
       "                         authors  average_rating        isbn         isbn13  \\\n",
       "0  [J.K. Rowling, Mary GrandPré]            4.57  0439785960  9780439785969   \n",
       "1  [J.K. Rowling, Mary GrandPré]            4.49  0439358078  9780439358071   \n",
       "2                 [J.K. Rowling]            4.42  0439554896  9780439554893   \n",
       "3  [J.K. Rowling, Mary GrandPré]            4.56  043965548X  9780439655484   \n",
       "\n",
       "  language_code    num_pages  ratings_count  text_reviews_count  \\\n",
       "0           eng          652        2095690               27591   \n",
       "1           eng          870        2153167               29221   \n",
       "2           eng          352           6333                 244   \n",
       "3           eng          435        2339585               36325   \n",
       "\n",
       "  publication_date        publisher date_parsed_iso  \n",
       "0        9/16/2006  Scholastic Inc.      2006-09-16  \n",
       "1         9/1/2004  Scholastic Inc.      2004-09-01  \n",
       "2        11/1/2003       Scholastic      2003-11-01  \n",
       "3         5/1/2004  Scholastic Inc.      2004-05-01  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_authors(author_str):\n",
    "    # Split by '/' and remove extra spaces around each author\n",
    "    return [a.strip() for a in author_str.split('/')]\n",
    "\n",
    "# Apply the function to the 'author' column\n",
    "df['authors'] = df['authors'].apply(split_authors)\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors_joined</th>\n",
       "      <th>authors_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>Garrison Keillor/Charles Bukowski/Robert Burns/Hayden Carruth/Raymond Carver/Billy Collins/Noël ...</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>Nicholas Dawidoff/Ernest Lawrence Thayer/Jacques Barzun/Robert Frost/John Updike/Leroy Satchel P...</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>Plato/John M. Cooper/Benjamin Jowett/Dorothea Frede/Alexander Nehamas/Paul Woodruff/Anthony Kenn...</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>James Patterson/Ted Bell/Raelynn Hillhouse/Gregg Hurwitz/Alex Kava/J.A. Konrath/John Lescroart/D...</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Orson Scott Card/Isaac Asimov/William Gibson/Michael Swanwick/Theodore Sturgeon/Larry Niven/Robe...</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           authors_joined  \\\n",
       "9870  Garrison Keillor/Charles Bukowski/Robert Burns/Hayden Carruth/Raymond Carver/Billy Collins/Noël ...   \n",
       "8139  Nicholas Dawidoff/Ernest Lawrence Thayer/Jacques Barzun/Robert Frost/John Updike/Leroy Satchel P...   \n",
       "2566  Plato/John M. Cooper/Benjamin Jowett/Dorothea Frede/Alexander Nehamas/Paul Woodruff/Anthony Kenn...   \n",
       "8736  James Patterson/Ted Bell/Raelynn Hillhouse/Gregg Hurwitz/Alex Kava/J.A. Konrath/John Lescroart/D...   \n",
       "9998  Orson Scott Card/Isaac Asimov/William Gibson/Michael Swanwick/Theodore Sturgeon/Larry Niven/Robe...   \n",
       "\n",
       "      authors_length  \n",
       "9870             750  \n",
       "8139             571  \n",
       "2566             540  \n",
       "8736             461  \n",
       "9998             441  "
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column that joins the list of authors into a single string (if not already a string)\n",
    "df['authors_joined'] = df['authors'].apply(lambda x: \"/\".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Create a column with the length of the joined authors string\n",
    "df['authors_length'] = df['authors_joined'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "# Sort the DataFrame by authors_length in descending order and display the top rows\n",
    "highest_length_authors = df.sort_values('authors_length', ascending=False)[['authors_joined', 'authors_length']]\n",
    "highest_length_authors.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some books are colections of texts/ poetry from many different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>authors_joined</th>\n",
       "      <th>authors_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>Good Poems for Hard Times</td>\n",
       "      <td>0143037676</td>\n",
       "      <td>9780143037675</td>\n",
       "      <td>Garrison Keillor/Charles Bukowski/Robert Burns/Hayden Carruth/Raymond Carver/Billy Collins/Noël ...</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>Baseball: a Literary Anthology</td>\n",
       "      <td>193108209X</td>\n",
       "      <td>9781931082099</td>\n",
       "      <td>Nicholas Dawidoff/Ernest Lawrence Thayer/Jacques Barzun/Robert Frost/John Updike/Leroy Satchel P...</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>Plato: Complete Works</td>\n",
       "      <td>0872203492</td>\n",
       "      <td>9780872203495</td>\n",
       "      <td>Plato/John M. Cooper/Benjamin Jowett/Dorothea Frede/Alexander Nehamas/Paul Woodruff/Anthony Kenn...</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>Thriller: Stories To Keep You Up All Night</td>\n",
       "      <td>0778322998</td>\n",
       "      <td>9780778322993</td>\n",
       "      <td>James Patterson/Ted Bell/Raelynn Hillhouse/Gregg Hurwitz/Alex Kava/J.A. Konrath/John Lescroart/D...</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Masterpieces: The Best Science Fiction of the Twentieth Century</td>\n",
       "      <td>0441011330</td>\n",
       "      <td>9780441011339</td>\n",
       "      <td>Orson Scott Card/Isaac Asimov/William Gibson/Michael Swanwick/Theodore Sturgeon/Larry Niven/Robe...</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689</th>\n",
       "      <td>The Best American Comics 2006</td>\n",
       "      <td>0618718745</td>\n",
       "      <td>9780618718740</td>\n",
       "      <td>Harvey Pekar/Anne Elizabeth Moore/Esther Pearl Watson/Lilli Carré/Robert Crumb/Chris Ware/Kim De...</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10158</th>\n",
       "      <td>Modern Classics of Science Fiction</td>\n",
       "      <td>0312088477</td>\n",
       "      <td>9780312088477</td>\n",
       "      <td>Gardner Dozois/Damon Knight/R.A. Lafferty/Samuel R. Delany/Brian W. Aldiss/Gene Wolfe/Joanna Rus...</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8921</th>\n",
       "      <td>The Flying Sorcerers: More Comic Tales of Fantasy</td>\n",
       "      <td>1857237250</td>\n",
       "      <td>9781857237252</td>\n",
       "      <td>Peter Haining/Roald Dahl/Terry Pratchett/Angela Carter/Arthur C. Clarke/Kurt Vonnegut Jr./C.S. L...</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>A Portrait of the Artist As a Young Man</td>\n",
       "      <td>0393926796</td>\n",
       "      <td>9780393926798</td>\n",
       "      <td>James Joyce/Walter Hettche/Hans Walter Gabler/John Paul Riquelme/John Mitchel/Michael Davitt/Can...</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>Key Topics in Conservation Biology</td>\n",
       "      <td>1405122498</td>\n",
       "      <td>9781405122498</td>\n",
       "      <td>David W. Macdonald/Alonzo C. Addison/Sandra Baker/Mark S. Boyce/David Brown/Stephen Cobb/N. Mark...</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 title  \\\n",
       "9870                                         Good Poems for Hard Times   \n",
       "8139                                    Baseball: a Literary Anthology   \n",
       "2566                                             Plato: Complete Works   \n",
       "8736                        Thriller: Stories To Keep You Up All Night   \n",
       "9998   Masterpieces: The Best Science Fiction of the Twentieth Century   \n",
       "10689                                    The Best American Comics 2006   \n",
       "10158                               Modern Classics of Science Fiction   \n",
       "8921                 The Flying Sorcerers: More Comic Tales of Fantasy   \n",
       "2101                           A Portrait of the Artist As a Young Man   \n",
       "2022                                Key Topics in Conservation Biology   \n",
       "\n",
       "             isbn         isbn13  \\\n",
       "9870   0143037676  9780143037675   \n",
       "8139   193108209X  9781931082099   \n",
       "2566   0872203492  9780872203495   \n",
       "8736   0778322998  9780778322993   \n",
       "9998   0441011330  9780441011339   \n",
       "10689  0618718745  9780618718740   \n",
       "10158  0312088477  9780312088477   \n",
       "8921   1857237250  9781857237252   \n",
       "2101   0393926796  9780393926798   \n",
       "2022   1405122498  9781405122498   \n",
       "\n",
       "                                                                                            authors_joined  \\\n",
       "9870   Garrison Keillor/Charles Bukowski/Robert Burns/Hayden Carruth/Raymond Carver/Billy Collins/Noël ...   \n",
       "8139   Nicholas Dawidoff/Ernest Lawrence Thayer/Jacques Barzun/Robert Frost/John Updike/Leroy Satchel P...   \n",
       "2566   Plato/John M. Cooper/Benjamin Jowett/Dorothea Frede/Alexander Nehamas/Paul Woodruff/Anthony Kenn...   \n",
       "8736   James Patterson/Ted Bell/Raelynn Hillhouse/Gregg Hurwitz/Alex Kava/J.A. Konrath/John Lescroart/D...   \n",
       "9998   Orson Scott Card/Isaac Asimov/William Gibson/Michael Swanwick/Theodore Sturgeon/Larry Niven/Robe...   \n",
       "10689  Harvey Pekar/Anne Elizabeth Moore/Esther Pearl Watson/Lilli Carré/Robert Crumb/Chris Ware/Kim De...   \n",
       "10158  Gardner Dozois/Damon Knight/R.A. Lafferty/Samuel R. Delany/Brian W. Aldiss/Gene Wolfe/Joanna Rus...   \n",
       "8921   Peter Haining/Roald Dahl/Terry Pratchett/Angela Carter/Arthur C. Clarke/Kurt Vonnegut Jr./C.S. L...   \n",
       "2101   James Joyce/Walter Hettche/Hans Walter Gabler/John Paul Riquelme/John Mitchel/Michael Davitt/Can...   \n",
       "2022   David W. Macdonald/Alonzo C. Addison/Sandra Baker/Mark S. Boyce/David Brown/Stephen Cobb/N. Mark...   \n",
       "\n",
       "       authors_length  \n",
       "9870              750  \n",
       "8139              571  \n",
       "2566              540  \n",
       "8736              461  \n",
       "9998              441  \n",
       "10689             428  \n",
       "10158             396  \n",
       "8921              384  \n",
       "2101              372  \n",
       "2022              361  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame by authors_length in descending order and select the top rows\n",
    "highest_length_authors = df.sort_values('authors_length', ascending=False).head(10)\n",
    "\n",
    "# Display the title, joined authors, and the authors_length for these entries\n",
    "highest_length_authors[['title','isbn', 'isbn13', 'authors_joined', 'authors_length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column \"author\", with the first of the author on the \"authors\" column (seems that the first is always the main one, other are sometimes artists, voice actors, etc. STILL keeping \"authors\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         authors        author\n",
      "0  [J.K. Rowling, Mary GrandPré]  J.K. Rowling\n",
      "1  [J.K. Rowling, Mary GrandPré]  J.K. Rowling\n",
      "2                 [J.K. Rowling]  J.K. Rowling\n",
      "3  [J.K. Rowling, Mary GrandPré]  J.K. Rowling\n",
      "4  [J.K. Rowling, Mary GrandPré]  J.K. Rowling\n"
     ]
    }
   ],
   "source": [
    "# Create a new column \"author\" that takes the first author from the \"authors\" column\n",
    "df['author'] = df['authors'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else np.nan)\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "print(df[['authors', 'author']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Author**\n",
    "\n",
    "Seems to have fixed some little issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'J.K. Rowling'}\n",
      "{'W. Frederick Zimmerman'}\n",
      "{'Douglas Adams'}\n",
      "{'Bill Bryson', 'Bill  Bryson'}\n",
      "{'J.R.R. Tolkien'}\n",
      "{'Chris   Smith'}\n",
      "{'Jude Fisher'}\n",
      "{'Dave Thomas'}\n",
      "{'Gary Paulsen'}\n",
      "{'Donna Ickes'}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'author', threshold=92, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:10]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized author names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W. Frederick Zimmerman</td>\n",
       "      <td>W. Frederick Zimmerman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>Douglas Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>Douglas Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>Douglas Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author         author_standard\n",
       "0            J.K. Rowling            J.K. Rowling\n",
       "1            J.K. Rowling            J.K. Rowling\n",
       "2            J.K. Rowling            J.K. Rowling\n",
       "3            J.K. Rowling            J.K. Rowling\n",
       "4            J.K. Rowling            J.K. Rowling\n",
       "5  W. Frederick Zimmerman  W. Frederick Zimmerman\n",
       "6            J.K. Rowling            J.K. Rowling\n",
       "7           Douglas Adams           Douglas Adams\n",
       "8           Douglas Adams           Douglas Adams\n",
       "9           Douglas Adams           Douglas Adams"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df['author'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "\n",
    "\n",
    "df['author_standard'] = df['author'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized author names added to DataFrame:\")\n",
    "df[['author', 'author_standard']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"language_code\"**\n",
    "\n",
    "Seems to be all right and similar to the prior Goodreads 2017 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique language codes from data: ['eng' 'en-US' 'fre' 'spa' 'en-GB' 'mul' 'grc' 'enm' 'en-CA' 'ger' 'jpn'\n",
      " 'ara' 'nl' 'zho' 'lat' 'por' 'srp' 'ita' 'rus' 'msa' 'glg' 'wel' 'swe'\n",
      " 'nor' 'tur' 'gla' 'ale']\n",
      "Valid ISO language codes: ['eng', 'spa', 'mul', 'grc', 'enm', 'jpn', 'ara', 'nl', 'zho', 'lat', 'por', 'srp', 'ita', 'rus', 'msa', 'glg', 'swe', 'nor', 'tur', 'gla', 'ale']\n",
      "Invalid ISO language codes: ['en-US', 'fre', 'en-GB', 'en-CA', 'ger', 'wel']\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "# Get unique language codes from the DataFrame (drop null values)\n",
    "unique_language_codes = df['language_code'].dropna().unique()\n",
    "print(\"Unique language codes from data:\", unique_language_codes)\n",
    "\n",
    "# Check each code to see if it is a valid ISO 3166-1 alpha-2 language code.\n",
    "valid_codes = []\n",
    "invalid_codes = []\n",
    "\n",
    "for code in unique_language_codes:\n",
    "    # Attempt to retrieve language information using alpha-2 code\n",
    "    language = pycountry.languages.get(alpha_2=code)\n",
    "    if language is None:\n",
    "        # If not found using alpha-2, try with alpha_3 as a fallback (if you expect 3-letter codes)\n",
    "        language = pycountry.languages.get(alpha_3=code)\n",
    "    if language:\n",
    "        valid_codes.append(code)\n",
    "    else:\n",
    "        invalid_codes.append(code)\n",
    "\n",
    "print(\"Valid ISO language codes:\", valid_codes)\n",
    "print(\"Invalid ISO language codes:\", invalid_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid ISO language codes after correction: []\n",
      "  language_raw lang_iso\n",
      "0          eng      eng\n",
      "1          eng      eng\n",
      "2          eng      eng\n",
      "3          eng      eng\n",
      "4          eng      eng\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={'language_code': 'language_raw'}, inplace=True)\n",
    "\n",
    "# Define a mapping for problematic codes to their ISO 639-1 equivalents\n",
    "corrections = {\n",
    "    'en-GB': 'en',\n",
    "    'en-US': 'en',\n",
    "    'en-CA': 'en',\n",
    "    'rum': 'ro',   # Romanian\n",
    "    'per': 'fa',   # Persian/Farsi\n",
    "    'fre': 'fr',   # French\n",
    "    'ger': 'de',   # German\n",
    "    'cze': 'cs',   # Czech\n",
    "    'slo': 'sk',   # Slovak\n",
    "    'gre': 'el',   # Greek\n",
    "    'es-MX': 'es',  # Mexican Spanish → generic Spanish\n",
    "    'wel': 'cy'     # Welsh (ISO 639-1: \"cy\")\n",
    "}\n",
    "\n",
    "def correct_language_code(code):\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    # If the code contains a hyphen, use only the base language part\n",
    "    if '-' in code:\n",
    "        code = code.split('-')[0]\n",
    "    # Apply corrections if the code is in our mapping\n",
    "    return corrections.get(code, code)\n",
    "\n",
    "# Create a new column \"language_iso\" with the corrected ISO codes\n",
    "df['lang_iso'] = df['language_raw'].apply(correct_language_code)\n",
    "\n",
    "# Optionally, validate the new codes using pycountry\n",
    "def validate_iso(code):\n",
    "    if not code:\n",
    "        return False\n",
    "    language = pycountry.languages.get(alpha_2=code)\n",
    "    if language is None:\n",
    "        language = pycountry.languages.get(alpha_3=code)\n",
    "    return language is not None\n",
    "\n",
    "invalid_iso = df.loc[~df['lang_iso'].apply(validate_iso), 'lang_iso'].unique()\n",
    "print(\"Invalid ISO language codes after correction:\", invalid_iso)\n",
    "\n",
    "# Display the original and corrected language columns\n",
    "print(df[['language_raw', 'lang_iso']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Standardise/ Harmonise \"publisher\" categorical values with Rapidfuzz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'Scholastic  Inc.', 'Scholastic Inc', 'Scholastic Inc.'}\n",
      "{'Scholastic'}\n",
      "{'Nimble Books'}\n",
      "{'Gramercy Books'}\n",
      "{'Del Rey Books'}\n",
      "{'Crown'}\n",
      "{'Random House Audio'}\n",
      "{'Wings Books'}\n",
      "{'Broadway Books'}\n",
      "{'William Morrow Paperbacks'}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'publisher', threshold=90, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:10]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized publisher names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>publisher_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scholastic</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scholastic</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nimble Books</td>\n",
       "      <td>Nimble Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Scholastic</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gramercy Books</td>\n",
       "      <td>Gramercy Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Del Rey Books</td>\n",
       "      <td>Del Rey Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Crown</td>\n",
       "      <td>Crown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publisher publisher_standard\n",
       "0  Scholastic Inc.    Scholastic Inc.\n",
       "1  Scholastic Inc.    Scholastic Inc.\n",
       "2       Scholastic         Scholastic\n",
       "3  Scholastic Inc.    Scholastic Inc.\n",
       "4       Scholastic         Scholastic\n",
       "5     Nimble Books       Nimble Books\n",
       "6       Scholastic         Scholastic\n",
       "7   Gramercy Books     Gramercy Books\n",
       "8    Del Rey Books      Del Rey Books\n",
       "9            Crown              Crown"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df['publisher'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "\n",
    "\n",
    "df['publisher_standard'] = df['publisher'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized publisher names added to DataFrame:\")\n",
    "df[['publisher', 'publisher_standard']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bookID', 'title', 'authors', 'average_rating', 'isbn', 'isbn13',\n",
       "       'language_raw', '  num_pages', 'ratings_count', 'text_reviews_count',\n",
       "       'publication_date', 'publisher', 'date_parsed_iso', 'authors_joined',\n",
       "       'authors_length', 'author', 'author_standard', 'lang_iso',\n",
       "       'publisher_standard'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['authors_joined', 'authors_length', 'author'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_mapping = {\n",
    "    'bookID': 'incremented_id',\n",
    "    'title': 'title',\n",
    "    'authors': 'authors',\n",
    "    'average_rating': 'avg_rating',\n",
    "    'isbn': 'isbn_10',\n",
    "    'isbn13': 'isbn_13',\n",
    "    'language_raw': 'language_raw',\n",
    "    '  num_pages': 'num_pages',  # Remove extra spaces\n",
    "    'ratings_count': 'ratings_count',\n",
    "    'text_reviews_count': 'text_reviews_count',\n",
    "    'publication_date': 'publication_date_raw',\n",
    "    'publisher': 'publisher_raw',\n",
    "    'author_standard': 'author_standard',\n",
    "    'lang_iso': 'lang_iso',\n",
    "    'publisher_standard': 'publisher'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/2_3_goodreads/goodreads_2019_2020_fixed_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)  Book‑Crossing Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) i) BookCrossing Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/q59hq4f165zf1s43dyw9y7d80000gn/T/ipykernel_57802/43527700.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/4_bookcrossing/book_crossing_Books.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check what is really causing this issue!**\n",
    "\n",
    "Seems that some date are being taken as NUMERIC and others as STR. Later well convert them to unix time though. We played around with the acceptable range of valid years to see the issue.\n",
    "\n",
    "- 4621 entries/ rows have **0** has their 'Year-Of-Publication'.\n",
    "- 12 entries/ rows with **years ahead of 2025** have their 'Year-Of-Publication', which is implausible since this dataset was published in 2024.\n",
    "- 3 entries/ rows have an issue with their structuring - The titles contain extra escaped quotes (\\\") and semicolons (;) that appear to be artefacts from the export process. This induced the titles to not be properly parsed during data extraction with pandas, the supposed 'Book-Author' values to be fused with \"Book-Title\" values, and consequently the other values to be moved to the incorrect column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Datasets/4_bookcrossing/book_crossing_Books.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 271360 entries, 0 to 271359\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   ISBN                 271360 non-null  object\n",
      " 1   Book-Title           271360 non-null  object\n",
      " 2   Book-Author          271358 non-null  object\n",
      " 3   Year-Of-Publication  271360 non-null  object\n",
      " 4   Publisher            271358 non-null  object\n",
      " 5   Image-URL-S          271360 non-null  object\n",
      " 6   Image-URL-M          271360 non-null  object\n",
      " 7   Image-URL-L          271357 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 16.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 271360\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**\n",
    "\n",
    "By these we can tell there are a few ibsn_13 but the majority is isbn_10 (10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "ISBN: {'min_length': 10, 'max_length': 13, 'mean_length': 10.000036851415095, 'median_length': 10.0}\n",
      "Book-Title: {'min_length': 1, 'max_length': 256, 'mean_length': 37.83116892688679, 'median_length': 32.0}\n",
      "Book-Author: {'min_length': 1, 'max_length': 143, 'mean_length': 13.950920923650676, 'median_length': 13.0}\n",
      "Year-Of-Publication: {'min_length': 1, 'max_length': 17, 'mean_length': 3.9490602889150943, 'median_length': 4.0}\n",
      "Publisher: {'min_length': 1, 'max_length': 134, 'mean_length': 15.903223785552665, 'median_length': 14.0}\n",
      "Image-URL-S: {'min_length': 60, 'max_length': 60, 'mean_length': 60.0, 'median_length': 60.0}\n",
      "Image-URL-M: {'min_length': 60, 'max_length': 60, 'mean_length': 60.0, 'median_length': 60.0}\n",
      "Image-URL-L: {'min_length': 60, 'max_length': 60, 'mean_length': 60.0, 'median_length': 60.0}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "ISBN                   0.000000\n",
      "Book-Title             0.000000\n",
      "Book-Author            0.000737\n",
      "Year-Of-Publication    0.000000\n",
      "Publisher              0.000737\n",
      "Image-URL-S            0.000000\n",
      "Image-URL-M            0.000000\n",
      "Image-URL-L            0.001106\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'ISBN': 271360, 'Book-Title': 242135, 'Book-Author': 102022, 'Year-Of-Publication': 118, 'Publisher': 16807, 'Image-URL-S': 271044, 'Image-URL-M': 271044, 'Image-URL-L': 271041}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ISBN': 100.0,\n",
       " 'Book-Title': 89.23017393867924,\n",
       " 'Book-Author': 37.59655070754717,\n",
       " 'Year-Of-Publication': 0.04348466981132076,\n",
       " 'Publisher': 6.193617334905661,\n",
       " 'Image-URL-S': 99.88354952830188,\n",
       " 'Image-URL-M': 99.88354952830188,\n",
       " 'Image-URL-L': 99.88244398584905}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN            Book-Title           Book-Author Year-Of-Publication  \\\n",
       "0  0195153448   Classical Mythology    Mark P. O. Morford                2002   \n",
       "1  0002005018          Clara Callan  Richard Bruce Wright                2001   \n",
       "2  0060973129  Decision in Normandy          Carlo D'Este                1991   \n",
       "\n",
       "                 Publisher  \\\n",
       "0  Oxford University Press   \n",
       "1    HarperFlamingo Canada   \n",
       "2          HarperPerennial   \n",
       "\n",
       "                                                    Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg   \n",
       "2  http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                    Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg   \n",
       "2  http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                    Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg  \n",
       "1  http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg  \n",
       "2  http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with invalid Year-Of-Publication:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270794</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271094</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4618 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year-Of-Publication Year-Of-Publication_clean  Year_numeric\n",
       "176                      0                         0             0\n",
       "188                      0                         0             0\n",
       "288                      0                         0             0\n",
       "351                      0                         0             0\n",
       "542                      0                         0             0\n",
       "...                    ...                       ...           ...\n",
       "270794                   0                         0             0\n",
       "270913                   0                         0             0\n",
       "271094                   0                         0             0\n",
       "271182                   0                         0             0\n",
       "271196                   0                         0             0\n",
       "\n",
       "[4618 rows x 3 columns]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Year-Of-Publication_clean'] = df['Year-Of-Publication'].astype(str).str.strip()\n",
    "df['Year_numeric'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
    "\n",
    "# Define a plausible range for valid years. Adjust min_year as needed. (some of this dates feel wrong)\n",
    "min_year = 1\n",
    "max_year = 3000\n",
    "\n",
    "invalid_years = df[\n",
    "    (df['Year_numeric'].isna()) |\n",
    "    (df['Year_numeric'] < min_year) |\n",
    "    (df['Year_numeric'] > max_year)\n",
    "]\n",
    "\n",
    "print(\"Entries with invalid Year-Of-Publication:\")\n",
    "invalid_years[['Year-Of-Publication', 'Year-Of-Publication_clean', 'Year_numeric']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn them to null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark invalid year entries (e.g., year 0) as NaN\n",
    "df.loc[df['Year_numeric'] < min_year, 'Year_numeric'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with invalid Year-Of-Publication:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37487</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55676</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78168</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80264</th>\n",
       "      <td>2050</td>\n",
       "      <td>2050</td>\n",
       "      <td>2050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97826</th>\n",
       "      <td>2050</td>\n",
       "      <td>2050</td>\n",
       "      <td>2050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116053</th>\n",
       "      <td>2038</td>\n",
       "      <td>2038</td>\n",
       "      <td>2038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118294</th>\n",
       "      <td>2026</td>\n",
       "      <td>2026</td>\n",
       "      <td>2026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192993</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209538</th>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220731</th>\n",
       "      <td>Gallimard</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221678</th>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228173</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240169</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255409</th>\n",
       "      <td>2037</td>\n",
       "      <td>2037</td>\n",
       "      <td>2037.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260974</th>\n",
       "      <td>2030</td>\n",
       "      <td>2030</td>\n",
       "      <td>2030.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year-Of-Publication Year-Of-Publication_clean  Year_numeric\n",
       "37487                 2030                      2030        2030.0\n",
       "55676                 2030                      2030        2030.0\n",
       "78168                 2030                      2030        2030.0\n",
       "80264                 2050                      2050        2050.0\n",
       "97826                 2050                      2050        2050.0\n",
       "116053                2038                      2038        2038.0\n",
       "118294                2026                      2026        2026.0\n",
       "192993                2030                      2030        2030.0\n",
       "209538   DK Publishing Inc         DK Publishing Inc           NaN\n",
       "220731           Gallimard                 Gallimard           NaN\n",
       "221678   DK Publishing Inc         DK Publishing Inc           NaN\n",
       "228173                2030                      2030        2030.0\n",
       "240169                2030                      2030        2030.0\n",
       "255409                2037                      2037        2037.0\n",
       "260974                2030                      2030        2030.0"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Year-Of-Publication_clean'] = df['Year-Of-Publication'].astype(str).str.strip()\n",
    "df['Year_numeric'] = pd.to_numeric(df['Year-Of-Publication_clean'], errors='coerce')\n",
    "\n",
    "# Define a plausible range for valid years. Adjust min_year as needed. (some of this dates feel wrong)\n",
    "min_year = 0\n",
    "max_year = 2024 # this dataset was published in 2024\n",
    "\n",
    "invalid_years = df[\n",
    "    (df['Year_numeric'].isna()) |\n",
    "    (df['Year_numeric'] < min_year) |\n",
    "    (df['Year_numeric'] > max_year)\n",
    "]\n",
    "\n",
    "print(\"Entries with invalid Year-Of-Publication:\")\n",
    "invalid_years[['Year-Of-Publication', 'Year-Of-Publication_clean', 'Year_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 209538:\n",
      "DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\\";Michael Teitelbaum\"\n",
      "\n",
      "Index 220731:\n",
      "Peuple du ciel, suivi de 'Les Bergers\\\";Jean-Marie Gustave Le ClÃ?Â©zio\"\n",
      "\n",
      "Index 221678:\n",
      "DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\\";James Buckley\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209538</th>\n",
       "      <td>078946697X</td>\n",
       "      <td>DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\\";Michael Teitelb...</td>\n",
       "      <td>2000</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220731</th>\n",
       "      <td>2070426769</td>\n",
       "      <td>Peuple du ciel, suivi de 'Les Bergers\\\";Jean-Marie Gustave Le ClÃ?Â©zio\"</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221678</th>\n",
       "      <td>0789466953</td>\n",
       "      <td>DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\\";Jam...</td>\n",
       "      <td>2000</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN  \\\n",
       "209538  078946697X   \n",
       "220731  2070426769   \n",
       "221678  0789466953   \n",
       "\n",
       "                                                                                                 Book-Title  \\\n",
       "209538  DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\\";Michael Teitelb...   \n",
       "220731                             Peuple du ciel, suivi de 'Les Bergers\\\";Jean-Marie Gustave Le ClÃ?Â©zio\"   \n",
       "221678  DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\\";Jam...   \n",
       "\n",
       "       Book-Author Year-Of-Publication  \\\n",
       "209538        2000   DK Publishing Inc   \n",
       "220731        2003           Gallimard   \n",
       "221678        2000   DK Publishing Inc   \n",
       "\n",
       "                                                           Publisher  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-S  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-M  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg   \n",
       "\n",
       "       Image-URL-L Year-Of-Publication_clean  Year_numeric  \n",
       "209538         NaN         DK Publishing Inc           NaN  \n",
       "220731         NaN                 Gallimard           NaN  \n",
       "221678         NaN         DK Publishing Inc           NaN  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_titles = df.loc[[209538, 220731, 221678], 'Book-Title']\n",
    "for idx, title in selected_titles.items():\n",
    "    print(f\"Index {idx}:\")\n",
    "    print(title)\n",
    "    print()\n",
    "\n",
    "df.loc[[209538, 220731, 221678]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Define a function to fix the fused Book-Title field.\n",
    "\n",
    "It expects **the problematic pattern '\";'** (an escaped quote followed by a semicolon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if \"Book-Author\" not in df.columns:\n",
    "    df.insert(2, \"Book-Author\", np.nan)\n",
    "\n",
    "def fix_title_and_author(fused_value):\n",
    "    if '\";' in fused_value:\n",
    "        # Split on the problematic pattern\n",
    "        parts = fused_value.split('\";')\n",
    "        # Remove extra quotes and whitespace from both parts\n",
    "        title = parts[0].replace('\"', '').strip()\n",
    "        author = parts[1].replace('\"', '').strip() if len(parts) > 1 else ''\n",
    "        return pd.Series([title, author])\n",
    "    else:\n",
    "        return pd.Series([fused_value, None])\n",
    "\n",
    "# Define the ISBNs of the rows to fix\n",
    "problematic_isbns = [\"078946697X\", \"2070426769\", \"0789466953\"]\n",
    "\n",
    "# Create a mask for the problematic rows\n",
    "mask = df['ISBN'].isin(problematic_isbns)\n",
    "\n",
    "for idx in df.loc[mask].index:\n",
    "    row = df.loc[idx].copy()\n",
    "    \n",
    "    # Apply the fix function\n",
    "    fixed_title, fixed_author = fix_title_and_author(row['Book-Title'])\n",
    "    lost_year = row['Book-Author']\n",
    "    \n",
    "    # Shift the remaining columns one position to the right:\n",
    "    df.at[idx, 'Year-Of-Publication'] = lost_year\n",
    "    df.at[idx, 'Publisher'] = row['Year-Of-Publication']\n",
    "    df.at[idx, 'Image-URL-S'] = row['Publisher']\n",
    "    df.at[idx, 'Image-URL-M'] = row['Image-URL-S']\n",
    "    df.at[idx, 'Image-URL-L'] = row['Image-URL-M']\n",
    "    df.at[idx, 'Year-Of-Publication_clean'] = row['Image-URL-L']\n",
    "    # Convert the value to numeric (non-numeric values will become NaN)\n",
    "    df.at[idx, 'Year_numeric'] = pd.to_numeric(row['Year-Of-Publication_clean'], errors='coerce')\n",
    "\n",
    "    # Update\n",
    "    df.at[idx, 'Book-Title'] = fixed_title\n",
    "    df.at[idx, 'Book-Author'] = fixed_author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, when you print the problematic rows, the values remain in their proper columns.\n",
    "df.drop(columns=['Year-Of-Publication_clean', 'Year_numeric'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209538</th>\n",
       "      <td>078946697X</td>\n",
       "      <td>DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\</td>\n",
       "      <td>Michael Teitelbaum</td>\n",
       "      <td>2000</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220731</th>\n",
       "      <td>2070426769</td>\n",
       "      <td>Peuple du ciel, suivi de 'Les Bergers\\</td>\n",
       "      <td>Jean-Marie Gustave Le ClÃ?Â©zio</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gallimard</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221678</th>\n",
       "      <td>0789466953</td>\n",
       "      <td>DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\</td>\n",
       "      <td>James Buckley</td>\n",
       "      <td>2000</td>\n",
       "      <td>DK Publishing Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN  \\\n",
       "209538  078946697X   \n",
       "220731  2070426769   \n",
       "221678  0789466953   \n",
       "\n",
       "                                                                                         Book-Title  \\\n",
       "209538              DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\\   \n",
       "220731                                                       Peuple du ciel, suivi de 'Les Bergers\\   \n",
       "221678  DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)\\   \n",
       "\n",
       "                            Book-Author Year-Of-Publication  \\\n",
       "209538               Michael Teitelbaum                2000   \n",
       "220731  Jean-Marie Gustave Le ClÃ?Â©zio                2003   \n",
       "221678                    James Buckley                2000   \n",
       "\n",
       "                Publisher  \\\n",
       "209538  DK Publishing Inc   \n",
       "220731          Gallimard   \n",
       "221678  DK Publishing Inc   \n",
       "\n",
       "                                                         Image-URL-S  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.THUMBZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.THUMBZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-M  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.MZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.MZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-L  \\\n",
       "209538  http://images.amazon.com/images/P/078946697X.01.LZZZZZZZ.jpg   \n",
       "220731  http://images.amazon.com/images/P/2070426769.01.LZZZZZZZ.jpg   \n",
       "221678  http://images.amazon.com/images/P/0789466953.01.LZZZZZZZ.jpg   \n",
       "\n",
       "       Year-Of-Publication_clean  Year_numeric  \n",
       "209538                      2000        2000.0  \n",
       "220731                      2003        2003.0  \n",
       "221678                      2000        2000.0  "
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Check for NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 271360 entries, 0 to 271359\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   ISBN                       271360 non-null  object \n",
      " 1   Book-Title                 271360 non-null  object \n",
      " 2   Book-Author                271358 non-null  object \n",
      " 3   Year-Of-Publication        271360 non-null  object \n",
      " 4   Publisher                  271358 non-null  object \n",
      " 5   Image-URL-S                271360 non-null  object \n",
      " 6   Image-URL-M                271360 non-null  object \n",
      " 7   Image-URL-L                271360 non-null  object \n",
      " 8   Year-Of-Publication_clean  271360 non-null  object \n",
      " 9   Year_numeric               266742 non-null  float64\n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 20.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN                            0\n",
       "Book-Title                      0\n",
       "Book-Author                     2\n",
       "Year-Of-Publication             0\n",
       "Publisher                       2\n",
       "Image-URL-S                     0\n",
       "Image-URL-M                     0\n",
       "Image-URL-L                     0\n",
       "Year-Of-Publication_clean       0\n",
       "Year_numeric                 4618\n",
       "dtype: int64"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118033</th>\n",
       "      <td>0751352497</td>\n",
       "      <td>A+ Quiz Masters:01 Earth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1999</td>\n",
       "      <td>Dorling Kindersley</td>\n",
       "      <td>http://images.amazon.com/images/P/0751352497.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0751352497.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0751352497.01.LZZZZZZZ.jpg</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187689</th>\n",
       "      <td>9627982032</td>\n",
       "      <td>The Credit Suisse Guide to Managing Your Personal Wealth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>Edinburgh Financial Publishing</td>\n",
       "      <td>http://images.amazon.com/images/P/9627982032.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/9627982032.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/9627982032.01.LZZZZZZZ.jpg</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                                Book-Title  \\\n",
       "118033  0751352497                                  A+ Quiz Masters:01 Earth   \n",
       "187689  9627982032  The Credit Suisse Guide to Managing Your Personal Wealth   \n",
       "\n",
       "       Book-Author Year-Of-Publication                       Publisher  \\\n",
       "118033         NaN                1999              Dorling Kindersley   \n",
       "187689         NaN                1995  Edinburgh Financial Publishing   \n",
       "\n",
       "                                                         Image-URL-S  \\\n",
       "118033  http://images.amazon.com/images/P/0751352497.01.THUMBZZZ.jpg   \n",
       "187689  http://images.amazon.com/images/P/9627982032.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-M  \\\n",
       "118033  http://images.amazon.com/images/P/0751352497.01.MZZZZZZZ.jpg   \n",
       "187689  http://images.amazon.com/images/P/9627982032.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-L  \\\n",
       "118033  http://images.amazon.com/images/P/0751352497.01.LZZZZZZZ.jpg   \n",
       "187689  http://images.amazon.com/images/P/9627982032.01.LZZZZZZZ.jpg   \n",
       "\n",
       "       Year-Of-Publication_clean  Year_numeric  \n",
       "118033                      1999        1999.0  \n",
       "187689                      1995        1995.0  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Book-Author\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128890</th>\n",
       "      <td>193169656X</td>\n",
       "      <td>Tyrant Moon</td>\n",
       "      <td>Elaine Corvidae</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.amazon.com/images/P/193169656X.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/193169656X.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/193169656X.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129037</th>\n",
       "      <td>1931696993</td>\n",
       "      <td>Finders Keepers</td>\n",
       "      <td>Linnea Sinclair</td>\n",
       "      <td>2001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.amazon.com/images/P/1931696993.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/1931696993.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/1931696993.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN       Book-Title      Book-Author Year-Of-Publication  \\\n",
       "128890  193169656X      Tyrant Moon  Elaine Corvidae                2002   \n",
       "129037  1931696993  Finders Keepers  Linnea Sinclair                2001   \n",
       "\n",
       "       Publisher  \\\n",
       "128890       NaN   \n",
       "129037       NaN   \n",
       "\n",
       "                                                         Image-URL-S  \\\n",
       "128890  http://images.amazon.com/images/P/193169656X.01.THUMBZZZ.jpg   \n",
       "129037  http://images.amazon.com/images/P/1931696993.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-M  \\\n",
       "128890  http://images.amazon.com/images/P/193169656X.01.MZZZZZZZ.jpg   \n",
       "129037  http://images.amazon.com/images/P/1931696993.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                         Image-URL-L  \\\n",
       "128890  http://images.amazon.com/images/P/193169656X.01.LZZZZZZZ.jpg   \n",
       "129037  http://images.amazon.com/images/P/1931696993.01.LZZZZZZZ.jpg   \n",
       "\n",
       "       Year-Of-Publication_clean  Year_numeric  \n",
       "128890                      2002        2002.0  \n",
       "129037                      2001        2001.0  "
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Publisher\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NovelBooks, Inc.\n",
    "- CreateSpace Independent Publishing Platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_corrections = {\n",
    "    \"193169656X\": \"NovelBooks, Inc.\",\n",
    "    \"1931696993\": \"CreateSpace Independent Publishing Platform.\"\n",
    "}\n",
    "\n",
    "# Update the Publisher column for the specified ISBNs\n",
    "for isbn, new_publisher in publisher_corrections.items():\n",
    "    df.loc[df['ISBN'] == isbn, 'Publisher'] = new_publisher\n",
    "\n",
    "# Verify the update by displaying the fixed entries\n",
    "fixed_entries = df.loc[df['ISBN'].isin(publisher_corrections.keys()), \n",
    "                       ['ISBN', 'Book-Title', 'Book-Author', 'Publisher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128890</th>\n",
       "      <td>193169656X</td>\n",
       "      <td>Tyrant Moon</td>\n",
       "      <td>Elaine Corvidae</td>\n",
       "      <td>NovelBooks, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129037</th>\n",
       "      <td>1931696993</td>\n",
       "      <td>Finders Keepers</td>\n",
       "      <td>Linnea Sinclair</td>\n",
       "      <td>CreateSpace Independent Publishing Platform.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN       Book-Title      Book-Author  \\\n",
       "128890  193169656X      Tyrant Moon  Elaine Corvidae   \n",
       "129037  1931696993  Finders Keepers  Linnea Sinclair   \n",
       "\n",
       "                                           Publisher  \n",
       "128890                              NovelBooks, Inc.  \n",
       "129037  CreateSpace Independent Publishing Platform.  "
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **RapidFuzz to Harmonise categoricals - String Similarity Algorithm**  \n",
    "\n",
    "**\"Publisher\" column**\n",
    "\n",
    "Seems to be alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'Oxford University Press'}\n",
      "{'HarperFlamingo Canada'}\n",
      "{'Harper Perennial', 'Harperperennial', 'HarperPerennial'}\n",
      "{'Farrar, Straus, Giroux', 'Farrar, Straus Giroux', 'Farrar, Straus and Giroux', 'Farrar Straus Giroux'}\n",
      "{'W. W. Norton &amp; Company', 'W.W. Norton &amp; Company Ltd', 'W.W. Norton &amp; Company'}\n",
      "{'Putnam Pub Group'}\n",
      "{'Berkley Publishing Group', 'The Berkley Publishing Group', 'Berkeley Publishing Group'}\n",
      "{'Audioworks'}\n",
      "{'Randon House', 'Random House'}\n",
      "{'Scribner', 'Scribners', \"Scribner's\"}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'Publisher', threshold=90, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:10]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized publisher names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "      <th>publisher_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Oxford University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN           Book-Title           Book-Author Year-Of-Publication  \\\n",
       "0  0195153448  Classical Mythology    Mark P. O. Morford                2002   \n",
       "1  0002005018         Clara Callan  Richard Bruce Wright                2001   \n",
       "\n",
       "                 Publisher  \\\n",
       "0  Oxford University Press   \n",
       "1    HarperFlamingo Canada   \n",
       "\n",
       "                                                    Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                    Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                    Image-URL-L  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg   \n",
       "\n",
       "  Year-Of-Publication_clean  Year_numeric       publisher_standard  \n",
       "0                      2002        2002.0  Oxford University Press  \n",
       "1                      2001        2001.0    HarperFlamingo Canada  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency for each publisher\n",
    "freq = df['Publisher'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    # Choose the publisher with the highest frequency in this group\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "# If it alone, it keeps its name obvi\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "\n",
    "# standardized publisher column in the DataFrame using the mapping, add not removing \n",
    "df['publisher_standard'] = df['Publisher'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized publisher names added to DataFrame:\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Book_Author\"** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Book-Author\n",
       "Agatha Christie        632\n",
       "William Shakespeare    567\n",
       "Stephen King           524\n",
       "Ann M. Martin          423\n",
       "Carolyn Keene          373\n",
       "                      ... \n",
       "Peter H. Irons           2\n",
       "Don G. Campbell          2\n",
       "Karin Goodwin            2\n",
       "John Rigby Hale          2\n",
       "Bob Harris               2\n",
       "Name: count, Length: 33629, dtype: int64"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the counts for each author\n",
    "author_counts = df['Book-Author'].value_counts()\n",
    "\n",
    "# Filter for authors that appear more than once\n",
    "duplicates = author_counts[author_counts > 1]\n",
    "duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Year-Of-Publication_clean</th>\n",
       "      <th>Year_numeric</th>\n",
       "      <th>publisher_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Oxford University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg</td>\n",
       "      <td>1991</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN            Book-Title           Book-Author Year-Of-Publication  \\\n",
       "0  0195153448   Classical Mythology    Mark P. O. Morford                2002   \n",
       "1  0002005018          Clara Callan  Richard Bruce Wright                2001   \n",
       "2  0060973129  Decision in Normandy          Carlo D'Este                1991   \n",
       "\n",
       "                 Publisher  \\\n",
       "0  Oxford University Press   \n",
       "1    HarperFlamingo Canada   \n",
       "2          HarperPerennial   \n",
       "\n",
       "                                                    Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg   \n",
       "2  http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                                    Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg   \n",
       "2  http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg   \n",
       "\n",
       "                                                    Image-URL-L  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg   \n",
       "1  http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg   \n",
       "2  http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg   \n",
       "\n",
       "  Year-Of-Publication_clean  Year_numeric       publisher_standard  \n",
       "0                      2002        2002.0  Oxford University Press  \n",
       "1                      2001        2001.0    HarperFlamingo Canada  \n",
       "2                      1991        1991.0          HarperPerennial  "
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ISBN Column**\n",
    "\n",
    "Splicing isbn into two new columns, \"isbn_10\" and \"isbn_13\", by checking the length of the values in the existing isbn column. Only string values with exactly 10 or 13 characters are kept in their respective columns; others are set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>isbn_10</th>\n",
       "      <th>isbn_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>0195153448</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>0060973129</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>0374157065</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>0393045218</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN     isbn_10 isbn_13\n",
       "0  0195153448  0195153448     NaN\n",
       "1  0002005018  0002005018     NaN\n",
       "2  0060973129  0060973129     NaN\n",
       "3  0374157065  0374157065     NaN\n",
       "4  0393045218  0393045218     NaN"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ISBN'] = df['ISBN'].astype(str).str.strip()\n",
    "\n",
    "df['isbn_10'] = df['ISBN'].apply(lambda x: x if len(x) == 10 else np.nan)\n",
    "df['isbn_13'] = df['ISBN'].apply(lambda x: x if len(x) == 13 else np.nan)\n",
    "\n",
    "df[['ISBN', 'isbn_10', 'isbn_13']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher',\n",
       "       'Image-URL-S', 'Image-URL-M', 'Image-URL-L',\n",
       "       'Year-Of-Publication_clean', 'Year_numeric', 'publisher_standard',\n",
       "       'isbn_10', 'isbn_13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['isbn', 'book_title', 'book_author', 'year_of_publication', 'publisher_raw', 'image_url_s', 'image_url_m', 'image_url_l', 'publication_year', 'year_numeric', 'publisher', 'isbn_10', 'isbn_13']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rename_mapping = {\n",
    "    'ISBN': 'isbn',\n",
    "    'Book-Title': 'book_title',\n",
    "    'Book-Author': 'book_author',\n",
    "    'Year-Of-Publication': 'year_of_publication',\n",
    "    'Publisher': 'publisher_raw',\n",
    "    'Image-URL-S': 'image_url_s',\n",
    "    'Image-URL-M': 'image_url_m',\n",
    "    'Image-URL-L': 'image_url_l',\n",
    "    'Year-Of-Publication_clean': 'publication_year',\n",
    "    'Year_numeric': 'year_numeric',\n",
    "    'publisher_standard': 'publisher',\n",
    "    'isbn_10': 'isbn_10',\n",
    "    'isbn_13': 'isbn_13'\n",
    "}\n",
    "\n",
    "# Apply the renaming mapping to the DataFrame\n",
    "df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Display the new standardized column names\n",
    "df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/4_bookcrossing/book_crossing_Books_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) ii) BookCrossing Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating\n",
       "0   276725  034545104X            0\n",
       "1   276726  0155061224            5\n",
       "2   276727  0446520802            0\n",
       "3   276729  052165615X            3\n",
       "4   276729  0521795028            6"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Datasets/4_bookcrossing/book_crossing_Ratings.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**\n",
    "There are wrong ISBNs neither 10 nor 13 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1149780\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**\n",
    "\n",
    "By these we can tell there are a few ibsn_13 but the majority is isbn_10 (10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "User-ID: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "ISBN: {'min_length': 8, 'max_length': 14, 'mean_length': 10.002014298387518, 'median_length': 10.0}\n",
      "Book-Rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "User-ID        0.0\n",
      "ISBN           0.0\n",
      "Book-Rating    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'User-ID': 105283, 'ISBN': 340556, 'Book-Rating': 11}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'User-ID': 9.156795212997269,\n",
       " 'ISBN': 29.619231505157508,\n",
       " 'Book-Rating': 0.000956704760910783}"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No NaN/ nule entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   User-ID      1149780 non-null  int64 \n",
      " 1   ISBN         1149780 non-null  object\n",
      " 2   Book-Rating  1149780 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Empty DataFrame\n",
      "Columns: [User-ID, ISBN, Book-Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check how many duplicate rows exist\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# If you want to view all duplicate rows (including all occurrences), use:\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Bad ISBNs** not 13 nor 10 digits\n",
    "\n",
    "8677 bad rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with ISBN not 10 or 13 characters:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>342310538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>342662429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>N3453124715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>20103389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>344242529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149238</th>\n",
       "      <td>033031582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149358</th>\n",
       "      <td>00969754916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149428</th>\n",
       "      <td>15655122046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149439</th>\n",
       "      <td>55305663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149779</th>\n",
       "      <td>05162443314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8677 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ISBN\n",
       "9          342310538\n",
       "40         342662429\n",
       "55       N3453124715\n",
       "247         20103389\n",
       "291        344242529\n",
       "...              ...\n",
       "1149238    033031582\n",
       "1149358  00969754916\n",
       "1149428  15655122046\n",
       "1149439     55305663\n",
       "1149779  05162443314\n",
       "\n",
       "[8677 rows x 1 columns]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where the ISBN length is not equal to 10 or 13\n",
    "invalid_isbn_entries = df[~df['ISBN'].apply(lambda x: len(x) in [10, 13])]\n",
    "\n",
    "print(\"Entries with ISBN not 10 or 13 characters:\")\n",
    "invalid_isbn_entries[['ISBN']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) iii) BookCrossing Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Datasets/4_bookcrossing/book_crossing_Users.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Age == 0 ? (should not be a thing)**\n",
    "\n",
    "It is odd how **416 users are 0 years old**, but we'll keep it this way (wouldnt be exactly data cleaning). Might be interesting in data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>220</td>\n",
       "      <td>bogota, bogota, colombia</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>indianapolis, indiana, usa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>562</td>\n",
       "      <td>adfdaf, australian capital territory, albania</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>1461</td>\n",
       "      <td>kolding, none, denmark</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>1910</td>\n",
       "      <td>vigo, galicia, spain</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      User-ID                                       Location  Age\n",
       "219       220                       bogota, bogota, colombia  0.0\n",
       "469       470                     indianapolis, indiana, usa  0.0\n",
       "561       562  adfdaf, australian capital territory, albania  0.0\n",
       "1460     1461                         kolding, none, denmark  0.0\n",
       "1909     1910                           vigo, galicia, spain  0.0"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Age\"] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>age_raw</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>washington, dc, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>timmins, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location  age_raw  age\n",
       "0        1                  nyc, new york, usa      NaN  NaN\n",
       "2        3     moscow, yukon territory, russia      NaN  NaN\n",
       "4        5  farnborough, hants, united kingdom      NaN  NaN\n",
       "6        7                 washington, dc, usa      NaN  NaN\n",
       "7        8            timmins, ontario, canada      NaN  NaN"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={\"Age\": \"age_raw\"}, inplace=True)\n",
    "df[\"age\"] = df[\"age_raw\"].replace(0, np.nan)\n",
    "df[df[\"age\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>age_raw</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>santa monica, california, usa</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>washington, dc, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>timmins, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>germantown, tennessee, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>albacete, wisconsin, spain</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>melbourne, victoria, australia</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>fort bragg, california, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>barcelona, barcelona, spain</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>mediapolis, iowa, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>calgary, alberta, canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>albuquerque, new mexico, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>chesapeake, virginia, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>rio de janeiro, rio de janeiro, brazil</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>weston, ,</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>langhorne, pennsylvania, usa</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User-ID                                Location  age_raw   age\n",
       "0         1                      nyc, new york, usa      NaN   NaN\n",
       "1         2               stockton, california, usa     18.0  18.0\n",
       "2         3         moscow, yukon territory, russia      NaN   NaN\n",
       "3         4               porto, v.n.gaia, portugal     17.0  17.0\n",
       "4         5      farnborough, hants, united kingdom      NaN   NaN\n",
       "5         6           santa monica, california, usa     61.0  61.0\n",
       "6         7                     washington, dc, usa      NaN   NaN\n",
       "7         8                timmins, ontario, canada      NaN   NaN\n",
       "8         9              germantown, tennessee, usa      NaN   NaN\n",
       "9        10              albacete, wisconsin, spain     26.0  26.0\n",
       "10       11          melbourne, victoria, australia     14.0  14.0\n",
       "11       12             fort bragg, california, usa      NaN   NaN\n",
       "12       13             barcelona, barcelona, spain     26.0  26.0\n",
       "13       14                   mediapolis, iowa, usa      NaN   NaN\n",
       "14       15                calgary, alberta, canada      NaN   NaN\n",
       "15       16            albuquerque, new mexico, usa      NaN   NaN\n",
       "16       17               chesapeake, virginia, usa      NaN   NaN\n",
       "17       18  rio de janeiro, rio de janeiro, brazil     25.0  25.0\n",
       "18       19                               weston, ,     14.0  14.0\n",
       "19       20            langhorne, pennsylvania, usa     19.0  19.0"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 278858\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "User-ID: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "age_raw: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "age: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "City: {'min_length': 0, 'max_length': 64, 'mean_length': 8.494828909337368, 'median_length': 8.0}\n",
      "State: {'min_length': 0, 'max_length': 80, 'mean_length': 8.60337377222017, 'median_length': 8.0}\n",
      "Country: {'min_length': 0, 'max_length': 64, 'mean_length': 5.372392919643113, 'median_length': 3.0}\n",
      "LocationQuery: {'min_length': 4, 'max_length': 105, 'mean_length': 26.470504701317516, 'median_length': 25.0}\n",
      "city_standard: {'min_length': 0, 'max_length': 64, 'mean_length': 8.499476436035545, 'median_length': 8.0}\n",
      "state_standard: {'min_length': 0, 'max_length': 80, 'mean_length': 8.603940370871092, 'median_length': 8.0}\n",
      "country_standard: {'min_length': 0, 'max_length': 64, 'mean_length': 5.371603981983533, 'median_length': 3.0}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "User-ID              0.000000\n",
      "age_raw             39.719857\n",
      "age                 39.869037\n",
      "City                 0.043750\n",
      "State                1.388520\n",
      "Country              1.636317\n",
      "LocationQuery        0.000000\n",
      "city_standard        0.043750\n",
      "state_standard       1.388520\n",
      "country_standard     1.636317\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'User-ID': 278858, 'age_raw': 165, 'age': 164, 'City': 32758, 'State': 6256, 'Country': 1258, 'LocationQuery': 57327, 'city_standard': 30566, 'state_standard': 5777, 'country_standard': 1175}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'User-ID': 100.0,\n",
       " 'age_raw': 0.05916990009252021,\n",
       " 'age': 0.05881129463741402,\n",
       " 'City': 11.747197498368344,\n",
       " 'State': 2.2434357271442815,\n",
       " 'Country': 0.45112566252357833,\n",
       " 'LocationQuery': 20.557774924872156,\n",
       " 'city_standard': 10.96113434077559,\n",
       " 'state_standard': 2.0716637141484195,\n",
       " 'country_standard': 0.42136140974976516}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1070\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "index: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Publishing Year: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Book Name: {'min_length': 4, 'max_length': 124, 'mean_length': 22.134670487106018, 'median_length': 18.0}\n",
      "Author: {'min_length': 7, 'max_length': 257, 'mean_length': 18.828971962616823, 'median_length': 15.0}\n",
      "language_code: {'min_length': 2, 'max_length': 5, 'mean_length': 3.5319567354965584, 'median_length': 3.0}\n",
      "Author_Rating: {'min_length': 6, 'max_length': 12, 'mean_length': 10.519626168224299, 'median_length': 12.0}\n",
      "Book_average_rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Book_ratings_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "genre: {'min_length': 7, 'max_length': 13, 'mean_length': 12.102803738317757, 'median_length': 13.0}\n",
      "gross sales: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "publisher revenue: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "sale price: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "sales rank: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Publisher : {'min_length': 9, 'max_length': 36, 'mean_length': 26.148598130841123, 'median_length': 30.0}\n",
      "units sold: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "index                  0.000000\n",
      "Publishing Year        0.093458\n",
      "Book Name              2.149533\n",
      "Author                 0.000000\n",
      "language_code          4.953271\n",
      "Author_Rating          0.000000\n",
      "Book_average_rating    0.000000\n",
      "Book_ratings_count     0.000000\n",
      "genre                  0.000000\n",
      "gross sales            0.000000\n",
      "publisher revenue      0.000000\n",
      "sale price             0.000000\n",
      "sales rank             0.000000\n",
      "Publisher              0.000000\n",
      "units sold             0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'User-ID': 278858, 'age_raw': 165, 'age': 164, 'City': 32758, 'State': 6256, 'Country': 1258, 'LocationQuery': 57327, 'city_standard': 30566, 'state_standard': 5777, 'country_standard': 1175}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'User-ID': 100.0,\n",
       " 'age_raw': 0.05916990009252021,\n",
       " 'age': 0.05881129463741402,\n",
       " 'City': 11.747197498368344,\n",
       " 'State': 2.2434357271442815,\n",
       " 'Country': 0.45112566252357833,\n",
       " 'LocationQuery': 20.557774924872156,\n",
       " 'city_standard': 10.96113434077559,\n",
       " 'state_standard': 2.0716637141484195,\n",
       " 'country_standard': 0.42136140974976516}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **NaN values**\n",
    "\n",
    "All entries have **\"User-ID\"** (it's just an incrementation with no real meaning outside connecting information to the review table) and **\"Location\"**.\n",
    "\n",
    "Only *60.3%* of the dataset has a value in **\"Age\"** (the rest is NaN \"110 762 entries\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278858 entries, 0 to 278857\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   User-ID   278858 non-null  int64  \n",
      " 1   Location  278858 non-null  object \n",
      " 2   age_raw   168096 non-null  float64\n",
      " 3   age       167680 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse Location Data\n",
    "\n",
    "Location data is currently messy. We will split it in 3 columns ('City', 'State', 'Country') so it is more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Location' not found in the DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>age_raw</th>\n",
       "      <th>age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  age_raw   age         City            State         Country\n",
       "0        1      NaN   NaN          nyc         new york             usa\n",
       "1        2     18.0  18.0     stockton       california             usa\n",
       "2        3      NaN   NaN       moscow  yukon territory          russia\n",
       "3        4     17.0  17.0        porto         v.n.gaia        portugal\n",
       "4        5      NaN   NaN  farnborough            hants  united kingdom"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the 'Location' column exists before attempting to split it\n",
    "if 'Location' in df.columns:\n",
    "    # Split the 'Location' column into 'City', 'State', and 'Country'\n",
    "    df[['City', 'State', 'Country']] = df['Location'].str.split(',', n=2, expand=True)\n",
    "    \n",
    "    # Remove any leading/trailing whitespace from the new columns\n",
    "    df['City'] = df['City'].str.strip()\n",
    "    df['State'] = df['State'].str.strip()\n",
    "    df['Country'] = df['Country'].str.strip()\n",
    "    \n",
    "    # Optionally, drop the original 'Location' column if it's no longer needed\n",
    "    df.drop(columns=['Location'], inplace=True)\n",
    "else:\n",
    "    print(\"Column 'Location' not found in the DataFrame.\")\n",
    "\n",
    "# Verify the new structure by printing a sample\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THERE ARE MISSSPEALINGS THAT SHOULD BE STANDARDIZED!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Harmonize Categoricals \"City\"**\n",
    "\n",
    "Played with the threshold to try to improve grouping without introducing errors in the matching of different Cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'nyc'}\n",
      "{'stockton'}\n",
      "{'moscow'}\n",
      "{'porto'}\n",
      "{'farnborough'}\n",
      "{'santa monica'}\n",
      "{'wahington', 'washinton', 'washngton', 'ashington', 'washington', 'wasington'}\n",
      "{'timmins'}\n",
      "{'germantown', 'germanton'}\n",
      "{'albacete'}\n",
      "{'melbourn', 'melboourne', 'melbouurne', 'melbour', 'melboure', 'melbourne', 'melborne'}\n",
      "{'fort bragg'}\n",
      "{'barcelna', 'barcelona', 'brcelona', 'barcelona.', 'bercelona', 'barcelone', 'bracelona', 'barceona', 'barcelon', 'bacelona'}\n",
      "{'mediapolis'}\n",
      "{'calgary', 'calagary'}\n",
      "{'albuerque', 'albuqueruqe', 'albuquqerque', 'albuquerque', 'albuqueque', 'albquerque', 'albuqeurque', 'aluqueruqe', 'albuqerque', 'albuqureque', 'albuqurque', 'al;buquerque', 'alburquerque', 'albuquerqe', 'albququerque'}\n",
      "{'chesapeake', 'chesapeke', 'cheapeake'}\n",
      "{'rio de janeiro'}\n",
      "{'weston'}\n",
      "{'langhorne'}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'City', threshold=93, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:20]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adding it has a new column to not lose info, cuz matching might be incorrect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized City names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>age_raw</th>\n",
       "      <th>age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationQuery</th>\n",
       "      <th>city_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>nyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>stockton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>porto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>farnborough</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  age_raw   age         City            State         Country  \\\n",
       "0        1      NaN   NaN          nyc         new york             usa   \n",
       "1        2     18.0  18.0     stockton       california             usa   \n",
       "2        3      NaN   NaN       moscow  yukon territory          russia   \n",
       "3        4     17.0  17.0        porto         v.n.gaia        portugal   \n",
       "4        5      NaN   NaN  farnborough            hants  united kingdom   \n",
       "\n",
       "                        LocationQuery city_standard  \n",
       "0                  nyc, new york, usa           nyc  \n",
       "1           stockton, california, usa      stockton  \n",
       "2     moscow, yukon territory, russia        moscow  \n",
       "3           porto, v.n.gaia, portugal         porto  \n",
       "4  farnborough, hants, united kingdom   farnborough  "
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df['City'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "        \n",
    "df['city_standard'] = df['City'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized City names added to DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Harmonize Categoricals \"State\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'news york', 'new  york', 'new york'}\n",
      "{'califorrnia', 'california', 'califonia'}\n",
      "{'yukon territory'}\n",
      "{'v.n.gaia', 'v.n. gaia'}\n",
      "{'hants'}\n",
      "{'dc'}\n",
      "{'ontario'}\n",
      "{'tennessee', 'tenessee'}\n",
      "{'wisconsin'}\n",
      "{'victoria'}\n",
      "{'barcelona.', 'barcelona'}\n",
      "{'iowa'}\n",
      "{'alberta'}\n",
      "{'new mexico'}\n",
      "{'virguinia', 'virginia'}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'State', threshold=94, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:15]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized State names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>age_raw</th>\n",
       "      <th>age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationQuery</th>\n",
       "      <th>city_standard</th>\n",
       "      <th>state_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  age_raw   age         City            State         Country  \\\n",
       "0        1      NaN   NaN          nyc         new york             usa   \n",
       "1        2     18.0  18.0     stockton       california             usa   \n",
       "2        3      NaN   NaN       moscow  yukon territory          russia   \n",
       "3        4     17.0  17.0        porto         v.n.gaia        portugal   \n",
       "4        5      NaN   NaN  farnborough            hants  united kingdom   \n",
       "\n",
       "                        LocationQuery city_standard   state_standard  \n",
       "0                  nyc, new york, usa           nyc         new york  \n",
       "1           stockton, california, usa      stockton       california  \n",
       "2     moscow, yukon territory, russia        moscow  yukon territory  \n",
       "3           porto, v.n.gaia, portugal         porto         v.n.gaia  \n",
       "4  farnborough, hants, united kingdom   farnborough            hants  "
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df['State'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "        \n",
    "df['state_standard'] = df['State'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized State names added to DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Harmonize Categoricals \"Country\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'usa'}\n",
      "{'russia'}\n",
      "{'portugal\"', 'portugal'}\n",
      "{'united kingdom.', 'united kingdom', 'united kindgdom', 'united kingdom\"'}\n",
      "{'canada'}\n",
      "{'spain'}\n",
      "{'autralia', 'australia\"', 'australia'}\n",
      "{'brazil'}\n",
      "{''}\n",
      "{'germany'}\n",
      "{'mexico'}\n",
      "{'china'}\n",
      "{'italy'}\n",
      "{'distrito federal'}\n",
      "{'france'}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'Country', threshold=94, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:15]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized Country names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>age_raw</th>\n",
       "      <th>age</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationQuery</th>\n",
       "      <th>city_standard</th>\n",
       "      <th>state_standard</th>\n",
       "      <th>country_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  age_raw   age         City            State         Country  \\\n",
       "0        1      NaN   NaN          nyc         new york             usa   \n",
       "1        2     18.0  18.0     stockton       california             usa   \n",
       "2        3      NaN   NaN       moscow  yukon territory          russia   \n",
       "3        4     17.0  17.0        porto         v.n.gaia        portugal   \n",
       "4        5      NaN   NaN  farnborough            hants  united kingdom   \n",
       "\n",
       "                        LocationQuery city_standard   state_standard  \\\n",
       "0                  nyc, new york, usa           nyc         new york   \n",
       "1           stockton, california, usa      stockton       california   \n",
       "2     moscow, yukon territory, russia        moscow  yukon territory   \n",
       "3           porto, v.n.gaia, portugal         porto         v.n.gaia   \n",
       "4  farnborough, hants, united kingdom   farnborough            hants   \n",
       "\n",
       "  country_standard  \n",
       "0              usa  \n",
       "1              usa  \n",
       "2           russia  \n",
       "3         portugal  \n",
       "4   united kingdom  "
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df['Country'].value_counts().to_dict()\n",
    "\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "        \n",
    "df['country_standard'] = df['Country'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized Country names added to DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file (without writing the index)\n",
    "df.to_csv(\"./Datasets/4_bookcrossing/book_crossing_Users_Loc.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Datasets/4_bookcrossing/book_crossing_Users_Loc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>city_standard</th>\n",
       "      <th>state_standard</th>\n",
       "      <th>country_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nyc</td>\n",
       "      <td>New York</td>\n",
       "      <td>Usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Stockton</td>\n",
       "      <td>California</td>\n",
       "      <td>Usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Yukon Territory</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Porto</td>\n",
       "      <td>V.N.Gaia</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Farnborough</td>\n",
       "      <td>Hants</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278853</th>\n",
       "      <td>278854</td>\n",
       "      <td>portland, oregon, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Portland</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278854</th>\n",
       "      <td>278855</td>\n",
       "      <td>tacoma, washington, united kingdom</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>Washington</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278855</th>\n",
       "      <td>278856</td>\n",
       "      <td>brampton, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brampton</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278856</th>\n",
       "      <td>278857</td>\n",
       "      <td>knoxville, tennessee, usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Knoxville</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278857</th>\n",
       "      <td>278858</td>\n",
       "      <td>dublin, n/a, ireland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ireland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278858 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User-ID                            Location   Age city_standard  \\\n",
       "0             1                  nyc, new york, usa   NaN           Nyc   \n",
       "1             2           stockton, california, usa  18.0      Stockton   \n",
       "2             3     moscow, yukon territory, russia   NaN        Moscow   \n",
       "3             4           porto, v.n.gaia, portugal  17.0         Porto   \n",
       "4             5  farnborough, hants, united kingdom   NaN   Farnborough   \n",
       "...         ...                                 ...   ...           ...   \n",
       "278853   278854               portland, oregon, usa   NaN      Portland   \n",
       "278854   278855  tacoma, washington, united kingdom  50.0        Tacoma   \n",
       "278855   278856           brampton, ontario, canada   NaN      Brampton   \n",
       "278856   278857           knoxville, tennessee, usa   NaN     Knoxville   \n",
       "278857   278858                dublin, n/a, ireland   NaN        Dublin   \n",
       "\n",
       "         state_standard country_standard  \n",
       "0              New York              Usa  \n",
       "1            California              Usa  \n",
       "2       Yukon Territory           Russia  \n",
       "3              V.N.Gaia         Portugal  \n",
       "4                 Hants   United Kingdom  \n",
       "...                 ...              ...  \n",
       "278853           Oregon              Usa  \n",
       "278854       Washington   United Kingdom  \n",
       "278855          Ontario           Canada  \n",
       "278856        Tennessee              Usa  \n",
       "278857              NaN          Ireland  \n",
       "\n",
       "[278858 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_standard</th>\n",
       "      <th>country_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278668</th>\n",
       "      <td>fl</td>\n",
       "      <td>hernando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278684</th>\n",
       "      <td>milano</td>\n",
       "      <td>argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278685</th>\n",
       "      <td>.</td>\n",
       "      <td>iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278810</th>\n",
       "      <td>±±¾©</td>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278840</th>\n",
       "      <td>denbighshire county</td>\n",
       "      <td>united kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10385 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             state_standard country_standard\n",
       "0                  new york              usa\n",
       "1                california              usa\n",
       "2           yukon territory           russia\n",
       "3                  v.n.gaia         portugal\n",
       "4                     hants   united kingdom\n",
       "...                     ...              ...\n",
       "278668                   fl         hernando\n",
       "278684               milano        argentina\n",
       "278685                    .             iran\n",
       "278810                 ±±¾©            china\n",
       "278840  denbighshire county   united kingdom\n",
       "\n",
       "[10385 rows x 2 columns]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"state_standard\", \"country_standard\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Google Maps API**\n",
    "\n",
    "(key not here cuz money can cost me)\n",
    "\n",
    "This code uses the Google Maps Geocoding API to validate and standardise country names that have already been processed through fuzzy matching (to have fewer unique values and therefore fewer API calls). \n",
    "\n",
    "The code defines a function that takes a country name and sends a query to the API. It then parses the returned address components to extract the officially recognised country name, which is considered more reliable. By caching these results for each unique country from the dataset, the code avoids redundant API calls and makes the process more efficient. \n",
    "\n",
    "Finally, it checks for any discrepancies between the original standardised country names and the API-validated ones, enabling further review and correction. This is important because it helps ensure that the country data in the dataset is as accurate with no misspellings and we can treat them as categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1175 unique countries.\n",
      "Mismatch: Original 'usa' corrected to 'United States'\n",
      "Mismatch: Original 'distrito federal' corrected to 'Brazil'\n",
      "Mismatch: Original 'u.a.e' corrected to 'United Arab Emirates'\n",
      "Mismatch: Original 'turkey' corrected to 'Türkiye'\n",
      "Mismatch: Original 'quit' corrected to 'Australia'\n",
      "Mismatch: Original 'denmark\"' corrected to 'Denmark'\n",
      "Mismatch: Original 'españa' corrected to 'Spain'\n",
      "Mismatch: Original 'london' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'victoria, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'british columbia, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'missouri, usa' corrected to 'United States'\n",
      "Mismatch: Original 'metropolitana, chile' corrected to 'Chile'\n",
      "Mismatch: Original 'clackamas' corrected to 'United States'\n",
      "Mismatch: Original 'spain\"' corrected to 'Spain'\n",
      "Mismatch: Original 'mansoura, egypt' corrected to 'Egypt'\n",
      "Mismatch: Original 'dc, usa' corrected to 'United States'\n",
      "Mismatch: Original 'basque country' corrected to 'Spain'\n",
      "Mismatch: Original 'england, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'israel\"' corrected to 'Israel'\n",
      "Mismatch: Original 'connecticut, usa' corrected to 'United States'\n",
      "Mismatch: Original 'gambia, the' corrected to 'The Gambia'\n",
      "Mismatch: Original 'iran\"' corrected to 'Iran'\n",
      "Mismatch: Original 'calabria' corrected to 'Italy'\n",
      "Mismatch: Original 'alderney' corrected to 'Guernsey'\n",
      "Mismatch: Original 'spain, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'nigeria\"' corrected to 'Nigeria'\n",
      "Mismatch: Original 'germany\"' corrected to 'Germany'\n",
      "Mismatch: Original 'cape verde' corrected to 'Cabo Verde'\n",
      "Mismatch: Original 'scotland' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'bayern, austria' corrected to 'Austria'\n",
      "Mismatch: Original 'hawaii, usa' corrected to 'United States'\n",
      "Mismatch: Original 'la argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'sind, pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'heard of it? :o), india' corrected to 'India'\n",
      "Mismatch: Original 'turkey\"' corrected to 'Türkiye'\n",
      "Mismatch: Original 'texas, usa' corrected to 'United States'\n",
      "Mismatch: Original 'morogoro,' corrected to 'Tanzania'\n",
      "Mismatch: Original 'monterrey' corrected to 'Mexico'\n",
      "Mismatch: Original 'jalisco, mexico, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'urugua' corrected to 'Uruguay'\n",
      "Mismatch: Original 'puerto rico, usa' corrected to 'Puerto Rico'\n",
      "Mismatch: Original ', costa rica' corrected to 'Costa Rica'\n",
      "Mismatch: Original 'voivodina, yugoslavia' corrected to 'Serbia'\n",
      "Mismatch: Original 'l`italia' corrected to 'United States'\n",
      "Mismatch: Original 'finland\"' corrected to 'Finland'\n",
      "Mismatch: Original 'francisco morazán, honduras' corrected to 'Honduras'\n",
      "Mismatch: Original 'netherlands, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'india\"' corrected to 'India'\n",
      "Mismatch: Original 'france\"' corrected to 'France'\n",
      "Mismatch: Original 'washington, usa' corrected to 'United States'\n",
      "Mismatch: Original 'central otago, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'czech republic' corrected to 'Czechia'\n",
      "Mismatch: Original 'romania\"' corrected to 'Romania'\n",
      "Mismatch: Original 'mexico, city, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'alabama, usa' corrected to 'United States'\n",
      "Mismatch: Original 'west yorkshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'norway\"' corrected to 'Norway'\n",
      "Mismatch: Original 'you listed stroud!), united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'alsace, bas rhin, france' corrected to 'France'\n",
      "Mismatch: Original 'the philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'maine, usa' corrected to 'United States'\n",
      "Mismatch: Original 'la france' corrected to 'France'\n",
      "Mismatch: Original 'bahamas' corrected to 'The Bahamas'\n",
      "Mismatch: Original 'canberra, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'granada, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'mississippi, usa' corrected to 'United States'\n",
      "Mismatch: Original 'maroc' corrected to 'Morocco'\n",
      "Mismatch: Original 'isreal' corrected to 'Israel'\n",
      "Mismatch: Original 'ontario, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'oklahoma, usa' corrected to 'United States'\n",
      "Mismatch: Original 'fernando de la mora' corrected to 'Paraguay'\n",
      "Mismatch: Original 'macedonia' corrected to 'North Macedonia'\n",
      "Mismatch: Original 'arizona, usa' corrected to 'United States'\n",
      "Mismatch: Original 'lombardia, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'brasil' corrected to 'Brazil'\n",
      "Mismatch: Original '19104' corrected to 'United States'\n",
      "Mismatch: Original 'minnesota, usa' corrected to 'United States'\n",
      "Mismatch: Original 'baden-wuerttemberg, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'colorado,' corrected to 'United States'\n",
      "Mismatch: Original 'new york, usa' corrected to 'United States'\n",
      "Mismatch: Original 'trinidad' corrected to 'Trinidad and Tobago'\n",
      "Mismatch: Original 'england' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'deutschland' corrected to 'Germany'\n",
      "Mismatch: Original 'austria\"' corrected to 'Austria'\n",
      "Mismatch: Original 'republic of panama, panama' corrected to 'Panama'\n",
      "Mismatch: Original 'sweden\"' corrected to 'Sweden'\n",
      "Mismatch: Original 'minnehaha' corrected to 'United States'\n",
      "Mismatch: Original 'michigan, usa' corrected to 'United States'\n",
      "Mismatch: Original 'piemonte, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'galiza' corrected to 'Spain'\n",
      "Mismatch: Original 'catalunya, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'karnataka, india' corrected to 'India'\n",
      "Mismatch: Original 'guandong province, china' corrected to 'China'\n",
      "Mismatch: Original 'pennsylvania, usa' corrected to 'United States'\n",
      "Mismatch: Original 'v9v 1h4, british columbia, canada' corrected to 'Canada'\n",
      "Mismatch: Original '&#32654;&#22269;' corrected to 'United States'\n",
      "Mismatch: Original 'cape, south africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'españa, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'distrito federal, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'texas, afghanistan' corrected to 'Afghanistan'\n",
      "Mismatch: Original 'canterbury, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'n/a, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'u.s. of a.' corrected to 'United States'\n",
      "Mismatch: Original 'arkansas, usa' corrected to 'United States'\n",
      "Mismatch: Original 'afganstand holla !!' corrected to 'Afghanistan'\n",
      "Mismatch: Original 'islas canarias, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'catalunya' corrected to 'Spain'\n",
      "Mismatch: Original 'serbia & montenegro' corrected to 'Serbia'\n",
      "Mismatch: Original 'maine, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'saskatchewan, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'trinidad & tobago' corrected to 'Trinidad and Tobago'\n",
      "Mismatch: Original 'ondo state, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'n/a, eritrea' corrected to 'Eritrea'\n",
      "Mismatch: Original 'n/a, greece' corrected to 'Greece'\n",
      "Mismatch: Original 'maryland, usa' corrected to 'United States'\n",
      "Mismatch: Original 'iowa, usa' corrected to 'United States'\n",
      "Mismatch: Original 'lk ammerland, deutschland' corrected to 'Germany'\n",
      "Mismatch: Original 'burma' corrected to 'Myanmar (Burma)'\n",
      "Mismatch: Original 'wisconsin, usa' corrected to 'United States'\n",
      "Mismatch: Original 'uk' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'df, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'alberta, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'selangor d.e., malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'n/a, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'ghana\"' corrected to 'Ghana'\n",
      "Mismatch: Original 'turkei' corrected to 'Türkiye'\n",
      "Mismatch: Original 'ksa' corrected to 'Saudi Arabia'\n",
      "Mismatch: Original 'pakistan, sindh, pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'new south wales, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'manitoba, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'maharashtra, india, india' corrected to 'India'\n",
      "Mismatch: Original 'iceland, iceland' corrected to 'Iceland'\n",
      "Mismatch: Original 'ca., usa' corrected to 'United States'\n",
      "Mismatch: Original 'sri lanka\\\"n/a\\\"\"' corrected to 'Sri Lanka'\n",
      "Mismatch: Original 'valtesse' corrected to 'Italy'\n",
      "Mismatch: Original 'washington, pakistan' corrected to 'United States'\n",
      "Mismatch: Original 'cote d`ivoire' corrected to 'Côte d'Ivoire'\n",
      "Mismatch: Original 'catalunya(catalonia)' corrected to 'Spain'\n",
      "Mismatch: Original 'mã?â©xico' corrected to 'Mexico'\n",
      "Mismatch: Original 'us virgin islands, caribbean sea' corrected to 'U.S. Virgin Islands'\n",
      "Mismatch: Original 'peru\"' corrected to 'Peru'\n",
      "Mismatch: Original 'n/a, south africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'queensland, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'lleida' corrected to 'Spain'\n",
      "Mismatch: Original 'nottinghamshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'florida, usa' corrected to 'United States'\n",
      "Mismatch: Original 'n/a, india' corrected to 'India'\n",
      "Mismatch: Original 'utah, usa' corrected to 'United States'\n",
      "Mismatch: Original 'itlay' corrected to 'Italy'\n",
      "Mismatch: Original 'u.s.a.' corrected to 'United States'\n",
      "Mismatch: Original 'germany, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'belgique' corrected to 'Belgium'\n",
      "Mismatch: Original 'brazil, brazil' corrected to 'Brazil'\n",
      "Mismatch: Original 'south wales., united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'st. vincent and the grenadines' corrected to 'Saint Vincent and the Grenadines'\n",
      "Mismatch: Original 'hessen, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'colorado, usa' corrected to 'United States'\n",
      "Mismatch: Original 'álava, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'catalonia' corrected to 'Spain'\n",
      "Mismatch: Original 'collin' corrected to 'United States'\n",
      "Mismatch: Original 'south dakota, usa' corrected to 'United States'\n",
      "Mismatch: Original 'zhengjiang' corrected to 'China'\n",
      "Mismatch: Original 'kansas, usa' corrected to 'United States'\n",
      "Mismatch: Original 'california, usa' corrected to 'United States'\n",
      "Mismatch: Original 'maharastra, india' corrected to 'India'\n",
      "Mismatch: Original 'oregon, usa' corrected to 'United States'\n",
      "Mismatch: Original 'scotland, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'rhode island, usa' corrected to 'United States'\n",
      "Mismatch: Original 'wales' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'south carolina, usa' corrected to 'United States'\n",
      "Mismatch: Original 'new zealand, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'uk, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'hennipen' corrected to 'United States'\n",
      "Mismatch: Original 'north carolina, usa' corrected to 'United States'\n",
      "Mismatch: Original 'new mexico, usa' corrected to 'United States'\n",
      "Mismatch: Original 'equatorial geuinea' corrected to 'Equatorial Guinea'\n",
      "Mismatch: Original 'italy\"' corrected to 'Italy'\n",
      "Mismatch: Original 'n/a, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'virginia, usa' corrected to 'United States'\n",
      "Mismatch: Original 'italia' corrected to 'Italy'\n",
      "Mismatch: Original 'n/a, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'indiana, usa' corrected to 'United States'\n",
      "Mismatch: Original 'good old usa !' corrected to 'United States'\n",
      "Mismatch: Original 'polska' corrected to 'Poland'\n",
      "Mismatch: Original 'goteborg' corrected to 'Sweden'\n",
      "Mismatch: Original 'chandigarh, india' corrected to 'India'\n",
      "Mismatch: Original 'california, united states' corrected to 'United States'\n",
      "Mismatch: Original 'south australia, australia' corrected to 'Australia'\n",
      "Mismatch: Original ', pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'baltimore' corrected to 'United States'\n",
      "Mismatch: Original 'hampden' corrected to 'New Zealand'\n",
      "Mismatch: Original 'cambridgeshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'ust' corrected to 'Philippines'\n",
      "Mismatch: Original 'polk' corrected to 'United States'\n",
      "Mismatch: Original 'vermont, usa' corrected to 'United States'\n",
      "Mismatch: Original 'benguet, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'malaysia, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'norfolk, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'wales, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'bernalillo' corrected to 'United States'\n",
      "Mismatch: Original 'effingham' corrected to 'United States'\n",
      "Mismatch: Original 'almería, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'veneto, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'italia, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'na, usa' corrected to 'United States'\n",
      "Mismatch: Original 'nova scotia, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'turkey,' corrected to 'Türkiye'\n",
      "Mismatch: Original 'windham' corrected to 'United States'\n",
      "Mismatch: Original 'etelä-suomi, finland' corrected to 'Finland'\n",
      "Mismatch: Original 'belgium\"' corrected to 'Belgium'\n",
      "Mismatch: Original 'pa, usa' corrected to 'United States'\n",
      "Mismatch: Original 'ile de france, france' corrected to 'France'\n",
      "Mismatch: Original 'new hampshire, usa' corrected to 'United States'\n",
      "Mismatch: Original 'aragón, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'veracruz, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'n/a, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'sinaloa, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'brazil\"' corrected to 'Brazil'\n",
      "Mismatch: Original 'new jersey, usa' corrected to 'United States'\n",
      "Mismatch: Original 'auckland' corrected to 'New Zealand'\n",
      "Mismatch: Original 'holland, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'kentucky, usa' corrected to 'United States'\n",
      "Mismatch: Original 'illinois, usa' corrected to 'United States'\n",
      "Mismatch: Original 'switzerland, switzerland' corrected to 'Switzerland'\n",
      "Mismatch: Original 'channel islands, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'cote d`or, burgundy, france' corrected to 'France'\n",
      "Mismatch: Original 'pasco' corrected to 'United States'\n",
      "Mismatch: Original 'perlis, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'united states of america' corrected to 'United States'\n",
      "Mismatch: Original 'ferrara' corrected to 'Italy'\n",
      "Mismatch: Original 'milano' corrected to 'Italy'\n",
      "Mismatch: Original 'n/a, japan' corrected to 'Japan'\n",
      "Mismatch: Original 'slo' corrected to 'United States'\n",
      "Mismatch: Original 'sichuan/china, china' corrected to 'China'\n",
      "Mismatch: Original 'beijing, china' corrected to 'China'\n",
      "Mismatch: Original 'marrion' corrected to 'United States'\n",
      "Mismatch: Original 'gulf view, n/a, trinidad and tobago' corrected to 'Trinidad and Tobago'\n",
      "Mismatch: Original 'commonwealth of northern mariana islands' corrected to 'Northern Mariana Islands'\n",
      "Mismatch: Original 'china, china' corrected to 'China'\n",
      "Mismatch: Original 'india, india' corrected to 'India'\n",
      "Mismatch: Original 'nigeria, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'cnina' corrected to 'China'\n",
      "Mismatch: Original 'hb, ka, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'holy see' corrected to 'Vatican City'\n",
      "Mismatch: Original 'são paulo, brazil' corrected to 'Brazil'\n",
      "Mismatch: Original 'noord-brabant, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'región metropolitana, chile' corrected to 'Chile'\n",
      "Mismatch: Original 'alava, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'oeiras, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'ohio, usa' corrected to 'United States'\n",
      "Mismatch: Original 'holland' corrected to 'Netherlands'\n",
      "Mismatch: Original 'trentino alto adige, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'nottingham, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'espaã?â±a' corrected to 'Uruguay'\n",
      "Mismatch: Original 'noord holland, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'lazio' corrected to 'Italy'\n",
      "Mismatch: Original 'lazio, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'dekalb' corrected to 'United States'\n",
      "Mismatch: Original 'massachusetts, usa' corrected to 'United States'\n",
      "Mismatch: Original 'la belgique' corrected to 'Belgium'\n",
      "Mismatch: Original 'berlin, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'v region, chile' corrected to 'Chile'\n",
      "Mismatch: Original 'el general carlos ibañes del campo., chile' corrected to 'Chile'\n",
      "Mismatch: Original 'chesterfield south carolina, usa' corrected to 'United States'\n",
      "Mismatch: Original 'maricopa' corrected to 'United States'\n",
      "Mismatch: Original 'tamil nadu, india' corrected to 'India'\n",
      "Mismatch: Original 'paris, france' corrected to 'France'\n",
      "Mismatch: Original 'uk, wales, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'berkshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'attica, greece' corrected to 'Greece'\n",
      "Mismatch: Original 'utrecht, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'chile\"' corrected to 'Chile'\n",
      "Mismatch: Original 'algérie' corrected to 'Algeria'\n",
      "Mismatch: Original 'north yorkshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'nrw' corrected to 'Germany'\n",
      "Mismatch: Original 'tasmania, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'nyhamnsläge' corrected to 'Sweden'\n",
      "Mismatch: Original 'national capital region, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'gh,accra, ghana' corrected to 'Ghana'\n",
      "Mismatch: Original 'egypt\"' corrected to 'Egypt'\n",
      "Mismatch: Original 'block-o,r.k.puram,sector-13,new delhi , india, india' corrected to 'India'\n",
      "Mismatch: Original 'richmond country' corrected to 'United States'\n",
      "Mismatch: Original 'camberley, berkshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original ', portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'republic of korea' corrected to 'South Korea'\n",
      "Mismatch: Original 'san luis potosí, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'california,' corrected to 'United States'\n",
      "Mismatch: Original 'saint loius' corrected to 'United States'\n",
      "Mismatch: Original 'quebec, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'croatia\"' corrected to 'Croatia'\n",
      "Mismatch: Original 'greece, greece\"' corrected to 'Greece'\n",
      "Mismatch: Original 'central java/jawa tengah, indonesia' corrected to 'Indonesia'\n",
      "Mismatch: Original 'england, england' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'framingham' corrected to 'United States'\n",
      "Mismatch: Original ', usa' corrected to 'United States'\n",
      "Mismatch: Original 'kyklades, greece' corrected to 'Greece'\n",
      "Mismatch: Original 'tx, florida, usa' corrected to 'United States'\n",
      "Mismatch: Original 'bedfordshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'trapani, sicilia, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'kykladon, n/a, greece' corrected to 'Greece'\n",
      "Mismatch: Original 'san bernardino' corrected to 'United States'\n",
      "Mismatch: Original 'nyc' corrected to 'United States'\n",
      "Mismatch: Original 'montana, usa' corrected to 'United States'\n",
      "Mismatch: Original 'united kindgonm' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'bvi, caribbean sea' corrected to 'British Virgin Islands'\n",
      "Mismatch: Original 'cumbria, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'd.f., mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'pennsylvania, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'ee.uu' corrected to 'United States'\n",
      "Mismatch: Original 'maekel,asmera, eritrea' corrected to 'Eritrea'\n",
      "Mismatch: Original 'new brunswick, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'wielkopolska, poland' corrected to 'Poland'\n",
      "Mismatch: Original 'negeri sembilan, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'andalucía, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'guernsey, channel islands' corrected to 'Guernsey'\n",
      "Mismatch: Original 'south island, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'espaã±a' corrected to 'Uruguay'\n",
      "Mismatch: Original ', turkey' corrected to 'Türkiye'\n",
      "Mismatch: Original 'acoruña,galicia, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'washtenaw' corrected to 'United States'\n",
      "Mismatch: Original 'tennessee, usa' corrected to 'United States'\n",
      "Mismatch: Original 'cabanatuan city, nueva ecija, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'cebu, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'pais vasco, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'california, baja california, usa, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'suisse' corrected to 'Switzerland'\n",
      "Mismatch: Original 'kuala lumpur, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'western australia, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'puerto rico, pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'iceland\"' corrected to 'Iceland'\n",
      "Mismatch: Original 'south africa, south africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'macau' corrected to 'Macao'\n",
      "Mismatch: Original 'frome' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'brunei darussalam' corrected to 'Brunei'\n",
      "Mismatch: Original 'greece\"' corrected to 'Greece'\n",
      "Mismatch: Original 'cape town' corrected to 'South Africa'\n",
      "Mismatch: Original 'barcelona, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'st. helena' corrected to 'Saint Helena, Ascension and Tristan da Cunha'\n",
      "Mismatch: Original 'sultanate of oman' corrected to 'Oman'\n",
      "Mismatch: Original 'hagiwara, gifu, japan' corrected to 'Japan'\n",
      "Mismatch: Original 'canada, ontario, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'aroostook' corrected to 'United States'\n",
      "Mismatch: Original 'fredonia - land of the brave and free' corrected to 'United States'\n",
      "Mismatch: Original 'china\"' corrected to 'China'\n",
      "Mismatch: Original 'shandong province,china, china' corrected to 'China'\n",
      "Mismatch: Original 'guangdong, china' corrected to 'China'\n",
      "Mismatch: Original 'pontevedra, galiza, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'kudarat, isulan, sultan kudarat, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'qatar, qatar' corrected to 'Qatar'\n",
      "Mismatch: Original 'america' corrected to 'United States'\n",
      "Mismatch: Original 'bademn würtemberg' corrected to 'Germany'\n",
      "Mismatch: Original 'smaland, sweden' corrected to 'Sweden'\n",
      "Mismatch: Original 'lisboa, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'kwara state, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'la suisse' corrected to 'Switzerland'\n",
      "Mismatch: Original 'gb' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'worcester' corrected to 'United States'\n",
      "Mismatch: Original 'região autónoma da madeira, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'auckland, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'italy, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'provincia de arauco, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'entre ríos, argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'henderson, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'canarias, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'méxico' corrected to 'Mexico'\n",
      "Mismatch: Original 'poland\"' corrected to 'Poland'\n",
      "Mismatch: Original 'ex rabiee el gizie, egypt' corrected to 'Egypt'\n",
      "Mismatch: Original 'perak, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'congo' corrected to 'Democratic Republic of the Congo'\n",
      "Mismatch: Original 'osceola' corrected to 'United States'\n",
      "Mismatch: Original 'east anglia, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'van wert' corrected to 'United States'\n",
      "Mismatch: Original 'stadt, switzerland' corrected to 'Switzerland'\n",
      "Mismatch: Original 'zapopan, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original '87510' corrected to 'United States'\n",
      "Mismatch: Original 'alentejo, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'europe, n/a, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'hungary\"' corrected to 'Hungary'\n",
      "Mismatch: Original 'green brook, new jersey, usa' corrected to 'United States'\n",
      "Mismatch: Original 'dublin' corrected to 'Ireland'\n",
      "Mismatch: Original 'extremadura, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'mugla, turkey' corrected to 'Türkiye'\n",
      "Mismatch: Original 'p.r. china' corrected to 'China'\n",
      "Mismatch: Original 'cagayan, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'ciudad de mexico, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'molise, usa' corrected to 'United States'\n",
      "Mismatch: Original 'usa & canada' corrected to 'Canada'\n",
      "Mismatch: Original 'nordrhein-westfalen, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'myanmar' corrected to 'Myanmar (Burma)'\n",
      "Mismatch: Original 'molise, china' corrected to 'China'\n",
      "Mismatch: Original 'essex' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'sao tome and principe' corrected to 'São Tomé and Príncipe'\n",
      "Mismatch: Original 'georgia, usa' corrected to 'United States'\n",
      "Mismatch: Original 'bagan datoh, perak, n/a, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'vietnam\"' corrected to 'Vietnam'\n",
      "Mismatch: Original 'hubei province, hong kong' corrected to 'Hong Kong'\n",
      "Mismatch: Original 'portugal, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'emilia romagna, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'los angeles, usa' corrected to 'United States'\n",
      "Mismatch: Original 'sicilia' corrected to 'Italy'\n",
      "Mismatch: Original 'west virginia, usa' corrected to 'United States'\n",
      "Mismatch: Original 'singapore/united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'kievs`ka obl, ukraine' corrected to 'Ukraine'\n",
      "Mismatch: Original 'singapore, singapore' corrected to 'Singapore'\n",
      "Mismatch: Original 'punjab, pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'alabama, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'west midlands, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'andalucia, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'antalya/olympos, istanbul,eskisehir,antalya/olympos, turkey' corrected to 'Türkiye'\n",
      "Mismatch: Original 'dorchester,ma 02121, massachusetts, usa' corrected to 'United States'\n",
      "Mismatch: Original 'canary islands, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'alachua' corrected to 'United States'\n",
      "Mismatch: Original 'china öð¹ú' corrected to 'China'\n",
      "Mismatch: Original 'phitsanulok, thailand' corrected to 'Thailand'\n",
      "Mismatch: Original 'lombardia' corrected to 'Italy'\n",
      "Mismatch: Original 'chiba ken, japan' corrected to 'Japan'\n",
      "Mismatch: Original 'otago, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'mérida' corrected to 'Spain'\n",
      "Mismatch: Original 'antigua & barbuda' corrected to 'Antigua and Barbuda'\n",
      "Mismatch: Original ', new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'hillsborough' corrected to 'United States'\n",
      "Mismatch: Original 'bourgogne, france' corrected to 'France'\n",
      "Mismatch: Original 'greece (=hellas)' corrected to 'Greece'\n",
      "Mismatch: Original 'australia, victoria, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'san franicsco' corrected to 'United States'\n",
      "Mismatch: Original 'u.s>' corrected to 'United States'\n",
      "Mismatch: Original 'agusan del sur, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'jersey, channel islands' corrected to 'Jersey'\n",
      "Mismatch: Original 'u.s.a!' corrected to 'United States'\n",
      "Mismatch: Original 'n/a, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'co. kilkenny, ireland' corrected to 'Ireland'\n",
      "Mismatch: Original 'edo. de mexico, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original '02458' corrected to 'United States'\n",
      "Mismatch: Original '5057chadwick ct.' corrected to 'United States'\n",
      "Mismatch: Original 'the netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'us' corrected to 'United States'\n",
      "Mismatch: Original 'fl, usa' corrected to 'United States'\n",
      "Mismatch: Original 'metro manila, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'lower saxony, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'usa, rhode island, usa' corrected to 'United States'\n",
      "Mismatch: Original 'belgi' corrected to 'Belgium'\n",
      "Mismatch: Original 'pinallas' corrected to 'United States'\n",
      "Mismatch: Original 'constantine, algeria' corrected to 'Algeria'\n",
      "Mismatch: Original 'sudan\"' corrected to 'Sudan'\n",
      "Mismatch: Original 'south west, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'new york, georgia, usa' corrected to 'United States'\n",
      "Mismatch: Original 'k1c7b1' corrected to 'Canada'\n",
      "Mismatch: Original 'orense' corrected to 'Spain'\n",
      "Mismatch: Original ', cape verde' corrected to 'Cabo Verde'\n",
      "Mismatch: Original 'indianapolis, in (ind-indianapolis intl.), usa' corrected to 'United States'\n",
      "Mismatch: Original 'japan military, usa' corrected to 'United States'\n",
      "Mismatch: Original 'seoul, south korea' corrected to 'South Korea'\n",
      "Mismatch: Original 'tn, usa' corrected to 'United States'\n",
      "Mismatch: Original 'louisiana, usa' corrected to 'United States'\n",
      "Mismatch: Original 'pa' corrected to 'United States'\n",
      "Mismatch: Original 'auckland.nz, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original '61 men,guishan li,xueyuan rd. hexi dist, china' corrected to 'China'\n",
      "Mismatch: Original 'sagadan, tubod, lanao del norte, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'middlesex, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'surabaya, indonesia' corrected to 'Indonesia'\n",
      "Mismatch: Original 'serbia and montenegro' corrected to 'Serbia'\n",
      "Mismatch: Original 'sth island n>z>, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original '\\n/a\\\", mexico\"' corrected to 'Mexico'\n",
      "Mismatch: Original 'region of capital / bratislava, slovakia' corrected to 'Slovakia'\n",
      "Mismatch: Original 'camden' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'mexico\"' corrected to 'Mexico'\n",
      "Mismatch: Original 'canada, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'liaoning' corrected to 'China'\n",
      "Mismatch: Original '13 - 10ºesq., portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'china people`s republic' corrected to 'China'\n",
      "Mismatch: Original 'italien' corrected to 'Italy'\n",
      "Mismatch: Original 'ventura county' corrected to 'United States'\n",
      "Mismatch: Original 'españa, alaska, usa' corrected to 'United States'\n",
      "Mismatch: Original 'norway, norway' corrected to 'Norway'\n",
      "Mismatch: Original 'alaska, usa' corrected to 'United States'\n",
      "Mismatch: Original 'berguedà' corrected to 'Spain'\n",
      "Mismatch: Original 'hamburg, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'basilicata' corrected to 'Italy'\n",
      "Mismatch: Original 'n/a, denmark' corrected to 'Denmark'\n",
      "Mismatch: Original 'ysa' corrected to 'United States'\n",
      "Mismatch: Original 'africa, liberia' corrected to 'Liberia'\n",
      "Mismatch: Original 'kathmandu, nepal' corrected to 'Nepal'\n",
      "Mismatch: Original 'êàëóæñêàÿ îáë, russia' corrected to 'Russia'\n",
      "Mismatch: Original 'provincia del cachapoal, chile' corrected to 'Chile'\n",
      "Mismatch: Original 'ghana, ghana' corrected to 'Ghana'\n",
      "Mismatch: Original 'sardegna' corrected to 'Italy'\n",
      "Mismatch: Original 'rosario' corrected to 'Argentina'\n",
      "Mismatch: Original 'usa now, the world tomorrow' corrected to 'United States'\n",
      "Mismatch: Original 'leyte, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'u.s.a>' corrected to 'United States'\n",
      "Mismatch: Original ', malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'støední cechy, czech republic' corrected to 'Czechia'\n",
      "Mismatch: Original 'aotearoa' corrected to 'New Zealand'\n",
      "Mismatch: Original 'burkinafasu' corrected to 'Burkina Faso'\n",
      "Mismatch: Original 'querétaro, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'good old u.s.a.' corrected to 'United States'\n",
      "Mismatch: Original 'yukon territory, china' corrected to 'China'\n",
      "Mismatch: Original 'unit' corrected to 'United States'\n",
      "Mismatch: Original 'w sussex, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'rosello' corrected to 'Spain'\n",
      "Mismatch: Original 'nebraska, australia' corrected to 'Mexico'\n",
      "Mismatch: Original 'devon, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'asturias, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'v götalands län, sweden' corrected to 'Sweden'\n",
      "Mismatch: Original 'cebu city, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'bruselas, belgium' corrected to 'Belgium'\n",
      "Mismatch: Original 'england uk' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'ruhrgebiet, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'county cork, ireland' corrected to 'Ireland'\n",
      "Mismatch: Original 'distrito federal, brazil' corrected to 'Brazil'\n",
      "Mismatch: Original 'provincia mi, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'storkøbenhavn, denmark' corrected to 'Denmark'\n",
      "Mismatch: Original 'n/a, usa' corrected to 'Paraguay'\n",
      "Mismatch: Original 'brittany, france' corrected to 'France'\n",
      "Mismatch: Original 'n/a, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'usa, guam' corrected to 'Guam'\n",
      "Mismatch: Original 'new london' corrected to 'United States'\n",
      "Mismatch: Original 'jharkhand, india' corrected to 'India'\n",
      "Mismatch: Original 'washington, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'region de coquimbo pais chile, chile' corrected to 'Chile'\n",
      "Mismatch: Original '23232' corrected to 'United States'\n",
      "Mismatch: Original 'sanjose, desamparados, costa rica' corrected to 'Costa Rica'\n",
      "Mismatch: Original 'copenhagen' corrected to 'Denmark'\n",
      "Mismatch: Original 'burma\"' corrected to 'Myanmar (Burma)'\n",
      "Mismatch: Original 'canda' corrected to 'Canada'\n",
      "Mismatch: Original 'north dakota, usa' corrected to 'United States'\n",
      "Mismatch: Original 'yamunanagar, haryana, india' corrected to 'India'\n",
      "Mismatch: Original 'dhaka, bangladesh, bangladesh' corrected to 'Bangladesh'\n",
      "Mismatch: Original 'sweden, sweden' corrected to 'Sweden'\n",
      "Mismatch: Original 'travelling...., british columbia, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'midi-pyrénées, france' corrected to 'France'\n",
      "Mismatch: Original 'nz, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'n/a, sweden' corrected to 'Sweden'\n",
      "Mismatch: Original 'capital federal, argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'phippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'sarajevo, bosnia and herzegovina' corrected to 'Bosnia and Herzegovina'\n",
      "Mismatch: Original 'emilia, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'jamaica\"' corrected to 'Jamaica'\n",
      "Mismatch: Original 'sardinia' corrected to 'Italy'\n",
      "Mismatch: Original 'cape town , south africa, south africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'mi, oakland' corrected to 'United States'\n",
      "Mismatch: Original 'lecce' corrected to 'Italy'\n",
      "Mismatch: Original 'u k' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'philippines, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'mozambique, mozambique' corrected to 'Mozambique'\n",
      "Mismatch: Original 'moçambique' corrected to 'Mozambique'\n",
      "Mismatch: Original 'andorra\"' corrected to 'Andorra'\n",
      "Mismatch: Original 'ventura' corrected to 'United States'\n",
      "Mismatch: Original 'gipuzkoa, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'ogunstate, ogun state, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'ma 01075), usa' corrected to 'United States'\n",
      "Mismatch: Original 'niedersachsen, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'n/a, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'western, sri lanka' corrected to 'Sri Lanka'\n",
      "Mismatch: Original ', mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'yishun st 11,#04-157, singapore' corrected to 'Singapore'\n",
      "Mismatch: Original 'bulacan, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'sarawak, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'mexico, d.f., mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'n.z., new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original 'usa\"' corrected to 'United States'\n",
      "Mismatch: Original 'madrid, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'selangor, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'n/a, china' corrected to 'China'\n",
      "Mismatch: Original ', denmark' corrected to 'Denmark'\n",
      "Mismatch: Original 'usa (currently living in england)' corrected to 'United States'\n",
      "Mismatch: Original ', england' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'roma' corrected to 'Italy'\n",
      "Mismatch: Original 'málaga, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'indiai' corrected to 'India'\n",
      "Mismatch: Original 'prov. de bs. as., argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'alabama, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'france, france' corrected to 'France'\n",
      "Mismatch: Original 'district of columbia, usa' corrected to 'United States'\n",
      "Mismatch: Original 'l`algérie' corrected to 'Algeria'\n",
      "Mismatch: Original 'buenos aires, argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'illes balears, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'fontana' corrected to 'United States'\n",
      "Mismatch: Original 'n/a, united arab emirates' corrected to 'United Arab Emirates'\n",
      "Mismatch: Original 'makassar, indonesia, indonesia' corrected to 'Indonesia'\n",
      "Mismatch: Original 'australii' corrected to 'Australia'\n",
      "Mismatch: Original 'u.a.e\"' corrected to 'United Arab Emirates'\n",
      "Mismatch: Original 'libya\"' corrected to 'Libya'\n",
      "Mismatch: Original 'italy has no states or provinces, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'gyeonggido, north korea' corrected to 'South Korea'\n",
      "Mismatch: Original 'uganda\"' corrected to 'Uganda'\n",
      "Mismatch: Original 'ukrain' corrected to 'Ukraine'\n",
      "Mismatch: Original 'onondaga nation' corrected to 'United States'\n",
      "Mismatch: Original '_ brasil' corrected to 'Brazil'\n",
      "Mismatch: Original 'galicia, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'hungary and usa' corrected to 'United States'\n",
      "Mismatch: Original 'toscana, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'queensland' corrected to 'Australia'\n",
      "Mismatch: Original 'cananda' corrected to 'Canada'\n",
      "Mismatch: Original 'ticino, switzerland' corrected to 'Switzerland'\n",
      "Mismatch: Original 'n/a, france' corrected to 'France'\n",
      "Mismatch: Original 'obviously, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original ', the netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'not applicable, japan' corrected to 'Japan'\n",
      "Mismatch: Original 'kansas, south africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'solano' corrected to 'United States'\n",
      "Mismatch: Original 'n.y, unite states' corrected to 'United States'\n",
      "Mismatch: Original 'minnesota, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'aberdeenshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'belize, c.a.' corrected to 'Belize'\n",
      "Mismatch: Original 'oyo state, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'pender' corrected to 'United States'\n",
      "Mismatch: Original 'lagos state, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'vicenza' corrected to 'Italy'\n",
      "Mismatch: Original 'address is:adem ademoski 13, macedonia' corrected to 'North Macedonia'\n",
      "Mismatch: Original 'p.r.c' corrected to 'Philippines'\n",
      "Mismatch: Original 'madrid / ourense / marinha, espanha / galiza' corrected to 'Spain'\n",
      "Mismatch: Original 'nigeria, ,ogba,lagosstate, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'nigeria in west  african, antigua and barbuda' corrected to 'Nigeria'\n",
      "Mismatch: Original 'maracopa' corrected to 'United States'\n",
      "Mismatch: Original 'deep padania, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'il canada' corrected to 'Canada'\n",
      "Mismatch: Original 'usa, usa' corrected to 'United States'\n",
      "Mismatch: Original 'republic of panama' corrected to 'Panama'\n",
      "Mismatch: Original 'ikotun egbe, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'ca, usa' corrected to 'United States'\n",
      "Mismatch: Original 'county galway, ireland' corrected to 'Ireland'\n",
      "Mismatch: Original 'phila' corrected to 'United States'\n",
      "Mismatch: Original 'bc, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'yu-song, guam-dong, 626-1, 302, south korea' corrected to 'South Korea'\n",
      "Mismatch: Original 'grand bahama, bahamas' corrected to 'The Bahamas'\n",
      "Mismatch: Original 'city, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'c.t., australia' corrected to 'Australia'\n",
      "Mismatch: Original ', italy' corrected to 'Italy'\n",
      "Mismatch: Original 'west yorkshire' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'region centro, argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'rimini, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'northern ireland' corrected to 'United Kingdom'\n",
      "Mismatch: Original '600 083, india' corrected to 'India'\n",
      "Mismatch: Original 'normandy, france' corrected to 'France'\n",
      "Mismatch: Original 'uyo' corrected to 'Nigeria'\n",
      "Mismatch: Original 'brabant, belgium' corrected to 'Belgium'\n",
      "Mismatch: Original 'sonora, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'lorraine, france' corrected to 'France'\n",
      "Mismatch: Original 'la altagracia, dominican republic' corrected to 'Dominican Republic'\n",
      "Mismatch: Original 'spotsylvania' corrected to 'United States'\n",
      "Mismatch: Original 'swaziland' corrected to 'Eswatini'\n",
      "Mismatch: Original 'nuevo león, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'livingston' corrected to 'United Kingdom'\n",
      "Mismatch: Original 's.corea' corrected to 'South Korea'\n",
      "Mismatch: Original 'australia, queensland, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'phillipines' corrected to 'Philippines'\n",
      "Mismatch: Original 'bosnia' corrected to 'Bosnia and Herzegovina'\n",
      "Mismatch: Original 'ant., colombia' corrected to 'Colombia'\n",
      "Mismatch: Original 'euskadi' corrected to 'Spain'\n",
      "Mismatch: Original 'canada eh' corrected to 'Canada'\n",
      "Mismatch: Original 'n/a, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'india, vellore, india' corrected to 'India'\n",
      "Mismatch: Original 'argentina, argentina' corrected to 'Argentina'\n",
      "Mismatch: Original 'le canada' corrected to 'Canada'\n",
      "Mismatch: Original 'netherlands,europe, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'korea' corrected to 'South Korea'\n",
      "Mismatch: Original 'nh' corrected to 'United States'\n",
      "Mismatch: Original 'uae' corrected to 'United Arab Emirates'\n",
      "Mismatch: Original 'yanhill' corrected to 'United States'\n",
      "Mismatch: Original 'wonderful usa' corrected to 'United States'\n",
      "Mismatch: Original 'uusa' corrected to 'United States'\n",
      "Mismatch: Original 'australia, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'rep san marino' corrected to 'San Marino'\n",
      "Mismatch: Original 'swazilandia' corrected to 'Eswatini'\n",
      "Mismatch: Original 'u.k.' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'gtr manchester, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'nanjing, china' corrected to 'China'\n",
      "Mismatch: Original 'maine' corrected to 'United States'\n",
      "Mismatch: Original 'british columbia, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original ', indonesia' corrected to 'Indonesia'\n",
      "Mismatch: Original 'anystate, usa, usa' corrected to 'United States'\n",
      "Mismatch: Original '30064' corrected to 'United States'\n",
      "Mismatch: Original 'buncombe' corrected to 'United States'\n",
      "Mismatch: Original 'navruz 5 proezd 4, uzbekistan' corrected to 'Uzbekistan'\n",
      "Mismatch: Original 'santa barbara' corrected to 'United States'\n",
      "Mismatch: Original 'hainan, china' corrected to 'China'\n",
      "Mismatch: Original 'trinidad/tobago.' corrected to 'Trinidad and Tobago'\n",
      "Mismatch: Original 'metropolitan washington, dc area (dc, md, and va), usa' corrected to 'United States'\n",
      "Mismatch: Original 'houston' corrected to 'United States'\n",
      "Mismatch: Original 'vestergade 8, denmark' corrected to 'Denmark'\n",
      "Mismatch: Original '\\n/a\\\", brazil\"' corrected to 'Brazil'\n",
      "Mismatch: Original 'països catalans, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'newfoundland, canada' corrected to 'Canada'\n",
      "Mismatch: Original 'heliopolis, egypt' corrected to 'Egypt'\n",
      "Mismatch: Original 'slovak republik' corrected to 'Slovakia'\n",
      "Mismatch: Original 'cheshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'croatia, croatia' corrected to 'Croatia'\n",
      "Mismatch: Original ', australia' corrected to 'Australia'\n",
      "Mismatch: Original 'co. carlow, ireland' corrected to 'Ireland'\n",
      "Mismatch: Original 'bavaria' corrected to 'Germany'\n",
      "Mismatch: Original 's.africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'australian capital territory, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'vlaanderen, belgium' corrected to 'Belgium'\n",
      "Mismatch: Original 'malta, europe.' corrected to 'Malta'\n",
      "Mismatch: Original 'palm beach' corrected to 'United States'\n",
      "Mismatch: Original 'kedah, malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'usa, kansas, usa' corrected to 'United States'\n",
      "Mismatch: Original 'north carolina' corrected to 'United States'\n",
      "Mismatch: Original 'los estados unidos de norte america' corrected to 'United States'\n",
      "Mismatch: Original 'russian federation' corrected to 'Russia'\n",
      "Mismatch: Original 'courtenay' corrected to 'Canada'\n",
      "Mismatch: Original 'north island, new zealand' corrected to 'New Zealand'\n",
      "Mismatch: Original ', bangladesh\"' corrected to 'Bangladesh'\n",
      "Mismatch: Original 'nevada, usa' corrected to 'United States'\n",
      "Mismatch: Original 'pangasinan,,region 3, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original '\\n/a\\\", greece\"' corrected to 'Greece'\n",
      "Mismatch: Original 'morocco\"' corrected to 'Morocco'\n",
      "Mismatch: Original 'st.gallen, switzerland' corrected to 'Switzerland'\n",
      "Mismatch: Original ', netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'europe : uk, france, switzerland' corrected to 'France'\n",
      "Mismatch: Original 'kalamazoo, usa' corrected to 'United States'\n",
      "Mismatch: Original 'region iv, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'nebraska, usa' corrected to 'United States'\n",
      "Mismatch: Original 'kent, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'new providence, bahamas' corrected to 'The Bahamas'\n",
      "Mismatch: Original 'orange co' corrected to 'United States'\n",
      "Mismatch: Original 'texas' corrected to 'United States'\n",
      "Mismatch: Original ', canada' corrected to 'Canada'\n",
      "Mismatch: Original 'rizal, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'baden-württemberg' corrected to 'Germany'\n",
      "Mismatch: Original 'in europe, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'korea, south korea' corrected to 'South Korea'\n",
      "Mismatch: Original '4main,6cross,2phase,r`nagar,m`nagar,b`lore-10, india' corrected to 'India'\n",
      "Mismatch: Original 'alsace, france' corrected to 'France'\n",
      "Mismatch: Original 'shanghai, china' corrected to 'China'\n",
      "Mismatch: Original 'quetigny , côte-d`or, france' corrected to 'France'\n",
      "Mismatch: Original 'gujarat, india' corrected to 'India'\n",
      "Mismatch: Original 'london, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'hawaii, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'd.c., usa' corrected to 'United States'\n",
      "Mismatch: Original 'idaho, usa' corrected to 'United States'\n",
      "Mismatch: Original 'india, kerala, india' corrected to 'India'\n",
      "Mismatch: Original 'jawa-tengah, indonesia' corrected to 'Indonesia'\n",
      "Mismatch: Original 'san mateo' corrected to 'United States'\n",
      "Mismatch: Original 'davidson' corrected to 'United States'\n",
      "Mismatch: Original 'saint luica' corrected to 'Saint Lucia'\n",
      "Mismatch: Original 'thane, mumbai, pin 421 506, ambernath, thane , maharastra, india' corrected to 'India'\n",
      "Mismatch: Original '85021' corrected to 'United States'\n",
      "Mismatch: Original 'west indies, tobago' corrected to 'Trinidad and Tobago'\n",
      "Mismatch: Original 'n/a, afghanistan' corrected to 'Afghanistan'\n",
      "Mismatch: Original 'malkajgiri,hyderabad, ap, india' corrected to 'India'\n",
      "Mismatch: Original 'ohio' corrected to 'United States'\n",
      "Mismatch: Original 'western cape, south africa' corrected to 'South Africa'\n",
      "Mismatch: Original 'montenegro, yugoslavia' corrected to 'Montenegro'\n",
      "Mismatch: Original 'heilongjiang, china' corrected to 'China'\n",
      "Mismatch: Original 'west brabant, netherlands' corrected to 'Netherlands'\n",
      "Mismatch: Original 'whatcom' corrected to 'United States'\n",
      "Mismatch: Original 'nz' corrected to 'New Zealand'\n",
      "Mismatch: Original 'nordeste, brazil' corrected to 'Brazil'\n",
      "Mismatch: Original 'madrid' corrected to 'Spain'\n",
      "Mismatch: Original ', philippines\"' corrected to 'Philippines'\n",
      "Mismatch: Original 'shizuoka pref., japan' corrected to 'Japan'\n",
      "Mismatch: Original 'seoul korea, south korea' corrected to 'South Korea'\n",
      "Mismatch: Original 'toscana' corrected to 'Italy'\n",
      "Mismatch: Original 'butler' corrected to 'United States'\n",
      "Mismatch: Original 'a new year is ahead, and i know you have a pl, ghana' corrected to 'Ghana'\n",
      "Mismatch: Original 'costa brava, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'fairyland' corrected to 'United States'\n",
      "Mismatch: Original 'ahrensburg' corrected to 'Germany'\n",
      "Mismatch: Original 'costa rica, costa rica' corrected to 'Costa Rica'\n",
      "Mismatch: Original 'kern' corrected to 'United States'\n",
      "Mismatch: Original 'nova scotia, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'baleares, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'california, disgruntled states of america' corrected to 'United States'\n",
      "Mismatch: Original 'rheinland-pfalz, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'cape may' corrected to 'United States'\n",
      "Mismatch: Original 'hunan, china' corrected to 'China'\n",
      "Mismatch: Original 'españa\"' corrected to 'Spain'\n",
      "Mismatch: Original 'león, spain' corrected to 'Spain'\n",
      "Mismatch: Original 'kristianstad, åhus, sweden' corrected to 'Sweden'\n",
      "Mismatch: Original 'costa rica, américa central, costa rica' corrected to 'Costa Rica'\n",
      "Mismatch: Original 'liushi' corrected to 'China'\n",
      "Mismatch: Original 'bih' corrected to 'Bosnia and Herzegovina'\n",
      "Mismatch: Original 'w. malaysia' corrected to 'Malaysia'\n",
      "Mismatch: Original 'st.thomasi' corrected to 'Canada'\n",
      "Mismatch: Original 'camp arif jan, kuwait' corrected to 'Kuwait'\n",
      "Mismatch: Original 'alajuela, san carlos, costa rica' corrected to 'Costa Rica'\n",
      "Mismatch: Original 'okinawa, japan' corrected to 'Japan'\n",
      "Mismatch: Original 'n. ireland, bt66 6bx, northern ireland, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'fort bend' corrected to 'United States'\n",
      "Mismatch: Original 'eu, austria' corrected to 'Austria'\n",
      "Mismatch: Original 'trinidad and tobago west indies, trinidad and tobago' corrected to 'Trinidad and Tobago'\n",
      "Mismatch: Original 'geermany' corrected to 'Germany'\n",
      "Mismatch: Original 'western, singapore' corrected to 'Singapore'\n",
      "Mismatch: Original 'pasig city., philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'nsw, australia' corrected to 'Australia'\n",
      "Mismatch: Original 'north sumatera, indonesia' corrected to 'Indonesia'\n",
      "Mismatch: Original 'il, usa' corrected to 'United States'\n",
      "Mismatch: Original 'le madagascar' corrected to 'Madagascar'\n",
      "Mismatch: Original 'thailand, thailand' corrected to 'Thailand'\n",
      "Mismatch: Original 'tubigon, bohol, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'yakima' corrected to 'United States'\n",
      "Mismatch: Original 'balcistsa, india' corrected to 'India'\n",
      "Mismatch: Original 'päijät-häme, finland' corrected to 'Finland'\n",
      "Mismatch: Original 'lincolnshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original '\\n/a\\\", spain\"' corrected to 'Spain'\n",
      "Mismatch: Original 'vara?dinska ?upanija, croatia' corrected to 'Croatia'\n",
      "Mismatch: Original 'coimbra, coimbra, portugal' corrected to 'Portugal'\n",
      "Mismatch: Original 'zelezny brod, n/a, czech republic' corrected to 'Czechia'\n",
      "Mismatch: Original 'colorado, egypt' corrected to 'Egypt'\n",
      "Mismatch: Original 'nwfp, pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'behind omident petrol, osun state, nigeria' corrected to 'Nigeria'\n",
      "Mismatch: Original 'wisconsin, china' corrected to 'United States'\n",
      "Mismatch: Original 'p r china' corrected to 'China'\n",
      "Mismatch: Original 'isabela, basilan, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'yorkshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'morayshire, united kingdom' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'manila, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original '01776' corrected to 'United States'\n",
      "Mismatch: Original 'people`s republic of china' corrected to 'China'\n",
      "Mismatch: Original 'lane' corrected to 'United States'\n",
      "Mismatch: Original 'venezia giulia,italy, italy' corrected to 'Italy'\n",
      "Mismatch: Original 'wales, uk' corrected to 'United Kingdom'\n",
      "Mismatch: Original 'bonn, köln, germany' corrected to 'Germany'\n",
      "Mismatch: Original 'galiza neghra' corrected to 'Spain'\n",
      "Mismatch: Original 'shanxi province, china' corrected to 'China'\n",
      "Mismatch: Original 'asturies' corrected to 'Spain'\n",
      "Mismatch: Original 'nl' corrected to 'Netherlands'\n",
      "Mismatch: Original 'prov. di milano, italy' corrected to 'Italy'\n",
      "Mismatch: Original '&#20013;&#22269;' corrected to 'China'\n",
      "Mismatch: Original 'austin, usa' corrected to 'United States'\n",
      "Mismatch: Original 'wa,id,or,ak,bc, usa' corrected to 'United States'\n",
      "Mismatch: Original 'costa rica, san ramon de alajuela, costa rica' corrected to 'Costa Rica'\n",
      "Mismatch: Original 'germay' corrected to 'France'\n",
      "Mismatch: Original 'overijssel, netherlands, hungary' corrected to 'Netherlands'\n",
      "Mismatch: Original 'kawit,cavite, n/a, philippines' corrected to 'Philippines'\n",
      "Mismatch: Original 'va, usa' corrected to 'United States'\n",
      "Mismatch: Original 'piemonte, pakistan' corrected to 'Pakistan'\n",
      "Mismatch: Original 'estado de mexico, mexico' corrected to 'Mexico'\n",
      "Mismatch: Original 'laoning, china' corrected to 'China'\n",
      "Mismatch: Original 'hebei, china' corrected to 'China'\n",
      "Mismatch: Original 'p.r.china, china' corrected to 'China'\n",
      "Mismatch: Original 'hubei, china' corrected to 'China'\n",
      "Mismatch: Original 'victoria' corrected to 'Australia'\n",
      "Mismatch: Original 'bkk' corrected to 'Thailand'\n",
      "Mismatch: Original 'la svizzera' corrected to 'Switzerland'\n",
      "Mismatch: Original 'hernando' corrected to 'United States'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_standard</th>\n",
       "      <th>validated_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usa</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>russia</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portugal</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>united kingdom</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canada</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_standard validated_country\n",
       "0              usa     United States\n",
       "1           russia            Russia\n",
       "2         portugal          Portugal\n",
       "3   united kingdom    United Kingdom\n",
       "4           canada            Canada"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googlemaps\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "gmaps = googlemaps.Client(key=\"PutYoursNotMine\") \n",
    "\n",
    "def validate_country(country):\n",
    "    \"\"\"\n",
    "    Validate the standardized country using the Google Maps Geocoding API.\n",
    "    Returns the country (long_name) as provided by the API.\n",
    "    \"\"\"\n",
    "    if not isinstance(country, str) or not country.strip():\n",
    "        return None\n",
    "    query = country.strip()\n",
    "    try:\n",
    "        # Call the API.\n",
    "        geocode_result = gmaps.geocode(query)\n",
    "        # Pause briefly to avoid rate limits.\n",
    "        time.sleep(0.1)\n",
    "        if geocode_result:\n",
    "            components = geocode_result[0]['address_components']\n",
    "            for comp in components:\n",
    "                if \"country\" in comp.get(\"types\", []):\n",
    "                    return comp.get(\"long_name\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating country '{query}': {e}\")\n",
    "    return None\n",
    "\n",
    "# Extract the unique countries from the 'country_standard' column.\n",
    "unique_countries = df['country_standard'].dropna().unique()\n",
    "print(f\"Found {len(unique_countries)} unique countries.\")\n",
    "\n",
    "# Cache the API results to avoid duplicate calls.\n",
    "validation_cache = {}\n",
    "for country in unique_countries:\n",
    "    validated = validate_country(country)\n",
    "    validation_cache[country] = validated\n",
    "\n",
    "# Check for mismatches between the original and the validated country names.\n",
    "mismatches = []\n",
    "for orig, validated in validation_cache.items():\n",
    "    if validated and validated.lower() != orig.lower():\n",
    "        mismatches.append((orig, validated))\n",
    "        print(f\"Mismatch: Original '{orig}' corrected to '{validated}'\")\n",
    "\n",
    "# Optionally, create a DataFrame for further inspection.\n",
    "validation_df = pd.DataFrame(list(validation_cache.items()), columns=['country_standard', 'validated_country'])\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_standard</th>\n",
       "      <th>validated_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usa</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>russia</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portugal</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>united kingdom</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canada</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>hubei, china</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>victoria</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>bkk</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>la svizzera</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>hernando</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country_standard validated_country\n",
       "0                 usa     United States\n",
       "1              russia            Russia\n",
       "2            portugal          Portugal\n",
       "3      united kingdom    United Kingdom\n",
       "4              canada            Canada\n",
       "...               ...               ...\n",
       "1170     hubei, china             China\n",
       "1171         victoria         Australia\n",
       "1172              bkk          Thailand\n",
       "1173      la svizzera       Switzerland\n",
       "1174         hernando     United States\n",
       "\n",
       "[1175 rows x 2 columns]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validated_country</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russia</td>\n",
       "      <td>RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>GB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  validated_country country_code\n",
       "0     United States           US\n",
       "1     United States           US\n",
       "2            Russia           RU\n",
       "3          Portugal           PT\n",
       "4    United Kingdom           GB"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "def get_country_code(country_name):\n",
    "    \"\"\"\n",
    "    Given a validated country name (e.g., \"United States\"),\n",
    "    return the corresponding ISO 3166-1 alpha-2 code (e.g., \"US\").\n",
    "    If the country is not found, return None.\n",
    "    \"\"\"\n",
    "    if not isinstance(country_name, str) or not country_name.strip():\n",
    "        return None\n",
    "    \n",
    "    # Manual corrections for known edge cases\n",
    "    corrections = {\n",
    "        \"russia\": \"Russian Federation\",\n",
    "        # Add other corrections as needed\n",
    "    }\n",
    "    \n",
    "    # Normalize the input for comparison\n",
    "    normalized = country_name.strip()\n",
    "    # Apply correction if it exists (comparison is case-insensitive)\n",
    "    if normalized.lower() in corrections:\n",
    "        normalized = corrections[normalized.lower()]\n",
    "    \n",
    "    try:\n",
    "        country = pycountry.countries.lookup(normalized)\n",
    "        return country.alpha_2  # or country.alpha_3 for three-letter codes\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "# Now, applying the function to the DataFrame's validated_country column:\n",
    "df['country_code'] = df['validated_country'].apply(get_country_code)\n",
    "\n",
    "df[['validated_country', 'country_code']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Book Sales and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Publishing Year</th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>language_code</th>\n",
       "      <th>Author_Rating</th>\n",
       "      <th>Book_average_rating</th>\n",
       "      <th>Book_ratings_count</th>\n",
       "      <th>genre</th>\n",
       "      <th>gross sales</th>\n",
       "      <th>publisher revenue</th>\n",
       "      <th>sale price</th>\n",
       "      <th>sales rank</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>units sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>Beowulf</td>\n",
       "      <td>Unknown, Seamus Heaney</td>\n",
       "      <td>en-US</td>\n",
       "      <td>Novice</td>\n",
       "      <td>3.42</td>\n",
       "      <td>155903</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>34160.0</td>\n",
       "      <td>20496.0</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Batman: Year One</td>\n",
       "      <td>Frank Miller, David Mazzucchelli, Richmond Lewis, Dennis O'Neil</td>\n",
       "      <td>eng</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>4.23</td>\n",
       "      <td>145267</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>12437.5</td>\n",
       "      <td>7462.5</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Go Set a Watchman</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>eng</td>\n",
       "      <td>Novice</td>\n",
       "      <td>3.31</td>\n",
       "      <td>138669</td>\n",
       "      <td>genre fiction</td>\n",
       "      <td>47795.0</td>\n",
       "      <td>28677.0</td>\n",
       "      <td>8.69</td>\n",
       "      <td>3</td>\n",
       "      <td>Amazon Digital Services,  Inc.</td>\n",
       "      <td>5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Publishing Year          Book Name  \\\n",
       "0      0           1975.0            Beowulf   \n",
       "1      1           1987.0   Batman: Year One   \n",
       "2      2           2015.0  Go Set a Watchman   \n",
       "\n",
       "                                                            Author  \\\n",
       "0                                           Unknown, Seamus Heaney   \n",
       "1  Frank Miller, David Mazzucchelli, Richmond Lewis, Dennis O'Neil   \n",
       "2                                                       Harper Lee   \n",
       "\n",
       "  language_code Author_Rating  Book_average_rating  Book_ratings_count  \\\n",
       "0         en-US        Novice                 3.42              155903   \n",
       "1           eng  Intermediate                 4.23              145267   \n",
       "2           eng        Novice                 3.31              138669   \n",
       "\n",
       "           genre  gross sales  publisher revenue  sale price  sales rank  \\\n",
       "0  genre fiction      34160.0            20496.0        4.88           1   \n",
       "1  genre fiction      12437.5             7462.5        1.99           2   \n",
       "2  genre fiction      47795.0            28677.0        8.69           3   \n",
       "\n",
       "                       Publisher   units sold  \n",
       "0        HarperCollins Publishers        7000  \n",
       "1        HarperCollins Publishers        6250  \n",
       "2  Amazon Digital Services,  Inc.        5500  "
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Datasets/5_sales_N_ratings/Books_Data_Clean.csv\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"index\" column has no value, probably was left by the person that built this data set as an artifact.\n",
    "\n",
    "Dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = df.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1070\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "Publishing Year: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Book Name: {'min_length': 4, 'max_length': 124, 'mean_length': 22.134670487106018, 'median_length': 18.0}\n",
      "Author: {'min_length': 7, 'max_length': 257, 'mean_length': 18.828971962616823, 'median_length': 15.0}\n",
      "language_code: {'min_length': 2, 'max_length': 5, 'mean_length': 3.5319567354965584, 'median_length': 3.0}\n",
      "Author_Rating: {'min_length': 6, 'max_length': 12, 'mean_length': 10.519626168224299, 'median_length': 12.0}\n",
      "Book_average_rating: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Book_ratings_count: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "genre: {'min_length': 7, 'max_length': 13, 'mean_length': 12.102803738317757, 'median_length': 13.0}\n",
      "gross sales: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "publisher revenue: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "sale price: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "sales rank: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "Publisher : {'min_length': 9, 'max_length': 36, 'mean_length': 26.148598130841123, 'median_length': 30.0}\n",
      "units sold: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "Publishing Year        0.093458\n",
      "Book Name              2.149533\n",
      "Author                 0.000000\n",
      "language_code          4.953271\n",
      "Author_Rating          0.000000\n",
      "Book_average_rating    0.000000\n",
      "Book_ratings_count     0.000000\n",
      "genre                  0.000000\n",
      "gross sales            0.000000\n",
      "publisher revenue      0.000000\n",
      "sale price             0.000000\n",
      "sales rank             0.000000\n",
      "Publisher              0.000000\n",
      "units sold             0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'Publishing Year': 150, 'Book Name': 1045, 'Author': 735, 'language_code': 8, 'Author_Rating': 4, 'Book_average_rating': 134, 'Book_ratings_count': 1064, 'genre': 4, 'gross sales': 831, 'publisher revenue': 610, 'sale price': 149, 'sales rank': 865, 'Publisher ': 9, 'units sold': 491}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Publishing Year': 14.018691588785046,\n",
       " 'Book Name': 97.66355140186917,\n",
       " 'Author': 68.69158878504673,\n",
       " 'language_code': 0.7476635514018692,\n",
       " 'Author_Rating': 0.3738317757009346,\n",
       " 'Book_average_rating': 12.52336448598131,\n",
       " 'Book_ratings_count': 99.4392523364486,\n",
       " 'genre': 0.3738317757009346,\n",
       " 'gross sales': 77.66355140186916,\n",
       " 'publisher revenue': 57.009345794392516,\n",
       " 'sale price': 13.925233644859814,\n",
       " 'sales rank': 80.8411214953271,\n",
       " 'Publisher ': 0.8411214953271028,\n",
       " 'units sold': 45.887850467289724}"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1070 entries, 0 to 1069\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Publishing Year      1069 non-null   float64\n",
      " 1   Book Name            1047 non-null   object \n",
      " 2   Author               1070 non-null   object \n",
      " 3   language_code        1017 non-null   object \n",
      " 4   Author_Rating        1070 non-null   object \n",
      " 5   Book_average_rating  1070 non-null   float64\n",
      " 6   Book_ratings_count   1070 non-null   int64  \n",
      " 7   genre                1070 non-null   object \n",
      " 8   gross sales          1070 non-null   float64\n",
      " 9   publisher revenue    1070 non-null   float64\n",
      " 10  sale price           1070 non-null   float64\n",
      " 11  sales rank           1070 non-null   int64  \n",
      " 12  Publisher            1070 non-null   object \n",
      " 13  units sold           1070 non-null   int64  \n",
      "dtypes: float64(5), int64(3), object(6)\n",
      "memory usage: 117.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publishing Year', 'Book Name', 'Author', 'language_code',\n",
       "       'Author_Rating', 'Book_average_rating', 'Book_ratings_count', 'genre',\n",
       "       'gross sales', 'publisher revenue', 'sale price', 'sales rank',\n",
       "       'Publisher ', 'units sold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"genre\" column**\n",
    "\n",
    "Drop genre name from \"genre\" column, its adds no value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'] = df['genre'].str.replace(r'\\bgenre\\b', '', case=False, regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very broad genre categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "fiction       884\n",
       "nonfiction    171\n",
       "children       15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"Publisher \" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Publisher \n",
       "Amazon Digital Services,  Inc.          600\n",
       "Random House LLC                        120\n",
       "Penguin Group (USA) LLC                 108\n",
       "HarperCollins Publishers                 71\n",
       "Hachette Book Group                      66\n",
       "Simon and Schuster Digital Sales Inc     56\n",
       "Macmillan                                41\n",
       "HarperCollins Publishing                  4\n",
       "HarperCollins Christian Publishing        4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Publisher '].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['publishing_year', 'book_name', 'author', 'language_code', 'author_rating', 'book_average_rating', 'book_ratings_count', 'genre', 'gross_sales', 'publisher_revenue', 'sale_price', 'sales_rank', 'publisher', 'units_sold']\n"
     ]
    }
   ],
   "source": [
    "def standardize_column_name(col):\n",
    "    # Remove leading/trailing whitespace, convert to lower-case, replace spaces with underscores\n",
    "    col = col.strip()\n",
    "    col = col.lower()\n",
    "    col = re.sub(r'\\s+', '_', col)\n",
    "    return col\n",
    "\n",
    "# Standardize all column names\n",
    "df.columns = [standardize_column_name(c) for c in df.columns]\n",
    "\n",
    "# Optionally, print the new column names to verify\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/5_sales_N_ratings/Books_Data_Clean_Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Amazon Kindle Books’23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Datasets/6_kindle/kindle_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 133102\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "asin: {'min_length': 10, 'max_length': 10, 'mean_length': 10.0, 'median_length': 10.0}\n",
      "title: {'min_length': 2, 'max_length': 211, 'mean_length': 66.46685248906853, 'median_length': 61.0}\n",
      "author: {'min_length': 1, 'max_length': 158, 'mean_length': 13.843710665752164, 'median_length': 13.0}\n",
      "soldBy: {'min_length': 5, 'max_length': 39, 'mean_length': 22.954282346672695, 'median_length': 23.0}\n",
      "imgUrl: {'min_length': 14, 'max_length': 62, 'mean_length': 61.99891812294331, 'median_length': 62.0}\n",
      "productURL: {'min_length': 36, 'max_length': 36, 'mean_length': 36.0, 'median_length': 36.0}\n",
      "stars: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "reviews: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "price: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "isKindleUnlimited: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "category_id: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "isBestSeller: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "isEditorsPick: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "isGoodReadsChoice: {'min_length': None, 'max_length': None, 'mean_length': None, 'median_length': None}\n",
      "publishedDate: {'min_length': 10, 'max_length': 10, 'mean_length': 10.0, 'median_length': 10.0}\n",
      "category_name: {'min_length': 3, 'max_length': 28, 'mean_length': 18.086136947604093, 'median_length': 20.0}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "asin                  0.000000\n",
      "title                 0.000000\n",
      "author                0.319304\n",
      "soldBy                6.936785\n",
      "imgUrl                0.000000\n",
      "productURL            0.000000\n",
      "stars                 0.000000\n",
      "reviews               0.000000\n",
      "price                 0.000000\n",
      "isKindleUnlimited     0.000000\n",
      "category_id           0.000000\n",
      "isBestSeller          0.000000\n",
      "isEditorsPick         0.000000\n",
      "isGoodReadsChoice     0.000000\n",
      "publishedDate        36.825893\n",
      "category_name         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'asin': 133102, 'title': 131913, 'author': 72805, 'soldBy': 49, 'imgUrl': 132909, 'productURL': 133102, 'stars': 38, 'reviews': 8441, 'price': 4795, 'isKindleUnlimited': 2, 'category_id': 31, 'isBestSeller': 2, 'isEditorsPick': 2, 'isGoodReadsChoice': 2, 'publishedDate': 7157, 'category_name': 31}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'asin': 100.0,\n",
       " 'title': 99.10670012471638,\n",
       " 'author': 54.698652161500206,\n",
       " 'soldBy': 0.036813872068038045,\n",
       " 'imgUrl': 99.85499842226263,\n",
       " 'productURL': 100.0,\n",
       " 'stars': 0.028549533440519304,\n",
       " 'reviews': 6.341752941353247,\n",
       " 'price': 3.6025003380865805,\n",
       " 'isKindleUnlimited': 0.0015026070231852263,\n",
       " 'category_id': 0.02329040885937101,\n",
       " 'isBestSeller': 0.0015026070231852263,\n",
       " 'isEditorsPick': 0.0015026070231852263,\n",
       " 'isGoodReadsChoice': 0.0015026070231852263,\n",
       " 'publishedDate': 5.377079232468333,\n",
       " 'category_name': 0.02329040885937101}"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 133102 entries, 0 to 133101\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   asin               133102 non-null  object \n",
      " 1   title              133102 non-null  object \n",
      " 2   author             132677 non-null  object \n",
      " 3   soldBy             123869 non-null  object \n",
      " 4   imgUrl             133102 non-null  object \n",
      " 5   productURL         133102 non-null  object \n",
      " 6   stars              133102 non-null  float64\n",
      " 7   reviews            133102 non-null  int64  \n",
      " 8   price              133102 non-null  float64\n",
      " 9   isKindleUnlimited  133102 non-null  bool   \n",
      " 10  category_id        133102 non-null  int64  \n",
      " 11  isBestSeller       133102 non-null  bool   \n",
      " 12  isEditorsPick      133102 non-null  bool   \n",
      " 13  isGoodReadsChoice  133102 non-null  bool   \n",
      " 14  publishedDate      84086 non-null   object \n",
      " 15  category_name      133102 non-null  object \n",
      "dtypes: bool(4), float64(2), int64(2), object(8)\n",
      "memory usage: 12.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'title', 'author', 'soldBy', 'imgUrl', 'productURL', 'stars',\n",
       "       'reviews', 'price', 'isKindleUnlimited', 'category_id', 'isBestSeller',\n",
       "       'isEditorsPick', 'isGoodReadsChoice', 'publishedDate', 'category_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>soldBy</th>\n",
       "      <th>imgUrl</th>\n",
       "      <th>productURL</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "      <th>price</th>\n",
       "      <th>isKindleUnlimited</th>\n",
       "      <th>category_id</th>\n",
       "      <th>isBestSeller</th>\n",
       "      <th>isEditorsPick</th>\n",
       "      <th>isGoodReadsChoice</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00TZE87S4</td>\n",
       "      <td>Adult Children of Emotionally Immature Parents: How to Heal from Distant, Rejecting, or Self-Inv...</td>\n",
       "      <td>Lindsay C. Gibson</td>\n",
       "      <td>Amazon.com Services LLC</td>\n",
       "      <td>https://m.media-amazon.com/images/I/713KZTsaYpL._AC_UY218_.jpg</td>\n",
       "      <td>https://www.amazon.com/dp/B00TZE87S4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "      <td>9.99</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>Parenting &amp; Relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B08WCKY8MB</td>\n",
       "      <td>From Strength to Strength: Finding Success, Happiness, and Deep Purpose in the Second Half of Life</td>\n",
       "      <td>Arthur C. Brooks</td>\n",
       "      <td>Penguin Group (USA) LLC</td>\n",
       "      <td>https://m.media-amazon.com/images/I/A1LZcJFs9EL._AC_UY218_.jpg</td>\n",
       "      <td>https://www.amazon.com/dp/B08WCKY8MB</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>Parenting &amp; Relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B09KPS84CJ</td>\n",
       "      <td>Good Inside: A Guide to Becoming the Parent You Want to Be</td>\n",
       "      <td>Becky Kennedy</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71RIWM0sv6L._AC_UY218_.jpg</td>\n",
       "      <td>https://www.amazon.com/dp/B09KPS84CJ</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "      <td>16.99</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>Parenting &amp; Relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07S7QPG6J</td>\n",
       "      <td>Everything I Know About Love: A Memoir</td>\n",
       "      <td>Dolly Alderton</td>\n",
       "      <td>HarperCollins Publishers</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71QdQpTiKZL._AC_UY218_.jpg</td>\n",
       "      <td>https://www.amazon.com/dp/B07S7QPG6J</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.95</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>Parenting &amp; Relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00N6PEQV0</td>\n",
       "      <td>The Seven Principles for Making Marriage Work: A Practical Guide from the Country's Foremost Rel...</td>\n",
       "      <td>John Gottman</td>\n",
       "      <td>Random House LLC</td>\n",
       "      <td>https://m.media-amazon.com/images/I/813o4WOs+wL._AC_UY218_.jpg</td>\n",
       "      <td>https://www.amazon.com/dp/B00N6PEQV0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.99</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-05</td>\n",
       "      <td>Parenting &amp; Relationships</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  \\\n",
       "0  B00TZE87S4   \n",
       "1  B08WCKY8MB   \n",
       "2  B09KPS84CJ   \n",
       "3  B07S7QPG6J   \n",
       "4  B00N6PEQV0   \n",
       "\n",
       "                                                                                                 title  \\\n",
       "0  Adult Children of Emotionally Immature Parents: How to Heal from Distant, Rejecting, or Self-Inv...   \n",
       "1   From Strength to Strength: Finding Success, Happiness, and Deep Purpose in the Second Half of Life   \n",
       "2                                           Good Inside: A Guide to Becoming the Parent You Want to Be   \n",
       "3                                                               Everything I Know About Love: A Memoir   \n",
       "4  The Seven Principles for Making Marriage Work: A Practical Guide from the Country's Foremost Rel...   \n",
       "\n",
       "              author                    soldBy  \\\n",
       "0  Lindsay C. Gibson   Amazon.com Services LLC   \n",
       "1   Arthur C. Brooks   Penguin Group (USA) LLC   \n",
       "2      Becky Kennedy  HarperCollins Publishers   \n",
       "3     Dolly Alderton  HarperCollins Publishers   \n",
       "4       John Gottman          Random House LLC   \n",
       "\n",
       "                                                           imgUrl  \\\n",
       "0  https://m.media-amazon.com/images/I/713KZTsaYpL._AC_UY218_.jpg   \n",
       "1  https://m.media-amazon.com/images/I/A1LZcJFs9EL._AC_UY218_.jpg   \n",
       "2  https://m.media-amazon.com/images/I/71RIWM0sv6L._AC_UY218_.jpg   \n",
       "3  https://m.media-amazon.com/images/I/71QdQpTiKZL._AC_UY218_.jpg   \n",
       "4  https://m.media-amazon.com/images/I/813o4WOs+wL._AC_UY218_.jpg   \n",
       "\n",
       "                             productURL  stars  reviews  price  \\\n",
       "0  https://www.amazon.com/dp/B00TZE87S4    4.8        0   9.99   \n",
       "1  https://www.amazon.com/dp/B08WCKY8MB    4.4        0  16.99   \n",
       "2  https://www.amazon.com/dp/B09KPS84CJ    4.8        0  16.99   \n",
       "3  https://www.amazon.com/dp/B07S7QPG6J    4.2        0   9.95   \n",
       "4  https://www.amazon.com/dp/B00N6PEQV0    4.7        0  13.99   \n",
       "\n",
       "   isKindleUnlimited  category_id  isBestSeller  isEditorsPick  \\\n",
       "0              False            6          True          False   \n",
       "1              False            6         False          False   \n",
       "2              False            6         False           True   \n",
       "3               True            6         False           True   \n",
       "4              False            6         False          False   \n",
       "\n",
       "   isGoodReadsChoice publishedDate              category_name  \n",
       "0              False    2015-06-01  Parenting & Relationships  \n",
       "1              False    2022-02-15  Parenting & Relationships  \n",
       "2              False    2022-09-13  Parenting & Relationships  \n",
       "3              False    2020-02-25  Parenting & Relationships  \n",
       "4              False    2015-05-05  Parenting & Relationships  "
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) WonderBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Datasets/7_wonderbk/wonderbooks.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cardinalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 103082\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**String Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String length statistics per column:\n",
      "Title: {'min_length': 1, 'max_length': 293, 'mean_length': 44.741875400166855, 'median_length': 39.0}\n",
      "Authors: {'min_length': 2, 'max_length': 305, 'mean_length': 25.969373896509573, 'median_length': 19.0}\n",
      "Description: {'min_length': 1, 'max_length': 23502, 'mean_length': 735.8172275789384, 'median_length': 578.0}\n",
      "Category: {'min_length': 12, 'max_length': 99, 'mean_length': 34.31217495319326, 'median_length': 33.0}\n",
      "Publisher: {'min_length': 1, 'max_length': 150, 'mean_length': 15.26375225566098, 'median_length': 14.0}\n",
      "Publish Date: {'min_length': 12, 'max_length': 28, 'mean_length': 23.437059816456802, 'median_length': 23.0}\n",
      "Price: {'min_length': 23, 'max_length': 27, 'mean_length': 23.107846180710503, 'median_length': 23.0}\n"
     ]
    }
   ],
   "source": [
    "string_stats = {col: string_length_stats(df[col]) for col in df.columns}\n",
    "print(\"\\nString length statistics per column:\")\n",
    "for col, stats in string_stats.items():\n",
    "    print(f\"{col}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null values per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values per column (as percentage):\n",
      "Title            0.000000\n",
      "Authors          0.000000\n",
      "Description     31.886265\n",
      "Category        25.387556\n",
      "Publisher        0.007761\n",
      "Publish Date     0.000000\n",
      "Price            0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(\"\\nNull values per column (as percentage):\")\n",
    "print(df.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct values per column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct values per column:\n",
      "{'Title': 97818, 'Authors': 63580, 'Description': 68831, 'Category': 3106, 'Publisher': 13029, 'Publish Date': 956, 'Price': 1387}\n"
     ]
    }
   ],
   "source": [
    "distinct_values = {col: safe_nunique(df[col]) for col in df.columns}\n",
    "print(\"\\nDistinct values per column:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniqueness ratio**\n",
    "\n",
    "(distinct values / total rows) * 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness ratio (in %):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Title': 94.89338584815971,\n",
       " 'Authors': 61.67905162880037,\n",
       " 'Description': 66.77305446149667,\n",
       " 'Category': 3.0131351739391943,\n",
       " 'Publisher': 12.63945208668827,\n",
       " 'Publish Date': 0.9274170078190178,\n",
       " 'Price': 1.345530742515667}"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness = {col: (safe_nunique(df[col]) / num_rows) * 100 for col in df.columns}\n",
    "print(\"\\nUniqueness ratio (in %):\")\n",
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103082 entries, 0 to 103081\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   Title         103082 non-null  object\n",
      " 1   Authors       103082 non-null  object\n",
      " 2   Description   70213 non-null   object\n",
      " 3   Category      76912 non-null   object\n",
      " 4   Publisher     103074 non-null  object\n",
      " 5   Publish Date  103082 non-null  object\n",
      " 6   Price         103082 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Authors', 'Description', 'Category', 'Publisher',\n",
       "       'Publish Date', 'Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goat Brothers</td>\n",
       "      <td>By Colton, Larry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>History , General</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>Friday, January 1, 1993</td>\n",
       "      <td>Price Starting at $8.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Missing Person</td>\n",
       "      <td>By Grumbach, Doris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fiction , General</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>Sunday, March 1, 1981</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't Eat Your Heart Out Cookbook</td>\n",
       "      <td>By Piscatella, Joseph C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooking , Reference</td>\n",
       "      <td>Workman Pub Co</td>\n",
       "      <td>Thursday, September 1, 1983</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment</td>\n",
       "      <td>By Davis, Paul D.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natl Pr Books</td>\n",
       "      <td>Monday, April 1, 1991</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Spangler's Breastfeeding : A Parent's Guide</td>\n",
       "      <td>By Spangler, Amy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amy Spangler</td>\n",
       "      <td>Saturday, February 1, 1997</td>\n",
       "      <td>Price Starting at $5.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    Title  \\\n",
       "0                                                                           Goat Brothers   \n",
       "1                                                                      The Missing Person   \n",
       "2                                                       Don't Eat Your Heart Out Cookbook   \n",
       "3  When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment   \n",
       "4                                         Amy Spangler's Breastfeeding : A Parent's Guide   \n",
       "\n",
       "                    Authors Description              Category  \\\n",
       "0          By Colton, Larry         NaN     History , General   \n",
       "1        By Grumbach, Doris         NaN     Fiction , General   \n",
       "2  By Piscatella, Joseph C.         NaN   Cooking , Reference   \n",
       "3         By Davis, Paul D.         NaN                   NaN   \n",
       "4          By Spangler, Amy         NaN                   NaN   \n",
       "\n",
       "          Publisher                 Publish Date                    Price  \n",
       "0         Doubleday      Friday, January 1, 1993  Price Starting at $8.79  \n",
       "1  Putnam Pub Group        Sunday, March 1, 1981  Price Starting at $4.99  \n",
       "2    Workman Pub Co  Thursday, September 1, 1983  Price Starting at $4.99  \n",
       "3     Natl Pr Books        Monday, April 1, 1991  Price Starting at $4.99  \n",
       "4      Amy Spangler   Saturday, February 1, 1997  Price Starting at $5.32  "
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"Publish Date\" column**\n",
    "\n",
    "Parsing into ISO format YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Publish Date publication_date\n",
      "0      Friday, January 1, 1993       1993-01-01\n",
      "1        Sunday, March 1, 1981       1981-03-01\n",
      "2  Thursday, September 1, 1983       1983-09-01\n",
      "3        Monday, April 1, 1991       1991-04-01\n",
      "4   Saturday, February 1, 1997       1997-02-01\n"
     ]
    }
   ],
   "source": [
    "df['publication_date'] = pd.to_datetime(df['Publish Date'], errors='coerce')\n",
    "\n",
    "# Display a sample of the original and parsed publish dates\n",
    "print(df[['Publish Date', 'publication_date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"Price\" column**\n",
    "\n",
    "Parsing the numeric price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'Price' does not contain '$':\n",
      "Empty DataFrame\n",
      "Columns: [Price]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price Starting at $8.79</td>\n",
       "      <td>8.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price Starting at $5.32</td>\n",
       "      <td>5.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price  price\n",
       "0  Price Starting at $8.79   8.79\n",
       "1  Price Starting at $4.99   4.99\n",
       "2  Price Starting at $4.99   4.99\n",
       "3  Price Starting at $4.99   4.99\n",
       "4  Price Starting at $5.32   5.32"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_dollar = df['Price'].str.contains(r'\\$', regex=True)\n",
    "print(\"Rows where 'Price' does not contain '$':\")\n",
    "print(df.loc[~has_dollar, ['Price']].head())\n",
    "\n",
    "df['price'] = df['Price'].str.extract(r'\\$(\\d+\\.\\d+)')[0].astype(float)\n",
    "df[['Price', 'price']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"Authors Date\" column**\n",
    "\n",
    "Removing By and putting Names in proper order: First name - Middle name - Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goat Brothers</td>\n",
       "      <td>By Colton, Larry</td>\n",
       "      <td>Larry Colton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Missing Person</td>\n",
       "      <td>By Grumbach, Doris</td>\n",
       "      <td>Doris Grumbach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't Eat Your Heart Out Cookbook</td>\n",
       "      <td>By Piscatella, Joseph C.</td>\n",
       "      <td>Joseph C. Piscatella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment</td>\n",
       "      <td>By Davis, Paul D.</td>\n",
       "      <td>Paul D. Davis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Spangler's Breastfeeding : A Parent's Guide</td>\n",
       "      <td>By Spangler, Amy</td>\n",
       "      <td>Amy Spangler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    Title  \\\n",
       "0                                                                           Goat Brothers   \n",
       "1                                                                      The Missing Person   \n",
       "2                                                       Don't Eat Your Heart Out Cookbook   \n",
       "3  When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment   \n",
       "4                                         Amy Spangler's Breastfeeding : A Parent's Guide   \n",
       "\n",
       "                    Authors               authors  \n",
       "0          By Colton, Larry          Larry Colton  \n",
       "1        By Grumbach, Doris        Doris Grumbach  \n",
       "2  By Piscatella, Joseph C.  Joseph C. Piscatella  \n",
       "3         By Davis, Paul D.         Paul D. Davis  \n",
       "4          By Spangler, Amy          Amy Spangler  "
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_authors(author_str):\n",
    "    if not isinstance(author_str, str):\n",
    "        return author_str\n",
    "    # Remove the \"By \" prefix if present (case-insensitive)\n",
    "    author_str = author_str.strip()\n",
    "    if author_str.lower().startswith(\"by \"):\n",
    "        author_str = author_str[3:].strip()\n",
    "    # If a comma is present, assume the format is \"Last, First\" and reformat to \"First Last\"\n",
    "    if ',' in author_str:\n",
    "        parts = [part.strip() for part in author_str.split(',')]\n",
    "        if len(parts) >= 2:\n",
    "            first_names = \" \".join(parts[1:])\n",
    "            return f\"{first_names} {parts[0]}\"\n",
    "    return author_str\n",
    "\n",
    "# Create a new column 'authors' with the cleaned values while keeping the original 'Authors' column intact\n",
    "df['authors'] = df['Authors'].apply(clean_authors)\n",
    "\n",
    "# Display a sample to verify the transformation\n",
    "df[['Title', 'Authors', 'authors']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **\"Publisher\" Standardization Synonyms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential standardized groups:\n",
      "{'Doubleday'}\n",
      "{'Putnam Pub Group', 'Putnam Pub. Group'}\n",
      "{'Workman Pub Co', 'Workman Pub. Co'}\n",
      "{'Natl Pr Books'}\n",
      "{'Amy Spangler'}\n",
      "{'Excalibur Press'}\n",
      "{'Health Communications, Inc.', 'Health Communications Inc'}\n",
      "{'VSP Books'}\n",
      "{'Random House'}\n",
      "{'Oxmoor House'}\n",
      "{'Workman Publishing Company'}\n",
      "{'Simons & Schuster', 'Simon & Schuster'}\n",
      "{'Crown'}\n",
      "{'HarperPerennial', 'Harper Perennial'}\n",
      "{'Vintage'}\n"
     ]
    }
   ],
   "source": [
    "merged_groups = get_standardized_groups(df, 'Publisher', threshold=94, scorer=fuzz.ratio)\n",
    "print(\"Potential standardized groups:\")\n",
    "for group in merged_groups[:15]:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardized Publisher names added to DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>parsed_publish_date</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>price</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goat Brothers</td>\n",
       "      <td>By Colton, Larry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>History , General</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>Friday, January 1, 1993</td>\n",
       "      <td>Price Starting at $8.79</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>8.79</td>\n",
       "      <td>Larry Colton</td>\n",
       "      <td>Doubleday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Missing Person</td>\n",
       "      <td>By Grumbach, Doris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fiction , General</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>Sunday, March 1, 1981</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "      <td>1981-03-01</td>\n",
       "      <td>1981-03-01</td>\n",
       "      <td>4.99</td>\n",
       "      <td>Doris Grumbach</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't Eat Your Heart Out Cookbook</td>\n",
       "      <td>By Piscatella, Joseph C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooking , Reference</td>\n",
       "      <td>Workman Pub Co</td>\n",
       "      <td>Thursday, September 1, 1983</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "      <td>1983-09-01</td>\n",
       "      <td>1983-09-01</td>\n",
       "      <td>4.99</td>\n",
       "      <td>Joseph C. Piscatella</td>\n",
       "      <td>Workman Pub Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment</td>\n",
       "      <td>By Davis, Paul D.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natl Pr Books</td>\n",
       "      <td>Monday, April 1, 1991</td>\n",
       "      <td>Price Starting at $4.99</td>\n",
       "      <td>1991-04-01</td>\n",
       "      <td>1991-04-01</td>\n",
       "      <td>4.99</td>\n",
       "      <td>Paul D. Davis</td>\n",
       "      <td>Natl Pr Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Spangler's Breastfeeding : A Parent's Guide</td>\n",
       "      <td>By Spangler, Amy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amy Spangler</td>\n",
       "      <td>Saturday, February 1, 1997</td>\n",
       "      <td>Price Starting at $5.32</td>\n",
       "      <td>1997-02-01</td>\n",
       "      <td>1997-02-01</td>\n",
       "      <td>5.32</td>\n",
       "      <td>Amy Spangler</td>\n",
       "      <td>Amy Spangler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    Title  \\\n",
       "0                                                                           Goat Brothers   \n",
       "1                                                                      The Missing Person   \n",
       "2                                                       Don't Eat Your Heart Out Cookbook   \n",
       "3  When Your Corporate Umbrella Begins to Leak: A Handbook for White Collar Re-Employment   \n",
       "4                                         Amy Spangler's Breastfeeding : A Parent's Guide   \n",
       "\n",
       "                    Authors Description              Category  \\\n",
       "0          By Colton, Larry         NaN     History , General   \n",
       "1        By Grumbach, Doris         NaN     Fiction , General   \n",
       "2  By Piscatella, Joseph C.         NaN   Cooking , Reference   \n",
       "3         By Davis, Paul D.         NaN                   NaN   \n",
       "4          By Spangler, Amy         NaN                   NaN   \n",
       "\n",
       "          Publisher                 Publish Date                    Price  \\\n",
       "0         Doubleday      Friday, January 1, 1993  Price Starting at $8.79   \n",
       "1  Putnam Pub Group        Sunday, March 1, 1981  Price Starting at $4.99   \n",
       "2    Workman Pub Co  Thursday, September 1, 1983  Price Starting at $4.99   \n",
       "3     Natl Pr Books        Monday, April 1, 1991  Price Starting at $4.99   \n",
       "4      Amy Spangler   Saturday, February 1, 1997  Price Starting at $5.32   \n",
       "\n",
       "  parsed_publish_date publication_date  price               authors  \\\n",
       "0          1993-01-01       1993-01-01   8.79          Larry Colton   \n",
       "1          1981-03-01       1981-03-01   4.99        Doris Grumbach   \n",
       "2          1983-09-01       1983-09-01   4.99  Joseph C. Piscatella   \n",
       "3          1991-04-01       1991-04-01   4.99         Paul D. Davis   \n",
       "4          1997-02-01       1997-02-01   5.32          Amy Spangler   \n",
       "\n",
       "  publisher_standard  \n",
       "0          Doubleday  \n",
       "1   Putnam Pub Group  \n",
       "2     Workman Pub Co  \n",
       "3      Natl Pr Books  \n",
       "4       Amy Spangler  "
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df['Publisher'].value_counts().to_dict()\n",
    "# mapping dictionary\n",
    "mapping = {}\n",
    "for group in merged_groups:\n",
    "    canonical = max(group, key=lambda x: freq.get(x, 0))\n",
    "    for pub in group:\n",
    "        mapping[pub] = canonical\n",
    "\n",
    "for pub in unique_publishers:\n",
    "    if pub not in mapping:\n",
    "        mapping[pub] = pub\n",
    "        \n",
    "df['publisher_standard'] = df['Publisher'].map(lambda x: mapping.get(x, x))\n",
    "\n",
    "print(\"\\nStandardized Publisher names added to DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Authors', 'Description', 'Category', 'Publisher',\n",
       "       'Publish Date', 'Price', 'parsed_publish_date', 'publication_date',\n",
       "       'price', 'authors', 'publisher_standard'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Blocking Strategy - Three similarity metrics\n",
    "To perform the blocking we will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import isbnlib\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the tables referring to the books and their attributes were used, the dataset tables referring to ratings, users or others were not used.\n",
    "### 4.1. Normalization of the datasets\n",
    "#### 4.1.1. Necessary functions\n",
    "First, we had to normalize the datasets in order for the metrics to be more accurate. Below are the functions that we had to create and use to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had to normalize the names of the columns so that we could use da similarity metrics by column\n",
    "def normalize_column_names(df):\n",
    "    column_map = {\n",
    "        'ISBN': 'isbn13',\n",
    "        'ISBN_13': 'isbn13',\n",
    "        'parent_asin': 'asin',\n",
    "        'ASIN': 'asin',\n",
    "        'authors': 'author',\n",
    "        'Authors': 'author',\n",
    "        'Book-Author': 'author',\n",
    "        'Author': 'author',\n",
    "        'AUTHOR': 'author',\n",
    "        'title_text': 'title',\n",
    "        'TITLE': 'title',\n",
    "        'Book-Title': 'title',\n",
    "        'Book Name': 'title',\n",
    "        'Title': 'title'\n",
    "    }\n",
    "    df = df.rename(columns={col: column_map.get(col, col) for col in df.columns})\n",
    "    return df\n",
    "\n",
    "\n",
    "# This funcion normalize the titles of each book\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return unidecode(text) \n",
    "\n",
    "\n",
    "# This one is to normalize da isbn of each book\n",
    "def normalize_isbn(isbn):\n",
    "    if not isbn or not isinstance(isbn, str) or isbn.strip() == \"\":  # some isbns are in varchar insted of integer in our datasets\n",
    "        return None\n",
    "    isbn = isbnlib.canonical(isbn)\n",
    "    if isbnlib.is_isbn10(isbn):\n",
    "        return isbnlib.to_isbn13(isbn)\n",
    "    return isbn if isbnlib.is_isbn13(isbn) else None\n",
    "\n",
    "\n",
    "# Function to normalize asin\n",
    "def normalize_asin(asin):\n",
    "    if pd.isna(asin):\n",
    "        return \"\"\n",
    "    return asin.strip().upper()\n",
    "\n",
    "\n",
    "# Function to normalize the column author in some datasets, that only have 1 author\n",
    "def normalize_author1(author):\n",
    "    if pd.isna(author):\n",
    "        return \"\"\n",
    "    author = unidecode(author)              \n",
    "    author = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", author)      \n",
    "    author = re.sub(r\"\\s+\", \" \", author)     \n",
    "    author = author.strip().upper()          \n",
    "    return author\n",
    "\n",
    "\n",
    "# Function to normalize the column authors in some datasets that separate the authors by \"/\")\n",
    "def normalize_authors2(authors):\n",
    "    if pd.isna(authors) or not isinstance(authors, str):\n",
    "        return \"\"\n",
    "    author_list = [a.strip() for a in authors.split('/')]\n",
    "    normalized_authors = []\n",
    "    for author in author_list:\n",
    "        author = unidecode(author)\n",
    "        author = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", author) \n",
    "        author = re.sub(r\"\\s+\", \" \", author)  \n",
    "        author = author.strip().upper() \n",
    "        normalized_authors.append(author)\n",
    "    return \", \".join(normalized_authors)\n",
    "\n",
    "\n",
    "# Function to normalize the column author in some datasets, that only have 1 author\n",
    "def normalize_authors3(authors):\n",
    "    if pd.isna(authors) or not isinstance(authors, str) or authors.strip() == \"\":\n",
    "        return \"\"\n",
    "    author_list = [a.strip() for a in authors.split(\",\")]\n",
    "    normalized_authors = []\n",
    "    for author in author_list:\n",
    "        author = unidecode(author)   \n",
    "        author = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", author)\n",
    "        author = re.sub(r\"\\s+\", \" \", author)  \n",
    "        author = author.strip().upper()  \n",
    "        normalized_authors.append(author)\n",
    "    return \", \".join(normalized_authors)\n",
    "\n",
    "\n",
    "# Function to normalize the column authors in some datasets that separate the authors by \"and\")\n",
    "def normalize_author4(author):\n",
    "    if pd.isna(author) or not isinstance(author, str): \n",
    "        return \"\"\n",
    "   \n",
    "    authors = author.split(',')\n",
    "   \n",
    "    normalized_authors = []\n",
    "    for individual_author in authors:\n",
    "        individual_author = individual_author.strip()\n",
    "        individual_author = individual_author.replace(\"by \", \"\").replace(\"By \", \"\")\n",
    "        individual_author = unidecode(individual_author)\n",
    "        individual_author = individual_author.upper()\n",
    "        normalized_authors.append(individual_author)\n",
    "        return \", \".join(normalized_authors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Aplication of Normalization\n",
    "Note that we only use the columns necessary for blocking, that is, we remove the columns that will not be necessary to perform blocking.\n",
    "#### a) Amazon Reviews 2023 (metadata)\n",
    "In this dataset we only had to normalize the collunms: \"parent_asin\" and \"title\".We only used the table about the metadata of the books for the blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\1_amazon\\\\amazon_meta_books.csv\")\n",
    "\n",
    "df1['parent_asin'] = df1['parent_asin'].apply(normalize_asin)\n",
    "df1['title'] = df1['title'].apply(normalize_text)\n",
    "df1=normalize_column_names(df1)\n",
    "df1_normalized_only = df1[['asin', 'title']]\n",
    "\n",
    "output_path1 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\1_amazon\\\\amazon_meta_books_normalized.csv\"\n",
    "n1=df1.to_csv(output_path1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Goodreads - 2017 (metadata)\n",
    "In this dataset we only had to normalize the collunms: \"title\", \"authors\" and \"isbn13\". We only used the table about the metadata of the books for the blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\Datasets\\Datasets\\\\2_3_goodreads\\\\goodReads_2019_2020_fixed.csv\")\n",
    "\n",
    "df2['title'] = df2['title'].apply(normalize_text)\n",
    "df2['authors'] = df2['authors'].apply(normalize_authors2)\n",
    "df2['isbn13'] = df2['isbn13'].apply(normalize_isbn)\n",
    "df2=normalize_column_names(df2)\n",
    "df2_normalized_only = df2[['title', 'author', 'isbn13']]\n",
    "\n",
    "output_path2 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\2_3_goodreads\\\\goodReads_2019_2020_normalized.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Goodreads - 2019 & 2020\n",
    "In this dataset we only had to normalize the collumns: \"asin\" and \"isbn\". This dataset is in json format so it was treated differently from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\2_3_goodreads\\\\goodreads_3000RCount.json\"\n",
    "df = pd.read_json(input_path, lines=True)\n",
    "\n",
    "df['asin'] = df.apply(lambda row: row.get('asin') or row.get('kindle_asin') or \"\", axis=1)\n",
    "df['isbn'] = df.apply(lambda row: row.get('isbn13') or row.get('isbn') or \"\", axis=1)\n",
    "df['isbn'] = df['isbn'].apply(normalize_isbn)\n",
    "df_cleaned = df[['asin', 'isbn']]\n",
    "\n",
    "output_path3 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\2_3_goodreads\\\\goodreads_3000RCount_normalized.csv\"\n",
    "df_cleaned.to_csv(output_path3, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) BookCrossing Community\n",
    "In this dataset we only had to normalize the collumns: \"ISBN\", \"Book-Title\" and \"Book-author\". We only look at the books table for the blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\4_bookcrossing\\\\book_crossing_Books.csv\")\n",
    "\n",
    "df4['ISBN'] = df4['ISBN'].apply(normalize_isbn)\n",
    "df4['Book-Title'] = df4['Book-Title'].apply(normalize_text)\n",
    "df4['Book-Author'] = df4['Book-Author'].apply(normalize_author1)\n",
    "df4=normalize_column_names(df4)\n",
    "df4_normalized_only = df4[['isbn13', 'title', 'author']]\n",
    "\n",
    "output_path4 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\4_bookcrossing\\\\book_crossing_Books_normalized.csv\"\n",
    "n4=df4.to_csv(output_path4, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Book Sales and Ratings\n",
    "In this dataset we only had to normalize the collumns: \"Book Name\" and \"Author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\5_sales_N_ratings\\\\Books_Data_Clean.csv\")\n",
    "\n",
    "df5['Book Name'] = df5['Book Name'].apply(normalize_text)\n",
    "df5['Author'] = df5['Author'].apply(normalize_authors3)\n",
    "df5=normalize_column_names(df5)\n",
    "df5_normalized_only = df5[['title', 'author']]\n",
    "\n",
    "output_path5 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\5_sales_N_ratings\\\\Books_Data_Clean_Normalized.csv\"\n",
    "n5=df5.to_csv(output_path5, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Book Sales and Rantings\n",
    "In this dataset we only had to normalize the collumns: \"ASIN\" and \"AUTHOR\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\6_ranks_print_kindle\\\\amazon_com_extras.csv\", encoding=\"latin1\", on_bad_lines=\"skip\", quotechar='\"')\n",
    "\n",
    "df6['ASIN'] = df6['ASIN'].apply(normalize_asin)\n",
    "df6['AUTHOR'] = df6['AUTHOR'].apply(normalize_author1)\n",
    "df6=normalize_column_names(df6)\n",
    "df6_normalized_only = df6[['asin','title', 'author']]\n",
    "\n",
    "output_path6 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\6_ranks_print_kindle\\\\amazon_com_extras_normalized.csv\"\n",
    "n6=df6.to_csv(output_path6, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g) Amazon Kindle Books Dataset 2023\n",
    "In this dataset we only had to normalize the collumns: \"asin\", \"title\" and \"author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\7_kindle\\\\kindle_data-v2.csv\")\n",
    "\n",
    "df7['asin'] = df7['asin'].apply(normalize_asin)\n",
    "df7['title'] = df7['title'].apply(normalize_text)\n",
    "df7['author'] = df7['author'].apply(normalize_author1)\n",
    "df7=normalize_column_names(df7)\n",
    "df7_normalized_only = df7[['asin','title', 'author']]\n",
    "\n",
    "output_path7 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\7_kindle\\\\kindle_data-v2_normalized.csv\"\n",
    "n7=df7.to_csv(output_path7, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h) WonderBook\n",
    "In this dataset we only had to normzalize the collumns: \"Title\" and \"Authors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.read_csv(\"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\8_wonderbk\\BooksDataset.csv\")\n",
    "\n",
    "df8['Title'] = df8['Title'].apply(normalize_text)\n",
    "df8['Authors'] = df8['Authors'].apply(normalize_author4)\n",
    "df8=normalize_column_names(df8)\n",
    "df8_normalized_only = df8[['title', 'author']]\n",
    "\n",
    "output_path8 = \"C:\\\\Users\\\\maria\\\\Downloads\\\\Trabalho de grupo\\\\Datasets\\\\Datasets\\\\8_wonderbk\\BooksDataset_Normalized.csv\"\n",
    "n8=df8.to_csv(output_path8, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Blocking aplication\n",
    "For the keys we use the following strategy:\n",
    "- First prioritize \"isbn\", comparing by exact match;\n",
    "- Then prioritize \"asin\", comparing by exact match;\n",
    "- Finally by \"title\" and \"author\" of the book, using the Jaccard (titles) and Jaro-Winkler (authors) similarity metrics.\n",
    "\n",
    "Note: The auxiliary function serves only to help create the Jaccard method.\n",
    "\n",
    "For each group of matches found:\n",
    "- Join the data from all the datasets where the book appeared.\n",
    "- Indicate which datasets it came from (datasets_match).\n",
    "\n",
    "Finally, the code exports everything to a CSV where each line is a book found in common, bringing the columns from all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the keys\n",
    "def gerar_chaves_por_dataset(df, nome_dataset):\n",
    "    chaves = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        isbn = str(row.get(\"isbn13\") or row.get(\"isbn\") or \"\").strip()\n",
    "        asin = str(row.get(\"asin\") or \"\").strip()\n",
    "        titulo = str(row.get(\"title\") or row.get(\"titulo\") or \"\").strip()\n",
    "        autor = str(row.get(\"author\") or row.get(\"autor\") or \"\").strip()\n",
    "\n",
    "        if isbn:\n",
    "            chave = \"isbn_\" + isbn\n",
    "        elif asin:\n",
    "            chave = \"asin_\" + asin\n",
    "        else:\n",
    "            chave = \"titulo_autor_\" + titulo + \"_\" + autor\n",
    "        chaves[chave] = (idx, nome_dataset)  \n",
    "    return chaves\n",
    "\n",
    "# Auxiliary function for the Jaccard similarity metric\n",
    "def jaccard_sim(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0\n",
    "\n",
    "def encontrar_comuns(datasets, nomes_datasets, limiar_autor=0.9, limiar_titulo=0.8, minimo_datasets=2):\n",
    "    chaves_por_dataset = []\n",
    "    for i, df in enumerate(datasets):\n",
    "        chaves_por_dataset.append(gerar_chaves_por_dataset(df, nomes_datasets[i]))\n",
    "\n",
    "    matches = []\n",
    "    total_datasets = len(datasets)\n",
    "\n",
    "    # Create a list to store the matches and the datasets where the match occurred\n",
    "    for chave_base in chaves_por_dataset[0]:\n",
    "        idxs = [None] * total_datasets\n",
    "        presencas = 1\n",
    "        datasets_com_match = [nomes_datasets[0]] \n",
    "\n",
    "        idxs[0] = chaves_por_dataset[0][chave_base][0]\n",
    "        \n",
    "        tipo = \"\"\n",
    "\n",
    "        if chave_base.startswith(\"isbn_\"):\n",
    "            tipo = \"isbn\"\n",
    "            for i in range(1, total_datasets):\n",
    "                if chave_base in chaves_por_dataset[i]:\n",
    "                    idxs[i] = chaves_por_dataset[i][chave_base][0]\n",
    "                    presencas += 1\n",
    "                    datasets_com_match.append(nomes_datasets[i])\n",
    "\n",
    "        elif chave_base.startswith(\"asin_\"):\n",
    "            tipo = \"asin\"\n",
    "            for i in range(1, total_datasets):\n",
    "                if chave_base in chaves_por_dataset[i]:\n",
    "                    idxs[i] = chaves_por_dataset[i][chave_base][0]\n",
    "                    presencas += 1\n",
    "                    datasets_com_match.append(nomes_datasets[i])\n",
    "\n",
    "        elif chave_base.startswith(\"titulo_autor_\"):\n",
    "            tipo = \"fuzzy\"\n",
    "            titulo_base = datasets[0].loc[chaves_por_dataset[0][chave_base][0]][\"title\"]\n",
    "            autor_base = datasets[0].loc[chaves_por_dataset[0][chave_base][0]][\"author\"]\n",
    "\n",
    "            for i in range(1, total_datasets):\n",
    "                for chave_cand, (idx_cand, _) in chaves_por_dataset[i].items():\n",
    "                    if chave_cand.startswith(\"titulo_autor_\"):\n",
    "                        row_cand = datasets[i].loc[idx_cand]\n",
    "                        sim_autor = jellyfish.jaro_winkler_similarity(autor_base, row_cand[\"author\"])\n",
    "                        sim_titulo = jaccard_sim(titulo_base, row_cand[\"title\"])\n",
    "                        if sim_autor >= limiar_autor and sim_titulo >= limiar_titulo:\n",
    "                            idxs[i] = idx_cand\n",
    "                            presencas += 1\n",
    "                            datasets_com_match.append(nomes_datasets[i])\n",
    "                            break\n",
    "\n",
    "        # If the book is found in at least 2 datasets, we add it to the result\n",
    "        if presencas >= minimo_datasets:\n",
    "            grupo = []\n",
    "            for i, idx in enumerate(idxs):\n",
    "                if idx is not None:\n",
    "                    grupo.append(datasets[i].loc[idx])\n",
    "\n",
    "            # Add information about which dataset contains the match\n",
    "            grupo.append({\"datasets_match\": \", \".join(datasets_com_match)})\n",
    "            matches.append(grupo)\n",
    "\n",
    "    return matches\n",
    "\n",
    "# Dataset paths\n",
    "if __name__ == \"__main__\":\n",
    "    caminhos_datasets = [\n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\1_amazon\\\\amazon_meta_books_normalized.csv\", \n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\2_3_goodreads\\\\goodreads_2019_2020_normalized.csv\", \n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\2_3_goodreads\\\\goodreads_3000RCount_normalized.csv\", \n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\4_bookcrossing\\\\book_crossing_Books_normalized.csv\", \n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\5_sales_N_ratings\\\\Books_Data_Clean_normalized.csv\",\n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\6_ranks_print_kindle\\\\amazon_com_extras_normalized.csv\",\n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\7_kindle\\\\kindle_data-v2_normalized.csv\",\n",
    "        \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\Datasets\\\\Datasets\\\\8_wonderbk\\\\BooksDataset_normalized.csv\"\n",
    "    ]\n",
    "    nomes_datasets = [\"1_amazon\", \"2_goodreads\", \"3_goodreads\", \"4_bookcrossing\", \"5_sales_N_ratings\", \"6_ranks_print_kindle\", \"7_kindle\", \"8_wonderbk\" ]\n",
    "\n",
    "    datasets = [pd.read_csv(caminho) for caminho in caminhos_datasets]\n",
    "\n",
    "    matches = encontrar_comuns(datasets, nomes_datasets, minimo_datasets=2)\n",
    "\n",
    "# Export results to a CSV with the datasets_match column\n",
    "    def exportar_csv(matches, nome_ficheiro):\n",
    "        linhas = []\n",
    "        for grupo in matches:\n",
    "            registo_comum = {}\n",
    "            for i, livro in enumerate(grupo[:-1]): \n",
    "                for chave, valor in livro.items():\n",
    "                    registo_comum[f\"{chave}_dataset{i+1}\"] = valor\n",
    "\n",
    "            registo_comum[\"datasets_match\"] = grupo[-1][\"datasets_match\"]\n",
    "            linhas.append(registo_comum)\n",
    "\n",
    "        df_resultado = pd.DataFrame(linhas)\n",
    "        df_resultado.to_csv(nome_ficheiro, index=False)\n",
    "\n",
    "    exportar_csv(matches, \"C:\\\\Users\\\\diogo\\\\Desktop\\\\Mestrado\\\\IPAI\\\\pasta\\\\matches_resultado.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
